Adoption of Spark ML native math librariesCloudera Docs
Adoption of Spark ML native math libraries
The range of acceleration provided by native libraries varies from model to model.
By: Zuling Kang, Senior Solutions Architect at Cloudera, Inc.
Anand Iyer and Vikram Saletore showed in their engineering blog post
     that native math libraries like OpenBLAS and Intel’s Math Kernel Library
     (MKL) accelerate the training performance of Spark ML. As for the matrix
     factorization model used in recommendation systems (the Alternating Least
     Squares (ALS) algorithm), both OpenBLAS and Intel’s MKL yield model
     training speeds that are 4.3 times faster than with the F2J implementation.
     Others, like the Latent Dirichlet Allocation (LDA), the Primary Component
     Analysis (PCA), and the Singular Value Decomposition (SVD) algorithms show
     56% to 72% improvements for Intel MKL, and 10% to 50% improvements for
     OpenBLAS.
However, the blog post also demonstrates that there are some
     algorithms, like Random Forest and Gradient Boosted Tree, that receive
     almost no speed acceleration after enabling OpenBLAS or MKL. The reasons
     for this are largely that the training set of these tree-based
     algorithms are not vectors. This indicates that the native libraries,
     either OpenBLAS or MKL, adapt better to algorithms whose training sets
     can be operated on as vectors and are computed as a whole. It is more
     effective to use math acceleration for algorithms that operate on
     training sets using matrix operations.

Parent topic: Native math libraries for Spark ML