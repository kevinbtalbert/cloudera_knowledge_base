Prepare your environmentCloudera Docs
Prepare your environment
Before you start analyzing your data using Flink and Kafka in CDP Public Cloud, you need
  to register and prepare your environment so that a chosen user or group can use the clusters and
  services in the environment, and submit Flink jobs securely.
As a first step, an admin needs to register an AWS, Azure or GCP environment. In CDP Public
      Cloud, this environment is based on the virtual private network in your cloud provider
      account. This means that CDP can access and identify the resources in your cloud provider
      account which is used for CDP services, and shared between clusters in the environment. After
      registering your AWS, Azure or GCP environment, you can provision clusters, and set users or
      groups to access your environment and services.
For more information about registering an AWS, Azure or GCP environment, see the Management Console documentation.
In this use case, an administrator with AdminEnvironment resource role registers an AWS
      environment where flink_users group is added as EnvironmentUser. The members of flink_users
      have the same resource role that is set for the group.

Assign resource rolesAs an administrator, you need to give permissions to users or groups to be able to         access and perform tasks in your Data Hub environment.Create IDBroker mappingAs an administrator, you must create IDBroker mapping for a user or group to access         cloud storage. As a part of Knox, the IDBroker allows a user to exchange cluster         authentication for temporary cloud credentials.Set workload passwordAs a user, you need to set a workload password for your EnvironmentUser account to be         able to access the Flink nodes through SSH connection.Create your streaming clustersAs a user, you need to create the Streaming Analytics and Streams Messaging clusters.         In this use case, the streaming clusters are created in the same Data Hub         environment.Set Ranger policiesAs an administrator, you need to set resource-based Ranger policies to give users or         groups access to write and read Kafka topics.Retrieve and upload keytab fileAs a user, you need to retrieve the keytab file of your profile and upload it to the         Streaming Analytics cluster to securely submit your Flink jobs.Create Atlas entity type definitionsAs an administrator, you must create the Atlas entity type definitions before         submitting a Flink job, and enable Atlas in Cloudera Manager to use Atlas for metadata         management.Parent topic: Analyzing your data with Kafka