SQL connectors for FlinkCloudera Docs
SQL connectors for Flink
In Flink SQL, the connector describes the external system that stores the data of a
  table. Cloudera Streaming Analytics offers you Kafka and Kudu as SQL connectors. You need to
  further choose the data formats and table schema based on your connector.
Some systems support different data formats. For example, a table that is stored in Kafka can
   encode its rows with CSV, JSON, or Avro. The table schema defines the schema of a table that is
   exposed to SQL queries. It describes how sources and sinks map the data format to the table
   schema.


Kudu connector
The Kudu connector in Cloudera Streaming Analytics offers compatibility with other
      supported catalogs, and capability to convert your Kudu tables into DataStreams. You need to
      add the Kudu dependency to your project, set up the catalog, and you can either use SQL
      queries or the Kudu catalog directly to create tables.



Kafka connectorCloudera Streaming Analytics provides Kafka as not only a DataStream connector, but also     enables Kafka in the Flink SQL feature. This means if you have designed your streaming     application to have Kafka as source and sink, you can retrieve your output data in tables. When     using the Kafka connector, you are required to specify one of the supported message     formats.Data types for Kafka connectorWhen reading data using the Kafka table connector, you must specify the format of the                 incoming messages so that Flink can map incoming data to table columns                 properly.Parent topic: SQL catalogs for Flink