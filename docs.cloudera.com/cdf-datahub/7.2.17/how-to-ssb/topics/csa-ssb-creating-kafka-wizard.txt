Creating Kafka tables using wizardCloudera Docs
Creating Kafka tables using wizard
After registering a Kafka data source, you can use the Kafka table wizard in
        Streaming SQL Console to create a Kafka table.
You can query your streaming data using Kafka tables in
            SQL Stream Builder (SSB). You have the option to use the Kafka service in your
            environment, or connect to an external Kafka service. When creating Kafka tables you can
            use the Add Kafka wizard, the predefined templates or you can directly add a custom
            CREATE TABLE statement with the required properties in the SQL window.You can also create Kafka tables using
                one of the Kafka templates. For more information, see the Kafka connectors and Using connectors with templates
                sections.


Make sure that you have registered Kafka as a Data Source.
Make sure that you have created topics in Kafka.importantWhen
                        creating the topic for the Kafka sink, make sure to not use log compaction
                        as it can cause the SQL job to fail.
Make sure there is generated data in the Kafka topic.
Make sure that you have the right permissions set in Ranger.



Navigate to the Streaming SQL Console.


Navigate to Management Console > Environments, and select the environment where you have created your
                cluster.


Select the Streaming Analytics cluster from the list of
                Data Hub clusters.


Select Streaming SQL Console from the list of
                services.
The Streaming SQL Console opens in a new window.



Open a project from the Projects page of Streaming SQL
            Console.


Select an already existing project from the list by clicking the
                  Open button or Switch button.


Create a new project by clicking the New Project
                button.


Import a project by clicking the Import button.


You are redirected to the Explorer view of the project.

Click  next to Virtual Tables.

Click New Kafka Table.
The Kafka Table window appears.



Provide a Table Name.

noteYou will use this name in the FROM clause when running
                        the SQL statement.


Select a registered Kafka provider as Kafka
                    cluster.

Select the Data format.

You can select JSON as data format.
You can select AVRO as data format.


Select a Kafka topic from the list.

noteThe automatically created topics for the websocket output is also listed
                        here. Select the topic you want to use for the SQL job.


Add a customized schema to Schema Definition or click
                        Detect Schema to read a sample of the JSON messages,
                    and automatically infer the schema.

noteThe Detect Schema functionality is only available for JSON data. If there
                        are no messages in the topic, then no schema will be inferred. If your
                        schema contains a field named timestamp, this causes a schema
                        validation error as timestamp is a reserved word used for Kafka
                        internal timestamps.


Customize your Kafka Table with the following options:


Configure the Event Time if you do not want to
                            use the default Kafka Timestamps.


Configure an Input Transform on the Data
                                Transformations tab.

noteThe Input Transformation is only supported for JSON data
                                formats.



Configure any Kafka properties required on the
                                Properties tab.


Select a policy for deserialization errors on the
                                Deserialization tab.


For more information about how to configure the Kafka table, see the
                        Configuring Kafka tables section.

Click Create and Review.

The Kafka Table is ready to be used for the SQL job either
            at the FROM or at the INSERT INTO statements.

Configuring Kafka tablesThe user-defined Kafka table can be configured based on the schema, event time, input     transformations and other Kafka specific properties using either the Kafka wizard or     DDL.Parent topic: Concept of tables in SSB