Concept of tables in SSBCloudera Docs
Concept of tables in SSB
The core abstraction for Streaming SQL is a Table which represents both inputs and
  outputs of the queries. SQL Stream Builder (SSB) tables are an extension of the tables used in
  Flink SQL to allow a bit more flexibility to the users. When creating tables in SSB, you have the
  option to either add them manually, import them automatically or create them using Flink SQL
  depending on the connector you want to use.
A Table is a logical definition of the data source that includes the location and connection
   parameters, a schema, and any required, context specific configuration parameters. Tables can be
   used for both reading and writing data in most cases. You can create and manage tables either
   manually or they can be automatically loaded from one of the catalogs as specified using the Data
   Sources section.
In SELECT queries the FROM clause defines the table sources
   which can be multiple tables at the same time in case of JOIN or more complex
   queries.
When you execute a query, the results go to the table you specify after the INSERT INTO
   statement in the SQL window. This allows you to create aggregations, filters, joins, and so on,
   and then route the results to another table. The schema for the results is the schema that you
   have created when you ran the query. 
For example:
   INSERT INTO air_traffic -- the name of the table sink
SELECT
lat,lon
FROM
airplanes -- the name of the table source
WHERE
icao <> 0;

Table types in SSB

Kafka Tables
Apache Kafka Tables represent data contained in a single Kafka topic in JSON, AVRO or CSV
      format. It can be defined using the Streaming SQL Console wizard or you can create Kafka
      tables from the pre-defined templates.
Tables from Catalogs
SSB supports Kudu, Hive and Schema Registry as catalog providers. After registering them
      using the Streaming SQL Console, the tables are automatically imported to SSB, and can be used
      in the SQL window for computations.

noteYou cannot edit the properties of the already existing tables that are automatically
       imported from the catalogs. To distinguish between editable and non-editable tables, in other
       words, user defined and catalog tables, the Edit and
        Delete table options are not available on the
        Tables page.

Flink Tables
Flink SQL tables represent tables created by the standard CREATE TABLE syntax. This
      supports full flexibility in defining new or derived tables and views. You can either provide
      the syntax by directly adding it to the SQL window or use one of the predefined DDL
      templates.
Webhook Tables
Webhooks can only be used as tables to write results to. When you use the Webhook Tables
      the result of your SQL query is sent to a specified webhook. 
Iceberg Tables
You can use Apache Iceberg tables when adding Apache Hive as a catalog. The Iceberg
      connector of Apache Flink is implemented in SSB that enables you to use Iceberg tables managed
      in the Hive catalog.



Creating Kafka tables using wizardAfter registering a Kafka data source, you can use the Kafka table wizard in         Streaming SQL Console to create a Kafka table.Creating Webhook tablesYou can configure the webhook table to perform an HTTP action per message (default)         or to create code that controls the frequency (for instance, every N messages). When         developing webhook sinks, it is recommended to check your webhook before pointing at your         true destination.Creating Flink tables using TemplatesYou can use the predefined templates to create tables by choosing one of the         connector templates on the Console page of Streaming SQL Console. The Flink SQL templates         are predefined examples of CREATE TABLE statements which you can fill out with your job         specific values.Creating Iceberg tablesApache Iceberg is an open, high-performance table format for organizing datasets that   can contain petabytes of data. Iceberg can be used to add tables to computing engines, such as   Apache Hive and Apache Flink, from which the data can be queried using SQL.