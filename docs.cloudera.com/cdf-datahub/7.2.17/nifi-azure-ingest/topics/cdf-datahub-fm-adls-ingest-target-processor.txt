Configure the processor for your data targetCloudera Docs
Configure the processor for your data target
Learn how you can configure the data target processor for your ADLS ingest data flow.
    This example assumes that you are moving data to Azure Data Lake Storage and shows you how to
    configure the corresponding processors.

Launch the Configure Processor window by right clicking the
          processor you added for writing data to ADLS (PutHDFS or
            PutAzureDataLakeStorage) and selecting
            Configure. 
This gives you a configuration dialog with the following tabs:
              Settings, Scheduling,
              Properties, Comments.
Configure the processor according to the behavior you expect in your data
          flow.
Make sure that you set all required properties, as you cannot start the processor until
            all mandatory properties have been configured.
When you have finished configuring the options you need, save the changes by
          clicking the Apply button.
noteCloudera recommends using the PutHDFS processor for writing data
            to ADLS in CDP Public Cloud as it leverages the CDP authentication framework.
In this example, the following properties are used for PutHDFS:
Table 1. PutHDFS processor properties

Property
Description
Example value for ingest data flow




Hadoop Configuration Resources


Specify the path to the core-site.xml configuration file. 
Make sure that the default file system (fs, default.FS) points to the ADLS
                      bucket you are writing to.
noteThe core-site.xml file is present on every Flow
                      Management cluster. Cloudera Manager stores the right
                        core-site.xml file in the same /etc directory for
                      every cluster.


/etc/hadoop/conf.cloudera.core_settings/core-site.xml




Kerberos Principal


Specify the Kerberos principal (your CDP username) to authenticate against
                      CDP.


srv_nifi-adls-ingest




Kerberos Password


Provide the password that should be used for authenticating with
                      Kerberos.


password




Directory


Provide the path to your target directory in Azure expressed in an
                        abfs compatible path format.


abfs://<YourFileSystem>@<YourStorageAccount>.dfs.core.windows.net/<TargetPathWithinFileSystem>



You can leave all other properties as default configurations.
For a complete list of PutHDFS properties, see the processor
              documentation.
If you want to use the PutAzureDataLakeStorage processor
            to store the data in ADLS, you have to configure your Azure connection in a secure way
            by providing:
note
PutAzureDataLakeStorage has several limitations:

You can add files to read-only buckets
There is no check for file overwriting. It is possible to overwrite data. 
To add files to a bucket root level, set the destination with an empty string,
                rather than " / ". 



Storage Account Name - the name of your Azure storage account that holds
              the containers where you want to write to)
Storage Account Key or your SAS Token - the authentication key that allows
              you to write data to the Azure storage account
noteBy default, all resources in Azure Storage are secured, and are available only
                  to the account owner. You can grant clients general access to your storage account
                  sharing the storage account key, or you can give more granular access for certain
                  users to specific resources in your storage account using the SAS Token.

Filesystem Name - the name of your Azure storage account file system where
              you want to write to
Directory Name - the name of the folder within your filesystem where you
              want to write to


Make sure that you set all required properties, as you cannot start the processor until
            all mandatory properties have been configured.

Your data flow is ready to ingest data into ADLS. Start the flow.

Related informationConfiguring a processorApache NiFi DocumentationData ingest use cases in Cloudera Data Flow for Data HubParent topic: Ingesting data into Azure Data Lake Storage
