Build the data flowCloudera Docs
Build the data flow 
From the Apache NiFi canvas, set up the elements of your data flow. This involves
        opening NiFi in CDP Public Cloud, adding processors to your NiFi canvas, and connecting the
        processors.

You should use the PutHBaseRecord processor to build your HBase
                ingest data flows. 


Open NiFi in CDP Public Cloud.


To access the NiFi service in your Flow Management cluster, navigate to
                                Management Console service > Data Hub
                                Clusters.


Click the tile representing the Flow Management cluster with which you
                            want to work.


Click the NiFi icon in the
                                Services section of the Cluster overview page
                            to access the NiFi UI.








Add the NiFi Processors to your canvas.


Select the Processor icon from the Cloudera Flow
                            Management actions pane, and drag a processor to the Canvas.


Use the Add Processor filter box to search for
                            the processor you want to add, and then click
                            Add.


Add each of the processors you want to use for your data flow.



Connect the two processors to create a flow.


Click the connection icon in the first processor, and drag it to the
                            second processor.


A Create Connection dialog displays. It has
                                Details and Settings
                            tabs. You can configure the connection's name, FlowFile expiration time
                            period, thresholds for back pressure, load balance strategy, and
                            prioritization. 


Click Add to close the dialog box and add the
                            connection to your flow.
Optionally, you can add success and failure funnels to your data flow,
                            which help you see where flow files are routed when your data flow is
                            running.




Your data flow may look similar to the following:





Create the Controller service for your data flow. You will need these services later
                on as you configure your data flow target processor. 


Related informationIngesting data into Apache Kafka in CDP Public CloudConsumeKafkaRecord_2_0PutHBaseRecordBuilding a data flowParent topic: Ingesting Data into HBase