Known issues in Streaming AnalyticsCloudera Docs
Known issues in Streaming Analytics
Learn about the known issues in Streaming Analytics clusters, the impact or changes to
    the functionality, and the workaround.


CSA-4464: CSA parcel is built with an interim CDP build
The CSA parcel is built using an interim CDP build and not with a
          build that corresponds to a release version. This can cause errors with components that
          have dependency to Flink.
If a Flink component transitively depends on some CDP
          related module and it is not accessible publicly, the dependency can be excluded. In case
          the project also depends on the excluded module, the publicly available version of the
          dependency can be added to Flink. For example, this could happen with
            kafka-clients, which is pulled in by
            flink-connector-kafka:...
<dependency>
	<groupId>org.apache.flink</groupId>
	<artifactId>flink-connector-kafka</artifactId>
	<version>${flink.version}</version>
	<exclusions>
		<exclusion>
			<groupId>org.apache.kafka</groupId>
			<artifactId>kafka-clients</artifactId>
		</exclusion>
	</exclusions>
</dependency>

<dependency>
	<groupId>org.apache.kafka</groupId>
	<artifactId>kafka-clients</artifactId>
	<version>3.1.2.7.2.17.0-334</version>
</dependency>
...


SQL Stream Builder


Limitations when configuring widgets
The following widget configuration optionns are not available
            for certain widgets on Streaming SQL Console:
Gauge visualization type: Expand on hover, Unit
Donut visualization type: Expand on hover, Title
Pie visualization type: Expand on hover

None



Flink
In Cloudera Streaming Analytics, the following SQL API features are in preview:
Match recognize
Top-N
Stream-Table join (without rowtime input)


DataStream conversion limitations


Converting between Tables and POJO DataStreams is currently not supported in
                CSA.
Object arrays are not supported for Tuple conversion.
The java.time class conversions for Tuple DataStreams are only
                supported by using explicit TypeInformation:
                  LegacyInstantTypeInfo,
                  LocalTimeTypeInfo.getInfoFor(LocalDate/LocalDateTime/LocalTime.class).
Only java.sql.Timestamp is supported for rowtime conversion,
                  java.time.LocalDateTime is not supported.




Kudu catalog limitations


CREATE TABLE
Primary keys can only be set by the kudu.primary-key-columns
                    property. Using the PRIMARY KEY constraint is not yet
                    possible.
Range partitioning is not supported.

When getting a table through the catalog, NOT NULL and
                  PRIMARY KEY constraints are ignored. All columns are described as
                being nullable, and not being primary keys.
Kudu tables cannot be altered through the catalog other than simply renaming
                them.




Schema Registry catalog limitations


Currently, the Schema Registry catalog / format only supports reading messages
                with the latest enabled schema for any given Kafka topic at the time when the SQL
                query was compiled.
No time-column and watermark support for Registry tables.
No CREATE TABLE support. Schemas have to be registered directly
                in the SchemaRegistry to be accessible through the catalog.
The catalog is read-only. It does not support table deletions or
                modifications.
By default, it is assumed that Kafka message values contain the schema id as a
                prefix, because this is the default behaviour for the
                  SchemaRegistry Kafka producer format. To consume messages with
                schema written in the header, the following property must be set for the Registry
                  client: store.schema.version.id.in.header: true.





Parent topic: Known issues in Cloudera DataFlow for Data Hub 7.2.17