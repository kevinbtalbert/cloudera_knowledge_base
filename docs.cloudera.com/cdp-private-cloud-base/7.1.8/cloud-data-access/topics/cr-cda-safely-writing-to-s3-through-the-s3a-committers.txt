Safely Writing to S3 Through the S3A CommittersCloudera Docs
Safely Writing to S3 Through the S3A Committers
The S3A committers are three different committers used to commit work directly to
    Mapreduce and Spark. The committers are enabled by default for Spark in CDP.
Introducing the S3A CommittersAmazon's S3 Object Store is not a filesystem: some expected behaviors of a filesystem     are missing. Configuring Directories for Intermediate DataIn addition to fs.s3a.committer.name, two other       core-site.xml configuration options are used to control where intermediate is     stored.Using the Directory Committer in MapReduceOnce the propertyfs.s3a.committer.name is set, Hadoop MapReduce jobs         writing to paths using the s3a:// schema will automatically switch to the new         committers.Verifying That an S3A Committer Was UsedWhen working correctly, the only sign the new committers are in use is that it should     be faster to use S3 as a destination of work. Cleaning up after failed jobsThe S3A committers upload data in the tasks, completing the uploads when the job is     committed. Using the S3Guard Command to List and Delete UploadsThe hadoop s3guard uploads command can also be used to list all     outstanding uploads under a path, and delete them. Advanced Committer ConfigurationThe Apache documentation covers the full set of configuration options for the     committers.Securing the S3A CommittersThere are a few security considerations when using the S3A committers.The S3A Committers and Third-Party Object StoresThe S3A committers will work with any object store which implements the AWS S3         protocols. Limitations of the S3A CommittersThere are limitations of the S3A committers associated with custom file output     formats, MapReduce API output format, and non-availability of Hive support.Troubleshooting the S3A CommittersThe Apache documentation contains information about     troubleshooting the committers.Parent topic: Working with Amazon S3