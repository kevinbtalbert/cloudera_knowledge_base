Accessing data stored in Amazon S3 through SparkCloudera Docs
Accessing data stored in Amazon S3 through Spark

important Cloudera components writing data to S3 are
      constrained by the inherent limitation of Amazon S3 known as eventual
        consistency. For more information, see Data Storage Considerations. 
 To access data stored in Amazon S3 from Spark applications, use Hadoop
      file APIs (SparkContext.hadoopFile,
        JavaHadoopRDD.saveAsHadoopFile,
        SparkContext.newAPIHadoopRDD, and
        JavaHadoopRDD.saveAsNewAPIHadoopFile) for reading and
      writing RDDs, providing URLs of the form
          s3a://bucket_name/path/to/file.
      You can read and write Spark SQL DataFrames using the Data Source API. 
Make sure that your environment is configured to allow
      access to the buckets you need. You must also configure the
        spark.yarn.access.hadoopFileSystems parameter to
      include the buckets you need to access. You can do this using the Spark
      client configuration, or at runtime as a command line parameter.
For example:

Client configuration
            (/etc/spark/conf/spark-defaults.conf)

spark.yarn.access.hadoopFileSystems=s3a://bucket1,s3a://bucket2

spark-shell

spark-shell --conf "spark.yarn.access.hadoopFileSystems=s3a://bucket1,s3a://bucket2" ...

spark-submit

spark-submit --conf "spark.yarn.access.hadoopFileSystems=s3a://bucket1,s3a://bucket2" ...



Examples of accessing Amazon S3 data from SparkThe following examples demonstrate basic patterns of accessing     data in S3 using Spark. The examples show the setup steps, application code,     and input and output files located in S3.Parent topic: Accessing external storage from Spark