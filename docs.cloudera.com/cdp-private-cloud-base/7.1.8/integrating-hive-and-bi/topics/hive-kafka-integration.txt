Apache Hive-Kafka integrationCloudera Docs
Apache Hive-Kafka integration
As an Apache Hive user, you can connect to, analyze, and transform data in Apache Kafka
  from Hive. You can offload data from Kafka to the Hive warehouse. Using Hive-Kafka integration,
  you can perform actions on real-time data and incorporate streamed data into your
  application.
You connect to Kafka data from Hive by creating an external table that maps to a Kafka topic.
   The table definition includes a reference to a Kafka storage handler that connects to Kafka. On
   the external table, Hive-Kafka integration supports ad hoc queries, such as questions about data
   changes in the stream over a period of time. You can transform Kafka data in the following
    ways:
Perform data masking
Join dimension tables or any stream
Aggregate data
Change the SerDe encoding of the original stream
Create a persistent stream in a Kafka topic

You can achieve data offloading by controlling its position in the stream. The Hive-Kafka
   connector supports the following serialization and deserialization formats:
JsonSerDe (default)
OpenCSVSerde
AvroSerDe


Related informationApache Kafka DocumentationCreating a table for a Kafka streamYou can create an external table in Apache Hive that represents an Apache Kafka         stream to query real-time data in Kafka. You use a storage handler and table properties that         map the Hive database to a Kafka topic and broker. If the Kafka data is not in JSON format,         you alter the table to specify a serializer-deserializer for another format.Querying Kafka dataYou can get useful information, including Kafka record metadata from a table of Kafka   data by using typical Hive queries.Perform ETL by ingesting data from Kafka into HiveYou can extract, transform, and load a Kafka record into Hive in a single         transaction.Writing data to KafkaYou can extract, transform, and load a Hive table to a Kafka topic for real-time   streaming of a large volume of Hive data. You need some understanding of write semantics and the   metadata columns required for writing data to Kafka.Setting consumer and producer table propertiesYou can use Kafka consumer and producer properties in the TBLPROPERTIES clause of a         Hive query. By prefixing the key with kafka.consumer or             kafka.producer, you can set the table properties.Kafka storage handler and table propertiesYou use the Kafka storage handler and table properties to specify the query connection     and configuration.