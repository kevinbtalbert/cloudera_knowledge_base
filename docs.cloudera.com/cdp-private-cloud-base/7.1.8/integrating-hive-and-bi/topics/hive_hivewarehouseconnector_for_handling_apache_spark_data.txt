Introduction to HWCCloudera Docs
Introduction to HWC
 HWC securely accesses Hive managed tables from Spark. You need to use Hive Warehouse
    Connector (HWC) software to query Apache Hive managed tables from Apache Spark.
To read Hive external tables from Spark, you do not need HWC. Spark uses native Spark to
      read external tables. If you configure HWC to work with managed tables, you can use the same
      configuration to work with external tables. However, you must know that accessing external
      tables through HWC is slower as compared to accessing external tables through native Spark
      libraries.
Supported applications and operations
The Hive Warehouse Connector supports the following applications:
Spark 2 (based on Apache Spark version 2.4.8 in the current CDP release)
Spark 3 (through the CDS 3.3 Powered by Apache Spark add-on service)
Spark shell
PySpark
The spark-submit script
sparklyr
Zeppelin with the Livy interpreter

The following list describes a few of the operations supported by the Hive Warehouse Connector: 
Describing a table
Creating a table in ORC using .createTable() or in any format using .executeUpdate()
Writing to a pre-existing or new table in Parquet, ORC, AVRO, or Textfile formats
Selecting Hive data and retrieving a DataFrame
Writing a DataFrame to a Hive-managed ORC table in batch
Executing a Hive update statement
Reading table data, transforming it in Spark, and writing it to a new Hive table
Writing a DataFrame or Spark stream to Hive using HiveStreaming
Partitioning data when writing a DataFrame



Related informationHMS storageOrc vs ParquetCDS Powered by Apache Spark OverviewSet upYou need to know how to use the Hive Warehouse Connector (HWC) with different     programming languages and build systems. You find out where HWC binaries are located in CDP     parcels and how a Spark application consumes the binaries.HWC limitationsYou need to be aware of HWC limitations, including Kerberos properties that are not   allowed, and unsupported operations and connections. These limitations are in addition to Direct   Reader mode, JDBC mode, Secure access mode, and HWC and DataFrames API limitations.Reading data through HWCYou can configure one of the several HWC modes to read Apache Hive managed tables         from Apache Spark. You need to know about the modes you can configure for querying Hive from         Spark. Examples of how to configure the modes are presented. Writing data through HWCA step-by-step procedure walks you through connecting to HiveServer (HS2) to perform         batch writes from Spark, which is recommended for production. You configure HWC for the         managed table write, launch the Spark session, and write ACID, managed tables to Apache         Hive.Apache Spark executor task statisticsYou can view Spark executor task statistics, such as the read or write metrics when you   use Hive Warehouse Connector (HWC) to query Hive managed tables from Spark. These metrics enable   you to view information about running Spark executors and help in troubleshooting performance   issues.Introduction to HWC and DataFrame APIsAs an Apache Spark developer, you learn the code constructs for executing Apache Hive     queries using the HiveWarehouseSession API. In Spark source code, you see how to create an     instance of HiveWarehouseSession. You also learn how to access a Hive ACID table using     DataFrames.HWC integration pyspark, sparklyr, and Zeppelin