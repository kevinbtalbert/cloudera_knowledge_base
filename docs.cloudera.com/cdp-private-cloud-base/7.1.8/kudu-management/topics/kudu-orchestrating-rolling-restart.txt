Orchestrate a rolling restart with no downtimeCloudera Docs
Orchestrate a rolling restart with no downtime
Kudu 1.12 provides tooling to restart a cluster with no downtime. This topic provides
        the steps to perform rolling restart.

noteIf any tables in the cluster have a replication factor of 1, some quiescing tablet
                servers will never become fully quiesced, as single-replica tablets do not naturally
                relinquish leadership. If such tables exist, use the kudu cluster
                    rebalance tool to move replicas of these tables away from the quiescing
                tablet server by specifying the --ignored_tservers,
                    --move_replicas_from_ignored_tservers, and
                    --tables options.
noteIf running with rack awareness, the following steps can be performed by restarting
                multiple tablet servers within a single rack at the same time. Use
                    ksck to ensure that the location assignment policy is enforced
                while going through these steps, and that no more than a single location is
                restarted at the same time. At least three locations should be defined in the
                cluster to safely restart multiple tablet service within one location.
Cloudera Manager can automate this process, by using the “Rolling Restart” command on
                the Kudu service.
noteCloudera Manager does not support automatic moving of the single-replica
                tablets.
Cloudera Manager will prompt you to specify how many tablet servers to restart
                concurrently. If running with rack awareness with and at least three racks specified
                across all hosts that contain Kudu roles, it is safe to specify the restart batch
                with up to one rack at a time, provided the rack assignment policy is being
                enforced. 
The following service configurations can be set to tune the parameters the rolling
                restart will run with: 
Rolling Restart Health Check Interval: the interval in seconds that
                        Cloudera Manager will run ksck after restarting a batch of
                        tablet servers, waiting for the cluster to become healthy.
Maximum Allowed Runtime to Rolling Restart a Batch of Servers: the
                        total amount of time in seconds Cloudera Manager will wait for the cluster
                        to become healthy after restarting a batch of tablet servers, before exiting
                        with an error.



Restart the master(s) one-by-one. If there is only a single master, this may
                    cause brief interference with on-going workloads.

Starting with a single tablet server, put the tablet server into maintenance
                    mode by using the kudu tserver state enter_maintenance
                    tool.

Start quiescing the tablet server using the kudu tserver quiesce
                        start tool. This signals Kudu to stop hosting leaders on the
                    specified tablet server and to redirect new scan requests to other tablet
                    servers.

Periodically run kudu tserver quiesce start with the
                        --error_if_not_fully_quiesced option, until it returns
                    success, indicating that all leaders have been moved away from the tablet server
                    and that all on-going scans have completed.

Restart the tablet server.

Periodically run ksck until the cluster ireports a healthy
                    status.

Exit maintenance mode on the tablet server by running kudu tserver
                        state exit_maintenance. This allows new tablet replicas to be
                    placed on the tablet server.

Repeat these steps for all tablet servers in the cluster.


Related informationChanging directory configurationMinimize cluster disruption during temporary planned downtime of a single tablet serverIf a single tablet server is brought down temporarily in a healthy cluster, all tablets     will remain available and clients will function as normal, after potential short delays due to     leader elections. However, if the downtime lasts for more than --follower_unavailable_considered_failed_sec (default     300) seconds, the tablet replicas on the down tablet server will be replaced by new replicas on     available tablet servers. This will cause stress on the cluster as tablets re-replicate and, if     the downtime lasts long enough, significant reduction in the number of replicas on the down     tablet server, which may require the rebalancer to fix. 