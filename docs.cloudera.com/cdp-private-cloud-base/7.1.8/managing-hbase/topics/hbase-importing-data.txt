Importing data into HBaseCloudera Docs
Importing data into HBase
In different scenarios different methods can be used to import data into HBase. 
Choose the right import methodLearn about how to choose the right data import method.Use snapshotsA snapshot captures the state of a table at the time the snapshot was taken.Use CopyTableCopyTable uses HBase read and write paths to copy part or all of a     table to a new table in either the same cluster or a different cluster. Use BulkLoadIn many situations, writing HFiles programmatically with your data, and bulk-loading   that data into HBase on the RegionServer, has advantages over other data ingest   mechanisms.Use cluster replicationIf your data is already in an HBase cluster, replication is useful for getting the data   into additional HBase clusters.Use SqoopSqoop can import records into a table in HBase. It has an out-of-the-box support for         HBaseUse SparkYou can write data to HBase from Apache Spark using def             saveAsHadoopDataset(conf: JobConf): Unit.Use a custom MapReduce jobMany of the methods to import data into HBase use MapReduce implicitly. If none of those   approaches fit your needs, you can use MapReduce directly to convert data to a series of HFiles or   API calls for import into HBaseUse HashTable and SyncTable ToolHashTable/SyncTable is a two steps tool for synchronizing table data without copying all cells in a specified row key/time period range.