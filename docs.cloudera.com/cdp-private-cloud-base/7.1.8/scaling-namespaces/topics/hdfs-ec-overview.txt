Erasure coding overviewCloudera Docs
Erasure coding overview
Data durability describes how resilient data is to loss. When data is stored in HDFS,
    CDP provides two options for data durability. You can use replication, which HDFS was originally
    built on, or Erasure Coding (EC). 

noteThe comparisons between EC and replication use a replication factor of 3 (three copies
        of data are maintained) since that is the default.


Replication
HDFS creates two copies of data, resulting in three total instances of data. These
          copies are stored on separate DataNodes to guard against data loss when a node is
          unreachable. When the data stored on a node is lost or inaccessible, it is replicated from
          one of the other nodes to a new node so that there are always multiple copies. The number
          of replications is configurable, but the default is three. Cloudera recommends keeping the
          replication factor to at least three when you have three or more DataNodes. A lower
          replication factor leads to a situation where the data is more vulnerable to DataNode
          failures since there are fewer copies of data spread out across fewer DataNodes..When
            data is written to an HDFS cluster that uses replication, additional copies of the data
            are automatically created. No additional steps are required.Replication supports
            all data processing engines that CDP supports.
Erasure Coding (EC)
EC is an alternative to replication. When an HDFS cluster uses EC,
            no additional direct copies of the data are generated. Instead, data
            is striped into blocks and encoded to generate parity blocks. If
            there are any missing or corrupt blocks, HDFS uses the remaining
            data and parity blocks to reconstruct the missing pieces in the
            background. This process provides a similar level of data durability
            to 3x replication but at a lower storage cost. Additionally, EC
              is applied when data is written. This means that to use EC, you
              must first create a directory and configure it for EC. Then, you
              can either replicate existing data or write new data into this
              directory.EC supports the following data processing engines:
Hive
MapReduce
Spark


With both data durability schemes, replication and EC, recovery happens
        in the background and requires no direct input from a user.
HDFS clusters can be configured with a single data durability scheme (3x replication or
      EC), or with a hybrid data durability scheme where EC enabled directories co-exist on a
      cluster with other directories that are protected with the traditional 3x replication model.
      This decision should be based on the temperature of the data (how often the data is accessed)
      stored in HDFS. Hot data, data that is accessed frequently, should use replication. Cold data,
      data that is accessed less frequently, can take advantage of EC's storage savings.

Understanding erasure coding policiesThe EC policy determines how data is encoded and decoded. An EC policy is made up of     the following parts: codec-number of data blocks-number of parity blocks-cell size. Comparing replication and erasure codingYou must consider factors such as data temperature, i/o cost, storage cost, and file     size when comparing replication and erasure coding.Best practices for rack and node setup for ECIn a CDP Private Cloud Base deployment, when setting up a cluster to take advantage of EC,     consider the number of racks and nodes in your setup.Prerequisites for enabling erasure codingBefore enabling erasure coding on your data, you must consider various factors such as     the type of policy to use, the type of data, and the rack or node requirements.Limitations of erasure codingThe limitations of erasure coding include non-support of XOR codecs and certain HDFS         functions.Using erasure coding for existing dataYou must set a supported EC policy for a directory and copy the existing data to the     directory.Using erasure coding for new dataYou must create a new directory and then set a supported EC policy for the     directory.Using ISA-L for Erasure CodingIntel Intelligent Storage Acceleration Library (ISA-L) is an open-source collection of     optimized low-level functions used for storage applications. The library can improve Erasure     Coding performance when the Reed-Solomon (RS) codecs are used.Advanced erasure coding configurationYou can customize the behavior of EC through a combination of the hdfs       ec subcommand and the Cloudera Manager Admin     Console.Erasure coding CLI commandUse the hdfs ec command to set erasure coding policies on       directories.Erasure coding examplesYou can use the hdfs ec command with its various options to set         erasure coding policies on directories.Parent topic: Optimizing data storage