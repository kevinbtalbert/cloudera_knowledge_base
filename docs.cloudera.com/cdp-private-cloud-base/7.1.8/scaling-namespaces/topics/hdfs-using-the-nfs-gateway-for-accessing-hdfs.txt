Using the NFS Gateway for accessing HDFSCloudera Docs
Using the NFS Gateway for
    accessing HDFS
Available in a CDP Private Cloud Base deployment, the NFS Gateway for HDFS allows clients to
    mount HDFS and interact with it through NFS, as if it were part of their local file system. The
    Gateway supports NFSv3.
After mounting HDFS, a
      client user can perform the following tasks:  

Browse the HDFS file system through their local file system on NFSv3 client-compatible
        operating systems.
Upload and download files between the HDFS file system and their local file system.
Stream data directly to HDFS through the mount point. File append is supported, but random
        write is not supported.

Prerequisites for using NFS Gateway

The NFS Gateway machine must be running all components that are necessary for running an
          HDFS client, such as a Hadoop core JAR file and a HADOOP_CONF directory.
The NFS Gateway can be installed on any DataNode, NameNode, or CDP client machine. Start
          the NFS server on that machine.



Configure the NFS GatewayYou must ensure that the proxy user for the NFS Gateway can proxy all the users         accessing the NFS mounts. In addition, you must configure settings specific to the         Gateway.Start and stop the NFS Gateway servicesYou must start the following daemons to run the NFS services on the Gateway:          rpcbind (or portmap), mountd, and          nfsd. The NFS Gateway process includes both nfsd and          mountd. Although NFS Gateway works with portmap included       with most Linux distributions, you must use the portmap included in the NFS       Gateway package on some Linux systems such as SLES 11 and RHEL 6.2.Access HDFS from the NFS GatewayTo access HDFS, you must first mount the namespace and then set up the NFS client       hosts to interact with HDFS through the Gateway.