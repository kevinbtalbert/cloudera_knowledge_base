Spark integration with HiveCloudera Docs
Spark integration with Hive
You need to know a little about Hive Warehouse Connector (HWC) and how to find more
        information because to access Hive from Spark, you need to use HWC implicitly or explicitly. 
You can use the Hive Warehouse Connector (HWC) to access Hive managed tables from Spark.
            HWC is specifically designed to access managed ACID v2 Hive tables, and supports writing
            to tables in Parquet, ORC, Avro, or Textfile formats. HWC is a Spark library/plugin that
            is launched with the Spark app.
You do not need HWC to read from or write to Hive external tables. Spark uses native
            Spark to access external tables.
Use the Spark Direct Reader and HWC for ETL jobs. For other jobs, consider using Apache
            Ranger and the HiveWarehouseConnector library to provide row and column, fine-grained
            access to the data.
HWC supports spark-submit and pyspark. The spark thrift server is not supported.

Related informationHive Warehouse Connector for accessing Apache Spark dataSpark Direct Reader for accessing Spark dataParent topic: Apache Hive Post-Upgrade Tasks