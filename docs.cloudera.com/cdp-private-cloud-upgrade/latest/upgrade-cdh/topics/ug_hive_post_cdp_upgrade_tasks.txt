Apache Hive Post-Upgrade TasksCloudera Docs
Apache Hive Post-Upgrade Tasks
A successful upgrade requires performing a number of procedures that you can follow
  using step-by-step instructions. Important configuration tasks set up security on your cluster.
  You learn about semantic changes that might affect your applications, and see how to find your
  tables or move them. You find out about the Hive Warehouse Connector (HWC) to access files from
  Spark. 


Customizing critical Hive configurationsAs Administrator, you need property configuration guidelines. You need to know which   properties you need to reconfigure after upgrading. You must understand which the upgrade process   carries over from the old cluster to the new cluster. Setting Hive Configuration OverridesYou need to know how to configure the critical customizations that the upgrade         process does not preserve from your old Hive cluster. Referring to your records about your         old configuration, you follow steps to set at least six critical property values. Hive Configuration Requirements and RecommendationsYou need to set certain Hive and HiveServer (HS2) configuration properties after     upgrading. You review recommendations for setting up CDP Private Cloud Base for your needs, and understand which configurations remain unchanged after upgrading, which     impact performance, and default values.Configuring HiveServer for ETL using YARN queuesYou need to set several configuration properties to allow placement of the Hive         workload on the Yarn queue manager, which is common for running an ETL job. You need to set         several parameters that effectively disable the reuse of containers. Each new query gets new         containers routed to the appropriate queue.Removing Hive on Spark ConfigurationsYour scripts, or queries, include the Hive on Spark configuration, which is no longer         supported, and you must know how to recognize and remove these configurations.Configuring authorization to tablesAlthough the upgrade process makes no change to the location of external tables, you need         to set up access to external tables in HDFS. If you choose the recommended Ranger security model for     authorization, you need to set up policies and configure Hive metastore (HMS).Making the Hive plugin for Ranger visibleAfter upgrading from HDP or CDH clusters to CDP, the Hive plugin for the Hive         Metastore and HiveServer2 appears in the Ranger Admin UI unless configuration property         problems due to upgrading exist. You can rectify the incorrect properties to fix the         problem. Setting up access control listsSeveral sources of information about setting up HDFS ACLS plus a brief Ranger overview   and pointer to Ranger information prepare you to set up Hive authorization.Configure encryption zone security Under certain conditions, you as Administrator, need to perform a security-related         task to allow users to access to tables stored in encryption zones. You find out how to         prevent access problems to these tables.Configure edge nodes as gateways If you use command-line clients, such as Sqoop, to access Hive, you must configure         these gateways to use defaults for your service. You can accomplish this task in a few         steps.Spark integration with HiveYou need to know a little about Hive Warehouse Connector (HWC) and how to find more         information because to access Hive from Spark, you need to use HWC implicitly or explicitly. Configure HiveServer HTTP modeIf you use Knox, you might need to change the HTTP mode configuration. If you         installed Knox on CDP Private Cloud Base and want to proxy HiveServer         with Knox, you need to change the default HiveServer transport mode         (hive.server2.transport.mode).         Configuring HMS for high availability To provide failover to a secondary Hive metastore if your primary instance goes         down, you need to know how to add a Metastore role in Cloudera Manager and configure a         property.Installing Hive on Tez and adding a HiveServer roleCloudera Runtime (CR) services include Hive on Tez and Hive Metastore (HMS). Hive on         Tez is a SQL query engine using Apache Tez that performs the HiveServer (HS2) role in a         Cloudera cluster. You need to install Hive on Tez and HMS in the correct order; otherwise,         HiveServer fails. You need to install additional HiveServer roles to Hive on Tez, not the         Hive service; otherwise, HiveServer fails.Updating Hive and Impala JDBC/ODBC driversAfter upgrading, Cloudera recommends that you update your Hive and Impala JDBC and         ODBC drivers. You follow a procedure to download a driver.Parent topic: CDH 5 to CDP Private Cloud Base post-upgrade transition steps