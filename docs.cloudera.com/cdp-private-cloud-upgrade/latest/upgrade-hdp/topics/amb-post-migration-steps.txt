Post transition stepsCloudera Docs
Post transition steps
You must complete the post transition steps to start the services in CDP Private Cloud Base.
The AM2CM tool transitions the component configurations. However, you must configure and
      perform additional steps to start the services in CDP Private Cloud Base. 
note

Review configuration warnings for all the services.
Review JVM parameters and configurations for all services as some of the JVM parameters
          and configurations are not transitioned. Most of the heap configurations are
          transitioned.
Review the Log4j configurations. Log4j configurations
          such as logs dir, size, and backup index are transitioned to Cloudera Manager as it uses
          the default log4j settings of Cloudera Manager.


noteIf the cluster is kerberized, you can generate keytabs in Cloudera Manager under
      Security section ->Kerberos Credentials ->Â Generate Missing
        Credentials

Enable Auto Start settingEnsure that you enable Auto Start Settings in Cloudera Manager         for all the components.ZooKeeperYou must complete the following steps to start the ZooKeeper service. Delete ZNODESRemove the ZNode for HDFS, YARN, Hive, HBase, and OozieRangerYou must change the port number for Ranger port numbers on Cloudera         Manager.Ranger KMSYou must add the key and value to Ranger KMS property. This procedure is not required         if you are on Cloudera Manager 7.4.4 and above and CDP 7.1.5 and above.Add Ranger policies for components on the CDP ClusterEnable the default ranger policies on the CDP Private Cloud Base cluster.Ranger Installation in High Availability with Load BalancerFor clusters that have multiple users and production availability requirements, you         may want to configure Ranger high availability (HA) with a load-balancing proxy server to         relay requests to and from Ranger.Set maximum retention days for Ranger auditsYou can now update the solr document expiry             ranger.audit.solr.config.ttl and             ranger.audit.solr.config.delete.trigger parameters from Ranger         configurations in Cloudera Manager and refresh the configurations to get the Solr collection         for Ranger audits updated with ttl and delete trigger.HDFSPerform the following post migration steps. SolrYou must create a Ranger Plug-in audit directory and HDFS Home directory and start the Solr service.KafkaYou must cleanup metadata on broker hosts after migrating the Ambari-managed HDP     cluster to CDP Private Cloud Base. If you are upgrading to CDP Private     Cloud Base 7.1.7, you can ignore step 3, step 4, and step 5.YARNPerform the following post migration steps.  SparkYou must initialize a few directories for the Spark service.TezInstall the Tez tar files on HDFS.HivePerform the post migration steps. If you are expediting the Hive upgrade process and     modified the upgrade process to skip materializing every table in the metastore, you need to     modify the Hive Strict Metastore Migration (HSMM) process by running the Hive Upgrade Check tool     and provided scripts. HBasePost-migration, Atlas hook for HBase is not enabled by default on AM2CM migrated         clusters. you must manually enable Atlas hook.Installing dependencies for HueHue in CDP 7.1.8 and higher uses Python 3. You must install Python 3.8 on all the Hue     hosts. Additionally, you must install psycopg2 package for PostgreSQL-backed Hue and MySQL     clients for MariaDB and MySQL databases depending on your operating systems.OoziePerform the following post migration tasks by validating the database URL, installing     the new shared libraries, accessing Oozie Loadbalancer URL, and configuring load     balancer.Atlas advanced configuration snippet (Safety valve)If non-default user names are used, then you must set this parameter. This setting is         applicable only if you are upgrading from HDP 2.6.5 to CDP Private Cloud Base 7.1.6. This         section is not applicable if you are upgrading to CDP Private Cloud Base 7.1.7.Migrating Atlas dataAfter Atlas is available on the CDP Private Cloud Base cluster,         you must import the Atlas data that the Atlas migration exporter utility exported from the         HDP 2.6.5.x cluster.PhoenixAdd the Apache Phoenix service. If you are using Phoenix Query Server (PQS) in your         source HDP deployment, you must manually add the Apache Phoenix service using Cloudera         Manager to complete the Phoenix upgrade.Starting all servicesYou must now start all the services. Hive Policy AdditionsHive Metastore Canary test might fail on clusters migrated from HDP because the service     account used to perform these tests is different between the two management     consoles.KnoxThe following post migration steps are optional. Client ConfigurationsMigrating from Ambari to Cloudera Manager can leave the Ambari-managed HDP artifacts     and links that may not have changed. After everything has been configured, deploy Client     Configuration [Cloudera Manager > Clusters >     Actions > Deploy Client Configurations]. This     fixes any missing configuration references in the cluster. Securing ZooKeeperBy default, the AM2CM tool adds the -Dzookeeper.skipACL=yes     configuration to assist with the migration. You must remove the     -Dzookeeper.skipACL=yes configuration under Java Configuration       Options for Zookeeper Service to secure ZooKeeper and restart the     service.Zeppelin Shiro configurationsIf you had Zeppelin Shiro configurations in the HDP cluster, then you must configure         them manually on the CDP Private Cloud Base cluster. Migrating Spark workloads to CDPMigrating Spark workloads from CDH or HDP to CDP involves learning the Spark semantic     changes in your source cluster and the CDP target cluster. You get details about how to handle     these changes. Apache Hive Expedited Migration TasksIf you chose to expedite the Hive upgrade process by postponing migration of your tables   and databases, you need to identify any problems in tables and get help with fixing those problems   before migrating the tables to CDP. You then need to migrate these tables to CDP before you can   use them.Apache Hive Changes in CDPYou need to know where your tables are located and the property changes that the upgrade   process makes. You need to perform some post-migration tasks before using Hive tables and handle   semantic changes.Apache Hive Post-Upgrade TasksA successful upgrade requires performing a number of procedures that you can follow   using step-by-step instructions. Important configuration tasks set up security on your cluster.   You learn about semantic changes that might affect your applications, and see how to find your   tables or move them. You find out about the Hive Warehouse Connector (HWC) to access files from   Spark. Parent topic: Transitioning to Cloudera Manager