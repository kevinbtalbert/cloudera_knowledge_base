Apache Hive Changes in CDPCloudera Docs
Apache Hive Changes in CDP
You need to know where your tables are located and the property changes that the upgrade
  process makes. You need to perform some post-migration tasks before using Hive tables and handle
  semantic changes.
 Understanding Apache Hive 3 major design features, such as default ACID transaction
   processing, can help you use Hive to address the growing needs of enterprise data warehouse
   systems.  

Hive Configuration Property ChangesYou need to know the property value changes made by the upgrade process as the change     might impact your work. You might need to consider reconfiguring property value defaults that     the upgrade changes.  Customizing critical Hive configurationsAs Administrator, you need property configuration guidelines. You need to know which   properties you need to reconfigure after upgrading. You must understand which the upgrade process   carries over from the old cluster to the new cluster. Setting Hive Configuration OverridesYou need to know how to configure the critical customizations that the upgrade         process does not preserve from your old Hive cluster. Referring to your records about your         old configuration, you follow steps to set at least six critical property values. Hive Configuration Requirements and RecommendationsYou need to set certain Hive and HiveServer (HS2) configuration properties after     upgrading. You review recommendations for setting up CDP Private Cloud Base for your needs, and understand which configurations remain unchanged after upgrading, which     impact performance, and default values.Removing the LLAP QueueBefore upgrading from HDP to CDP, if you used LLAP, a YARN interactive query queue         was created. This queue is carried over to your CDP cluster. You must remove this queue,         likely named llap, after the upgrade. Configuring HiveServer for ETL using YARN queuesYou need to set several configuration properties to allow placement of the Hive         workload on the Yarn queue manager, which is common for running an ETL job. You need to set         several parameters that effectively disable the reuse of containers. Each new query gets new         containers routed to the appropriate queue.Configuring authorization to tablesAlthough the upgrade process makes no change to the location of external tables, you need         to set up access to external tables in HDFS. If you choose the recommended Ranger security model for     authorization, you need to set up policies and configure Hive metastore (HMS).Updating Hive and Impala JDBC/ODBC driversAfter upgrading, Cloudera recommends that you update your Hive and Impala JDBC and         ODBC drivers. You follow a procedure to download a driver.Setting up access control listsSeveral sources of information about setting up HDFS ACLS plus a brief Ranger overview   and pointer to Ranger information prepare you to set up Hive authorization.Configure encryption zone security Under certain conditions, you as Administrator, need to perform a security-related         task to allow users to access to tables stored in encryption zones. You find out how to         prevent access problems to these tables.Renaming tablesTo harden the system, Hive data can be stored in HDFS encryption zones. RENAME has been   changed to prevent moving a table outside the same encryption zone or into a no-encryption   zone.Configure edge nodes as gateways If you use command-line clients, such as Sqoop, to access Hive, you must configure         these gateways to use defaults for your service. You can accomplish this task in a few         steps.Configure HiveServer HTTP modeIf you use Knox, you might need to change the HTTP mode configuration. If you         installed Knox on CDP Private Cloud Base and want to proxy HiveServer         with Knox, you need to change the default HiveServer transport mode         (hive.server2.transport.mode).         Configuring HMS for high availability To provide failover to a secondary Hive metastore if your primary instance goes         down, you need to know how to add a Metastore role in Cloudera Manager and configure a         property.Installing Hive on Tez and adding a HiveServer roleCloudera Runtime (CR) services include Hive on Tez and Hive Metastore (HMS). Hive on         Tez is a SQL query engine using Apache Tez that performs the HiveServer (HS2) role in a         Cloudera cluster. You need to install Hive on Tez and HMS in the correct order; otherwise,         HiveServer fails. You need to install additional HiveServer roles to Hive on Tez, not the         Hive service; otherwise, HiveServer fails.Handling table reference syntaxFor ANSI SQL compliance, Hive 3.x rejects `db.table` in SQL queries as described by the   Hive-16907 bug fix. A dot (.) is not allowed in table names. As a Data Engineer, you need to   ensure that Hive tables do not contain these references before migrating the tables to CDP, that   scripts are changed to comply with the SQL standard references, and that users are aware of the   requirement. Unsupported Interfaces and FeaturesYou need to know the interfaces available in HDP or CDH     platforms that are no longer supported in CDP. Some features you might have used are also unsupported.Changes to HDP Hive tablesAs a Data Scientist, Architect, Analyst, or other Hive user you need to locate and use     your Apache Hive 3 tables after an upgrade. The upgrade process from HDP 3.1.5 to CDP makes no     changes in the location of tables.Parent topic: Post transition steps