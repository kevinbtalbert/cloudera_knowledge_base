How replication policies workCloudera Docs
How replication policies work
In CDP Public Cloud
      Replication Manager, you create replication policies to establish the rules you want applied
      to your replication jobs. The policy rules you set can include which cluster is the source and
      which is the destination, what data is replicated, what day and time the replication job
      occurs, the frequency of job runs, and bandwidth restrictions.
The first time you run a job (an instance of a policy) with data that has not been
         previously replicated, Replication Manager creates a new folder or database and bootstraps
         the data. During a bootstrap operation, all data is replicated from the source cluster to
         the destination. As a result, the initial execution of a job can take a significant amount
         of time, depending on how much data is being replicated, network bandwidth, and so on. So
         you should plan the bootstrap operation accordingly.
After the bootstrap operation succeeds, an incremental replication is automatically
         performed for data replication. This job synchronizes, between the source and destination
         clusters, any events that occurred during the bootstrap process. After the data is
         synchronized, the replicated data is ready for use on the destination. Data is in a
         consistent state only after incremental replication has captured any new changes that
         occurred during bootstrap.
Subsequent replication jobs from the same source location to the same target on the
         destination are incremental data replication, so only the changed data is copied. 
When a bootstrap operation is interrupted, such as due to a network failure or an
         unrecoverable error, the Replication Manager does not retry the job instead it runs the job
         at the next scheduled interval, if available. Therefore, if the bootstrap operation is
         interrupted, you must manually correct the issue and then run the policy.
When scheduling how often you want a replication job to run, you should consider the
         recovery point objective (RPO) of the data being replicated; that is, what is the
         acceptable lag time between the active site and the replicated data on the destination.
Replication policy considerations
You should take into consideration certain guidelines when creating or
            modifying a replication policy. It is important for you to understand the security
            restrictions and encryption policies within Replication Manager.
The guidelines you need to consider before you create or modify a replication
            policy includes:

Data security
To use an S3 or ABFS cluster for your policy, register your credentials on the
                     Cloud Credentials page. A user with access to the
                  Replication Manager user interface has the ability to browse, within the
                  Replication Manager UI, the folder structure of any clusters enabled for
                  Replication Manager.
Therefore, users can view folders, files, and databases in the Replication
                  Manager user interface that they might not have access to in HDFS. Users cannot
                  view from the Replication Manager UI the content of files on the source or
                  destination clusters. Nor do these administrators have the ability to modify or
                  delete folders or files that are viewable from the Replication Manager UI.
Policy properties and settings
Consider the recovery point objective (RPO) of the data being replicated when you
                  schedule a replication policy. The RPO is the acceptable lag time between the
                  active site and the replicated data on the destination. Ensure that the frequency
                  is set so that a job finishes before the next job starts.
Jobs based on the same policy cannot overlap. If a job is not completed before
                  another job starts, the second job does not execute and is given the status
                  Skipped. If a job is consistently skipped, you might need to modify the frequency
                  of the job.
Specify bandwidth per map, in MBps. Each map is restricted to consume only the
                  specified bandwidth. This is not always exact. The map throttles back its
                  bandwidth consumption during a copy in such a way that the net bandwidth
                  used tends towards the specified value.
Cluster requirements


Pair the clusters before you include them in a replication
                        policy.
With a single cluster, you can replicate data on-premises to
                        cloud.
With a single cluster, you cannot replicate data on-premises to
                        on-premises.
If the clusters are Replication Manager-enabled, it appears in
                        the Source Cluster or Destination or Data Lake Cluster fields in the Create
                        Policy wizard. You must ensure that the clusters you select are healthy
                        before you start a policy instance (job).


Hive restrictions


When creating a schedule for a Hive replication policy, you
                        should set the frequency so that changes are replicated often enough to
                        avoid overly large copies.
ACID tables, managed tables, storage handler-based tables such as
                        Apache HBase, and column statistics are not replicated. ACID tables and
                        managed tables are converted to external tables after replication.





HDFS replication policiesYou can use the HDFS replication policies in CDP Public Cloud Replication Manager to         replicate HDFS data. The HDFS replication policies can replicate HDFS data and metadata from         classic clusters (CDH, CDP Private Cloud Base, and HDP) to CDP Public Cloud storage buckets         such as S3 and ABFS, and from cloud storage to classic clusters (CDH or CDP Private Cloud         Base clusters). To use an on-premises cluster (CDH or CDP Private Cloud Base cluster) in the         replication policy, you must register it as a classic cluster in the Management Console. To         use the cloud storage for data replication, you must register the cloud credentials in         Replication Manager so that the Replication Manager service can access the cloud storage.         You must also verify cluster access and configure minimum ports for replication before you         create HDFS replication policies.Hive replication policiesYou can create a Hive replication policy in CDP Public Cloud Replication Manager         after you configure the required Ranger policy in Ranger, register the on-premises cluster         (CDH or CDP Private Cloud Base) as a classic cluster in Management Console, register cloud         account credentials in the Replication Manager service, verify cluster access, and configure         minimum ports for replication. The replication load happens on the source on-premises         cluster. You can replicate data on-premises to the cloud with a single cluster if the         Metastore is running on the cloud.HBase replication policyYou can replicate HBase and Phoenix tables using HBase replication policies in CDP         Public Cloud Replication Manager. An HBase replication policy replicates the data at         table-level granularity. After you create an HBase replication policy, you can delete one or         more tables from the policy. Parent topic: Use Replication Manager to migrate to CDP Public Cloud