Number of drivesCloudera Docs
Number of drives
Traditionally, Hadoop has been thought of as a large I/O platform. While there are many
  new types of workloads being run on Cloudera clusters that may not be as I/O bound as traditional
  MapReduce applications, it is still useful to consider the I/O performance when designing a
  Cloudera cluster.
Unlike the number of cores in a CPU and the density of RAM, the speed at which data
   can be read from a spinning hard drive (spindle) has not changed much in the past 10 years. To
   counter the limited performance of hard drive read/write operations, Hadoop reads and writes from
   many drives in parallel. Every additional spindle added to a node increases the overall
   read/write speed of the cluster. 
noteSSDs have dramatically changed the persistent storage performance
   landscape, but the price per GB of spinning disks is still significantly less than that of SSD
   storage. As SSDs come down in cost and technologies such as Intelâ€™s Optane<tm> enter the
   market, workloads may swing back towards being CPU bound. Most Cloudera customers are still
   deploying clusters that store data on spinning hard disks.
Additional drives also come with the likelihood of more network traffic in the
   cluster. For the majority of cases, network traffic between nodes is generally limited by how
   fast data can be written to or read from a node. Therefore, the rule normally follows that, with
   more drives network speed requirements increase.
Generally speaking, the more drives a node has, the lower the cost per TB. However, the larger
   the quantity of data stored on one node, the longer the re-replication time if that node goes
   down. Hadoop clusters are designed to have many nodes. It is generally better to have more
   average nodes than fewer super nodes. This has a lot to do with both data protection, as well as
   increased parallelism for distributed computation engines such as MapReduce and Spark.
Lastly, the number of drives per node impacts the number of YARN containers
   configured for a node. YARN configuration and performance tuning is a complicated topic, but for
   I/O bound applications, the number of physical drives per host may be a limiting factor in
   determining the number of container slots configured per node.
Kafka clusters are often run on dedicated servers that do not run HDFS data nodes or
   processing components such as YARN and Impala. Because Kafka is a message-based system, fast
   storage and network I/O is critical to performance. Although Kafka does persist messages to disk,
   it is not generally necessary to store the entire contents of a Kafka topic log on the Kafka
   cluster indefinitely. Kafka brokers should be configured with dedicated spinning hard drives for
   the log data directories. Using SSDs instead of spinning disks has not shown to provide a
   significant performance improvement for Kafka.
Kafka drives should also be configured as RAID 10 because the loss of a single drive
   on a Kafka broker can cause the broker to experience an outage.

Related informationIntel Optane Technology for Data CentersYARN tuning overviewParent topic: Cluster hardware selection best practices