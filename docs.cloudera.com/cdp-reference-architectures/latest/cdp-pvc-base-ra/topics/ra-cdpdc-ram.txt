RAMCloudera Docs
RAM
More memory is always good and it is recommended to purchase as much as the budget
  allows. Applications such as Impala and Cloudera Search are often configured to use large amounts
  of heap, and a mixed workload cluster supporting both services should have sufficient RAM to allow
  all required services.
importantAllocate at least 4 GB memory for the operating
   system and other non-Hadoop use (although this amount may need to be higher if additional
   non-Hadoop applications are running on the cluster nodes, such as third-party active
   monitoring/alerting tools). The more services you run, the more memory is required; you need to
   use more capable hosts to accommodate these needs.
It is critical to performance that the total memory allocated to all Hadoop-related
   processes (including processes such as HBase) is less than the total memory on the node, taking
   into account the operating system and non-Hadoop processes. Oversubscribing the memory on a
   system can lead to the Linux kernelâ€™s out-of-memory process killer being invoked and important
   processes being terminated. Performance might be affected by over-allocation of memory to a
   Hadoop process, as this can lead to long Java garbage collection pauses. For processes such as
   HBase, you must factor in aspects such as off heap bucket cache configuration. 
For optimum performance, confer with your hardware vendor for defining optimal memory
   configuration layout.
While 128 GB RAM can be accommodated, this typically constrains the amount of memory
   allocated to services such as YARN and Impala, thereby reducing the query capacity of the
   cluster. 256 GB RAM is recommended with higher values also possible.

Related informationOff-heap BucketCacheParent topic: Cluster hardware selection best practices