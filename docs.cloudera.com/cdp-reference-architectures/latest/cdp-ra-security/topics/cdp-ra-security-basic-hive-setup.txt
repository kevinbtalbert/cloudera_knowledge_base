Basic hive setupCloudera Docs
Basic hive setup
In our scenario there are multiple tenants on a shared cluster. It is assumed that there
  are a number of encryption zones within the hdfs namespace supporting a variety of user cases. 
This will include Travel data with policy restrictions according to EU GDPR regulation
   requiring masking and encryption of policy data as well as internal public cloud usage data which
   summarizes energy consumption and can be reported as part of the sustainability initiative.
   Elements of the sustainability data are considered as corporately sensitive, therefore access is
   limited to certain tables.

Encryption at restThe company has a policy of having all Commercially Sensitive data, encrypted at rest,   and therefore according to the data classification, the Climate Accounting data will be stored in   the Finance encryption zone on the cluster. For the purposes of regulatory reporting the   provenance of the data must be assured to the highest corporate standards. Therefore, encryption   at rest is mandatory due to regulatory requirements.Cluster storage and Ranger mapping serviceRanger Mapping service adds a feature that was available to legacy CDH users that   automatically maps Hive Object policies to the underlying storage ACLs, this allows you to   authorize access to HDFS directories and files using policies defined for Hive tables, thus   simplifying protective access to the underlying data.Create databaseIn this example we will use a HIVE ACID table as we expect our table to feature a number   of slowly changing dimensions such as mileage and data center utility rates. Furthermore we donâ€™t   expect significant transaction volumes or no of Hive Objects such as partitions on our clusters   that could impact performance due to compactions.Create policiesCreate basic permissions for the DB with two roles: Full Access (Finance) and Read   Only/Limited Access (Cloudera Staff) with Ranger Policies.