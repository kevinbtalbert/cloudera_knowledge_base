ORC file formatCloudera Docs
ORC file format
You can conserve storage in a number of ways, but using the Optimized Row Columnar
    (ORC) file format for storing Apache Hive data is most effective. ORC is the default storage for
    Hive
    data.
The ORC file format for Hive data storage is recommended for the following reasons: 

Efficient compression: Stored as columns and compressed, which leads to smaller disk
        reads. The columnar format is also ideal for vectorization optimizations. 
Fast reads: ORC has a built-in index, min/max values, and other aggregates that cause
        entire stripes to be skipped during reads. In addition, predicate pushdown pushes filters
        into reads so that minimal rows are read. And Bloom filters further reduce the number of
        rows that are returned. 
Proven in large-scale deployments: Facebook uses the ORC file format for a 300+ PB
        deployment. 



ORC provides the best Hive performance overall. In addition, to specifying the storage
      format, you can also specify a compression algorithm for the table, as shown in the following
      example:
CREATE TABLE addresses (
 name string,
 street string,
 city string,
 state string,
 zip int
 ) STORED AS orc TBLPROPERTIES ("orc.compress"="Zlib");
Setting the compression algorithm is usually not required because your Hive settings include
      a default algorithm. Using ORC advanced properties, you can create bloom filters for columns
      frequently used in point lookups.
Hive supports Parquet and other formats for insert-only ACID tables and external tables. You
      can also write your own SerDes (Serializers, Deserializers) interface to support custom file
      formats.

