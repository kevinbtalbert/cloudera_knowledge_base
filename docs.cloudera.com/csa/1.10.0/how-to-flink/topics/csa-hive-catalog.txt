Hive catalogCloudera Docs
Hive catalog
You can add Hive as a catalog in Flink SQL by adding Hive dependency to your project,
    registering the Hive table in Java and setting it either globally in Cloudera Manager or the
    custom environment file.
The Hive catalog serves two purposes:
It is a persistent storage for pure Flink metadata
It is an interface for reading and writing existing Hive tables

Maven
      Dependency<dependency>
   <groupId>org.apache.flink</groupId>
   <artifactId>flink-connector-hive_2.12</artifactId>
   <version>1.16.1-csa1.10.0.0</version>
</dependency>

The following example shows how to register and use the Hive catalog from
   Java:String HIVE = "hive";
String DB = "default";
String HIVE_CONF_DIR = "/etc/hive/conf";
String HIVE_VERSION = "3.1.3000";

HiveCatalog catalog = new HiveCatalog(HIVE, DB, HIVE_CONF_DIR, HIVE_VERSION);
tableEnv.registerCatalog(HIVE, catalog);
tableEnv.useCatalog(HIVE);


noteAccording to the latest recommended setup on CDP, the Hive service hosts only the Hive
        Metastore Server. Make sure that a Gateway role is installed, or the HMS itself is deployed
        on the node where the Flink commands are submitted.
noteDo not use Flink to create general purpose batch tables in the Hive metastore that you
        expect to be used from other SQL engines. While these tables will be visible, Flink uses the
        additional properties extensively to describe the tables, and thus other systems might not
        be able to interpret them. Use Hive directly to create these tables instead.


Parent topic: SQL catalogs for Flink