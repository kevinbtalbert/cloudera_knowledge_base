ConnectorsCloudera Docs
Connectors
SQL Stream Builder (SSB) supports different connector types and data formats for Flink
  SQL tables to ease development and access to all kinds of data sources.
The following table summarizes the supported connectors and how they can be used in SSB:

Connector
Type
Description



Kafka
source/sink
Supported as exactly-once-sink


Hive
source/sink
Can be used as catalog


Kudu
source/sink
Can be used as catalog


Schema Registry
source/sink
Can be used as catalog


JDBC
source/sink
Can be used with Flink SQL. PostgreSQL, MySQL and Hive are supported.


Filesystems
source/sink
Filesystems such as HDFS, S3 and so on. Can be used with Flink SQL


Debezium CDC
source
Can be used with Flink SQL. PostgreSQL, MySQL, Oracle DB, Db2 and SQL Server are
        supported.


Webhook
sink
Can be used as HTTP POST/PUT with templates and headers


PostgreSQL
sink
Materialized View connection for reading views. Can be used with anything that reads
        PostgreSQL wire protocol


REST
sink
Materialized View connection for reading views. Can be used with anything that reads
        REST (such as notebooks, applications, and so on)


BlackHole
sink
Can be used with Flink SQL. 



Using connectors with templatesSome of the connectors have default templates in Streaming SQL Console that allows         you to create tables with them easily. These predefined templates not only contain the         CREATE TABLE statement, but every mandatory and optional argument that is needed for the         table.Adding new connectorsWhen adding new connectors to SQL Stream Builder, you need to specify the property         list, data types and must upload the connector JAR file to the Console.Kafka connectorsWhen using the Kafka connector, you can choose between using an internal or external   Kafka service. Based on the connector type you choose, there are mandatory fields where you must   provide the correct information. CDC connectorsYou can use the Debezium Change Data Capture (CDC) connector to stream changes in     real-time from MySQL, PostgreSQL, Oracle, Db2, SQL Server and feed data to Kafka, JDBC, the     Webhook sink or Materialized Views using SQL Stream Builder (SSB).JDBC connectorWhen using the JDBC connector, you can choose between using a PostgreSQL, MySQL or Hive   databases. Based on the connector type you choose, there are mandatory fields where you must   provide the correct information. Filesystem connectorWhen using the Filesystem connector, you can choose between HDFS, S3 and so on storage   systems. Based on the connector type you choose, there are mandatory fields where you must provide   the correct information. Datagen connectorThe Datagen connector can be used as a source to generate random data. The Datagen   connector works out of the box, no mandatory field is required to use the connector. The Datagen   connector is useful when you want to experiment and try out SQL Stream Builder or to test your SQL   queries using random data.Faker connectorThe Faker connector can be used as a source to generate random data. To use the Faker   connector, you need to specify the usage The Datagen connector is useful when you want to   experiment and try out SQL Stream Builder or to test your SQL queries using random   data.Blackhole connectorThe Blackhole connector can be used as a sink where you can write any type of data into.   The Blackhole connector works out of the box, no mandatory field is required to use the connector.   The Blackhole connector is useful when you want to experiment and try out SQL Stream Builder or to   test your SQL queries using random data.