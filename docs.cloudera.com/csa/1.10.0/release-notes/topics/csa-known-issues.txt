Known issues and limitationsCloudera Docs
Known issues and limitations
Learn about the known issues in Flink and SQL Stream Builder, the impact or changes to
    the functionality, and the workaround in Cloudera Streaming Analytics 1.10.0.
SQL Stream Builder


Limitations when configuring widgets
The following widget configuration optionns are not available
            for certain widgets on Streaming SQL Console:
Gauge visualization type: Expand on hover, Unit
Donut visualization type: Expand on hover, Title
Pie visualization type: Expand on hover

None


Upgrading to CSA 1.10 requires additional step
When upgrading to Streaming Analytics 1.10, the Update Admin
            Database step will fail during the upgrade process.
Select Repair Admin Database when
            the Update Admin Database process fails, and then complete the
              Update Admin Database process again.


Auto discovery is not supported for Apache Knox
You need to manually configure Knox with SQL Stream Builder to
            enable Knox authentication.
Complete the configuration based on the CDP Private Cloud
            Base version you use. For more information, see the Enabling Knox authentication for SSB
            documentation.


SSB service fails when using Active Directory (AD) Kerberos
            authentication
If you use AD Kerberos for authentication and the Load Balancer
            URL is not provided, it can cause the SQL Stream Builder (SSB) service to fail. The
            issue is caused by the keytab generation. When the keytab is generated by Cloudera
            Manager it requires the principals from the AD for the Load Balancer host, and without
            no host specified for the Load Balancer, the SSB service cannot be started by Cloudera
            Manager. This issue also persists when the Load Balancer role is not deployed or used
            with SSB.
Fill out the Load Balancer URL parameter in Cloudera
            Manager regardless of using Load Balancer with SSB. For more information, see the Enabling High Availability for SSB
            documentation.



Flink
In Cloudera Streaming Analytics, the following SQL API features are in preview:
Match recognize
Top-N
Stream-Table join (without rowtime input)


DataStream conversion limitations


Converting between Tables and POJO DataStreams is currently not supported in
                CSA.
Object arrays are not supported for Tuple conversion.
The java.time class conversions for Tuple DataStreams are only
                supported by using explicit TypeInformation:
                  LegacyInstantTypeInfo,
                  LocalTimeTypeInfo.getInfoFor(LocalDate/LocalDateTime/LocalTime.class).
Only java.sql.Timestamp is supported for rowtime conversion,
                  java.time.LocalDateTime is not supported.




Kudu catalog limitations


CREATE TABLE
Primary keys can only be set by the kudu.primary-key-columns
                    property. Using the PRIMARY KEY constraint is not yet
                    possible.
Range partitioning is not supported.

When getting a table through the catalog, NOT NULL and
                  PRIMARY KEY constraints are ignored. All columns are described as
                being nullable, and not being primary keys.
Kudu tables cannot be altered through the catalog other than simply renaming
                them.




Schema Registry catalog limitations


Currently, the Schema Registry catalog / format only supports reading messages
                with the latest enabled schema for any given Kafka topic at the time when the SQL
                query was compiled.
No time-column and watermark support for Registry tables.
No CREATE TABLE support. Schemas have to be registered directly
                in the SchemaRegistry to be accessible through the catalog.
The catalog is read-only. It does not support table deletions or
                modifications.
By default, it is assumed that Kafka message values contain the schema id as a
                prefix, because this is the default behaviour for the
                  SchemaRegistry Kafka producer format. To consume messages with
                schema written in the header, the following property must be set for the Registry
                  client: store.schema.version.id.in.header: true.





