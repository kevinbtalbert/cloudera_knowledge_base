Best practices for building Apache Spark applicationsCloudera Docs
Best practices for building Apache Spark applications

 Follow these best practices when building Apache Spark Scala and Java
      applications: 

 Compile your applications against the same version of Spark that
        you are running. 


            Build a single assembly JAR ("Uber" JAR) that includes all dependencies. In Maven, add
            the Maven assembly plug-in to build a JAR containing all dependencies:
          
<plugin>
  <artifactId>maven-assembly-plugin</artifactId>
  <configuration>
    <descriptorRefs>
      <descriptorRef>jar-with-dependencies</descriptorRef>
    </descriptorRefs>
  </configuration>
  <executions>
    <execution>
      <id>make-assembly</id>
      <phase>package</phase>
      <goals>
        <goal>single</goal>
      </goals>
    </execution>
  </executions>
</plugin>
 This plug-in manages the merge procedure for all available JAR
          files during the build. Exclude Spark, Hadoop, and Kafka classes from
          the assembly JAR, because they are already available on the cluster
          and contained in the runtime classpath. In Maven, specify Spark,
          Hadoop, and Kafka dependencies with scope provided.
          For example: 
<dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_2.11</artifactId>
  <version>2.4.0.7.0.0.0</version>
  <scope>provided</scope>
</dependency>



