Creating jobs in Cloudera Data EngineeringCloudera Docs
Creating jobs in Cloudera Data Engineering
A job in Cloudera Data Engineering (CDE) consists of defined
    configurations and resources (including application code). Jobs can be run
    on demand or scheduled.

In Cloudera Data Engineering (CDE), jobs are associated with virtual
        clusters. Before you can create a job, you must create a virtual cluster
        that can run it. For more information, see Creating virtual clusters.
importantThe user interface for CDE 1.17 and above
        has been updated. The left-hand menu was updated to provide easy access to commonly used
        pages. The steps below will vary slightly, for example, the Overview
        page has been replaced with the Home page. You can also create a job
        by clicking Jobs on the left-hand menu, then selecting your desired
        Virtual Cluster from a drop-down at the top of the Jobs page. To view
        CDE Services, click Administration on the left-hand menu. The new
        home page still displays Virtual Clusters, but now includes quick-access links located at
        the top for the following categories: Jobs,
          Resources, and Download & Docs.
important The CDE jobs API implicitly adds the default
        DataLake filesystem to the Spark configuration to save the user having
        to do that. If you need to reference other buckets, you can set the
          spark.yarn.access.hadoopFileSystems parameter with
        the extra comma-separated buckets needed. If you set this parameter in
        your application code before creating the session, it might override the
        default setting, leading to errors.


In the Cloudera Data Platform (CDP) management console, click the Data
            Engineering tile and click Overview.

In the CDE Services column, select the service that contains the
          virtual cluster that you want to create a job for.

In the Virtual Clusters column on the right, locate the virtual
          cluster that you want to use and click the View Jobs icon.
In the left hand menu, click
        Jobs.Click the Create Job
        button.Provide the Job Details:
Select Spark for the job type.
              For Airflow job types, see Automating data pipelines using
                Apache Airflow DAG files in Cloudera Data
              Engineering.
Specify the Name.
Select File or
                URL for your application file, and
              provide or specify the file. You can upload a new file or select a
              file from an existing resource.If you select URL and
              specify an Amazon AWS S3 URL, add the following configuration to
              the job:config_key:
spark.hadoop.fs.s3a.delegation.token.bindingconfig_value:
org.apache.knox.gateway.cloud.idbroker.s3a.IDBDelegationTokenBinding
If your application code is a JAR file, specify the
                Main Class.
Specify arguments if required. You can click the
                Add Argument button to add multiple
              command arguments as necessary.
Enter Configurations if needed.
              You can click the Add Configuration button
              to add multiple configuration parameters as
            necessary.
If your application code is a Python file, select the
                Python Version, and optionally select a
                Python Environment.
Click Advanced Configurations to display
          more customizations, such as additional files, initial executors,
          executor range, driver and executor cores and memory.By
          default, the executor range is set to match the range of CPU cores
          configured for the virtual cluster. This improves resource utilization
          and efficiency by allowing jobs to scale up to the maximum virtual
          cluster resources available, without manually tuning and optimizing
          the number of executors per job.Click Schedule to display scheduling
          options.You can schedule the application to run
          periodically using the Basic controls or by
          specifying a Cron Expression.

noteScheduled job runs start at the end of the
        first full schedule interval after the start date, at the end of the scheduled period. For
        example, if you schedule a job with a daily interval with a
          start_date of 14:00, the first scheduled run is triggered at the end of
        the next day, after 23:59:59. However if the start_date is set to 00:00, it
        is triggered at the end of the same day, after 23:59:59.
If you provided a schedule, click
            Schedule to create the job. If you did not
          specify a schedule, and you do not want the job to run immediately,
          click the drop-down arrow on Create and Run and
          select Create. Otherwise, click
            Create and Run to run the job
          immediately.
Optional: Toggle Alerts to send mail to
          the email address that you choose. You have the option to select Job Failure to send an
          email upon job failure, and Job SLE Miss to send an email on a Job service-level agreement
          miss.






noteThe Alerts option will only display if
            you have selected Configure Email Alerting during Virtual Cluster
            creation.



