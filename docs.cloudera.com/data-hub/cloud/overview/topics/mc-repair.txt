Node repairCloudera Docs
Node repair
Data Hub monitors clusters, ensuring that when host-level failures occur, they are
  reported right away and can be quickly resolved by performing manual repair which deletes and
  replaces failed nodes and reattaches the disks. 
For each Data Hub cluster, CDP checks for Cloudera Manager agent heartbeat on all
   cluster nodes. If the Cloudera Manager agent heartbeat is lost on a node, a failure is reported
   for that node. This may happen for the following reasons:

The Cloudera Manager agent process exited on a node
An instance crashed
An instance was terminated

Once a failure is reported, options are available for you to repair the failure
   manually. 
importantA manual repair operation on an unhealthy node will replace the disk
      mounted as root, which means all data on the root disk is lost.To avoid the loss of
        important data, do not store it in the root disk. Instead, do one of the following:

Store data that needs to persist beyond the lifetime of the cluster in S3 or ADLS Gen
            2, depending on the platform in use.


Store data that needs to survive a repair operation in
              /hadoopfs/fsN/ (where N is an integer), as long as it is not so
            large that it could crowd out components that use that location.
For example, storing 1 GB of data in /hadoopfs/fs1/save_me would
            be an option to ensure that the data is available in a replacement node after a manual
            repair operation.


Manual repair is enabled for all clusters by default and covers all nodes, including the
      Cloudera Manager server node (by default, Cloudera Manager is installed on the master node).
      When a node fails, a notification about node failure is printed in the Event History, the
      affected node is marked as unhealthy, and an option to repair the cluster is available from
      the Actions menu. From the Hardware tab you can choose to repair
      a single node or specific nodes within a host group. There are two ways to repair the cluster:
        
Repair the failed nodes: (1) All non-ephemeral disks are detached from the failed nodes.
          (2) Failed nodes are removed (3) New nodes of the same type are provisioned. (4) The disks
          are attached to the new volumes, preserving the data. 
Delete the failed nodes: Failed nodes are deleted with their attached volumes.


note
As of March 26, 2021, the manual repair option is available for all clusters and typically
          covers all nodes, including the master node, regardless of whether or not the cluster uses
          an external database.
For clusters created before March 26, 2021, manual repair covers all nodes except the
          master node (Cloudera Manager server node). There is an exception: if a cluster (created
          before March 26, 2021) uses an external database, then manual repair is enabled for
          the master node. Ephemeral disks cannot be reattached (they are deleted and new disks are
          provisioned). 



