Compaction in Cloudera Data WarehouseCloudera Docs
Compaction in Cloudera Data Warehouse
You understand the importance of compaction and the consequences of neglecting to
  perform compaction. Compaction keeps your Data Warehouse healthy.Over time tables belonging to a workload become fragmented due to operations performed on them by
   your workload users. These small, obsolete files might lead to performance degradation and query
   latency problems. Compaction plays a major role in improving response time to workload queries by
   reducing the number of underlying files for a table and eliminating the obsolete ones. Compaction
   runs periodically in the background to maintain the optimal state. 
Running periodic compaction is a best practice for the performance for ACID transactions. ACID
   inserts and deletes generate the problematic files that you might need to monitor and manage. In
   Cloudera Data Warehouse (CDW), compaction is always performed by a Hive Virtual Warehouse. 
Compaction prerequisitesTo prevent data loss or an unsuccessful compaction, you must meet the prerequisites 1)     to exclude compaction users from Ranger policies and 2) to check for adequate resources in your     first Hive Virtual Warehouse used for compaction.Change compactor configuration for Hive Virtual Warehouses on Cloudera Data Warehouse Public CloudTo enhance performance, the compactor is a set of background processes that compact       delta files, which are created as a by-product of data modifications. When it runs, it     incurs additional load on the Hive Virtual Warehouse assigned as the compactor in Cloudera Data       Warehouse (CDW) Public Cloud. You can change which Hive warehouse performs compaction to load-balance     this workload as necessary. Initiating automatic compactionTo start automatic compaction in Cloudera Data Warehouse, you set a property in the         Database Catalog.Configuring compactionYou configure some configure compaction properties from the Database Catalog and some from the Virtual Warehouse.Automating metadata invalidation after compaction As you insert or delete data from Hive ACID tables, Hive generates delta files. The compaction process consolidates the delta files and keeps the system healthy. If you query Hive ACID tables from an Impala Virtual Warehouse, you need to learn how to automatically invalidate metadata after compaction to prevent a possible query failure.How compaction worksWhen data changes are made on Cloudera Data Warehouse (CDW) with inserts, updates, and deletes, delta files are created.    The more changes that are made, the more delta files are created. When a large number of delta    files are created, query performance degrades. Compaction removes these delta files to enhance    query performance.Compactor processesThese background processes run inside the metastore and HiveServer2 in Cloudera Data    Warehouse (CDW) Public Cloud. They support the data modifications made as a result of ACID    transactions.How compaction interacts with the Data LakeIn the Data Lake on CDP, the initiator and cleaner processes also run in the metastore     as they do in Cloudera Data Warehouse (CDW) Public Cloud. However, the worker process runs in     HiveServer (HS2), which equates to a Hive Virtual     Warehouse..Cloudera Data Warehouse Public Cloud Compaction ArchitectureThis diagram illustrates how the components that perform compaction interact on Cloudera  Data Warehouse (CDW) Public Cloud.