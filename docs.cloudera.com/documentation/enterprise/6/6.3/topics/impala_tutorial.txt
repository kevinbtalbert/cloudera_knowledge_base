



Impala Tutorials | 6.3.x | Cloudera Documentation





















































 Documentation


Products
Services & Support
Solutions









Cloudera Enterprise 6.3.x | Other versions





CDH
Component GuidesImpala








View All Categories

Getting Started

Cloudera Personas
Planning a New Cloudera Enterprise Deployment
CDH

Hive
Impala
Kudu
Sentry
Spark
External Documentation


Cloudera Manager

Software Management

Parcels




Navigator

Getting Started
FAQ


Navigator Encryption

Navigator Key Trustee Server
Navigator Key HSM
Navigator HSM KMS
Navigator Encrypt


Proof-of-Concept Installation Guide

Before You Begin
Installing a Proof-of-Concept Cluster

Step 1: Run the Cloudera Manager Installer
Step 2: Install CDH Using the Wizard
Step 3: Set Up a Cluster


Managing the Embedded Database
Migrating Embedded PostgreSQL Database to External PostgreSQL Database


Getting Support
FAQ


Release Notes
Requirements and Supported Versions
Installation

Before You Install

Storage Space Planning for Cloudera Manager
Configure Network Names
Disabling the Firewall
Setting SELinux mode
Enable an NTP Service
Install Python 2.7 on Hue Hosts
Impala Requirements
Required Privileges
Ports

Cloudera Manager and Navigator
Navigator Encryption
CDH Components
DistCp
Third-Party Components


Recommended Role Distribution
Custom Installation Solutions

Configuring a Local Parcel Repository
Configuring a Local Package Repository
Manually Install Cloudera Software Packages
Creating Virtual Images of Cluster Hosts
Configuring a Custom Java Home Location
Creating a CDH Cluster Using a Cloudera Manager Template


Service Dependencies in Cloudera Manager


Installing Cloudera Manager and CDH

Step 1: Configure a Repository
Step 2: Install JDK
Step 3: Install Cloudera Manager Server
Step 4: Install Databases

Install and Configure MariaDB
Install and Configure MySQL
Install and Configure PostgreSQL
Install and Configure Oracle Database


Step 5: Set up the Cloudera Manager Database
Step 6: Install CDH and Other Software
Step 7: Set Up a Cluster


Installing Navigator Data Management
Installing Navigator Encryption

Installing Cloudera Navigator Key Trustee Server
Installing Cloudera Navigator Key HSM
Installing Key Trustee KMS
Installing Navigator HSM KMS Backed by Thales HSM
Installing Navigator HSM KMS Backed by Luna HSM
Installing Cloudera Navigator Encrypt


After Installation

Deploying Clients
Testing the Installation
Installing the GPL Extras Parcel
Migrating from Packages to Parcels
Migrating from Parcels to Packages
Secure Your Cluster


Troubleshooting Installation Problems
Uninstalling Cloudera Software

Uninstalling a CDH Component From a Single Host




Upgrade Guide
Cluster Management

Cloudera Manager

Cloudera Manager Admin Console

Home Page
Documentation
Automatic Logout


FAQ
Cloudera Manager API

Cluster Automation


Cloudera Manager Administration

Starting, Stopping, and Restarting the Cloudera Manager Server
Configuring Cloudera Manager Server Ports
Moving the Cloudera Manager Server to a New Host
Migrating Embedded PostgreSQL Database to External PostgreSQL Database
Migrating from PostgreSQL Database Server to MySQL/Oracle Database Server
Managing the Cloudera Manager Server Log
Cloudera Manager Agents

Starting, Stopping, and Restarting Cloudera Manager Agents
Configuring Cloudera Manager Agents
Managing Cloudera Manager Agent Logs


Configuring Network Settings
Managing Licenses
Sending Usage and Diagnostic Data to Cloudera
Exporting and Importing Cloudera Manager Configuration
Backing Up Cloudera Manager
Other Tasks and Settings
Cloudera Management Service


Extending Cloudera Manager


Cluster Configuration Overview

Modifying Configuration Properties Using Cloudera Manager
Autoconfiguration
Custom Configuration
Stale Configurations
Client Configuration Files
Viewing and Reverting Configuration Changes
Exporting and Importing Cloudera Manager Configuration
Cloudera Manager Configuration Properties Reference


Managing Clusters

Adding and Deleting Clusters
Starting, Stopping, Refreshing, and Restarting a Cluster
Pausing a Cluster in AWS
Renaming a Cluster
Cluster-Wide Configuration
Virtual Private Clusters and Cloudera SDX

Compatibility Considerations for Virtual Private Clusters
Tutorial: Using Impala, Hive and Hue with Virtual Private Clusters
Networking Considerations for Virtual Private Clusters


Managing Services

HBase
HDFS

Data Durability

Enabling Erasure Coding


NameNodes

Backing Up and Restoring HDFS Metadata
Moving NameNode Roles
Sizing NameNode Heap Memory
Backing Up and Restoring NameNode Metadata


DataNodes

Configuring Storage Directories for DataNodes
Configuring Storage Balancing for DataNodes
Performing Disk Hot Swap for DataNodes


JournalNodes
Configuring Short-Circuit Reads
Configuring HDFS Trash
Preventing Inadvertent Deletion of Directories
HDFS Balancers
Enabling WebHDFS
Adding HttpFS
Adding and Configuring an NFS Gateway
Setting HDFS Quotas
Configuring Mountable HDFS
Configuring Centralized Cache Management in HDFS
Configuring Proxy Users to Access HDFS
Using CDH with Isilon Storage
Configuring Heterogeneous Storage in HDFS


Hive
Hue

Adding a Hue Service and Role Instance
Managing Hue Analytics Data Collection
Enabling Hue Applications Using Cloudera Manager


Impala

The Impala Service
Modifying Impala Startup Options
Post-Installation Configuration for Impala
Configuring Impala to Work with ODBC
Configuring Impala to Work with JDBC


Key-Value Store Indexer
Kudu
Solr
Spark

Managing Spark Using Cloudera Manager
Managing the Spark History Server


Sqoop 1 Client
YARN (MRv2) and MapReduce (MRv1)

Managing YARN
Managing YARN ACLs
Managing MapReduce


Managing ZooKeeper
Configuring Services to Use the GPL Extras Parcel




Managing Hosts

Viewing Host Details
Using the Host Inspector
Adding a Host to the Cluster
Specifying Racks for Hosts
Host Templates
Performing Maintenance on a Cluster Host

Tuning and Troubleshooting Host Decommissioning
Maintenance Mode


Changing Hostnames
Deleting Hosts
Moving a Host Between Clusters


Managing Services

Adding a Service
Comparing Configurations for a Service Between Clusters
Add-on Services
Starting, Stopping, and Restarting Services
Rolling Restart
Aborting a Pending Command
Deleting Services
Renaming a Service
Configuring Maximum File Descriptors
Exposing Hadoop Metrics to Graphite
Exposing Hadoop Metrics to Ganglia


Managing Roles

Role Instances
Role Groups


Monitoring and Diagnostics

Introduction to Cloudera Manager Monitoring

Time Line
Health Tests
Home Page
Viewing Charts for Cluster, Service, Role, and Host Instances
Configuring Monitoring Settings


Monitoring Clusters
Inspecting Network Performance
Monitoring Services

Monitoring Service Status
Viewing Service Status
Viewing Service Instance Details
Viewing Role Instance Status

The Processes Tab


Running Diagnostic Commands for Roles
Periodic Stacks Collection
Viewing Running and Recent Commands
Monitoring Resource Management


Monitoring Hosts

Host Details
Host Inspector


Monitoring Activities

Monitoring MapReduce Jobs

Viewing and Filtering MapReduce Activities
Viewing the Jobs in a Pig, Oozie, or Hive Activity
Task Attempts
Viewing Activity Details in a Report Format
Comparing Similar Activities
Viewing the Distribution of Task Attempts


Monitoring Impala Queries

Query Details


Monitoring YARN Applications
Monitoring Spark Applications


Events
Alerts

Managing Alerts

Configuring Alert Email Delivery
Configuring Alert SNMP Delivery
Configuring Custom Alert Scripts




Triggers

Cloudera Manager Trigger Use Cases


Lifecycle and Security Auditing
Charting Time-Series Data

Dashboards
tsquery Language
Metric Aggregation


Logs

Viewing the Cloudera Manager Server Log
Viewing the Cloudera Manager Agent Logs
Managing Disk Space for Log Files


Reports

Directory Usage Report
Disk Usage Reports
Activity, Application, and Query Reports
The File Browser
Downloading HDFS Directory Access Permission Reports


Troubleshooting Cluster Configuration and Operation
Monitoring Reference

Cloudera Manager Entity Types
Cloudera Manager Entity Type Attributes
Cloudera Manager Events

HEALTH_CHECK Category
SYSTEM Category
AUDIT_EVENT Category
HBASE Category
LOG_MESSAGE Category
ACTIVITY_EVENT Category


Cloudera Manager Health Tests

Active Database Health Tests
Active Key Trustee Server Health Tests
Activity Monitor Health Tests
Alert Publisher Health Tests
Authentication Server Health Tests
Authentication Server Load Balancer Health Tests
Authentication Service Health Tests
Cloudera Management Service Health Tests
DataNode Health Tests
Event Server Health Tests
Failover Controller Health Tests
Flume Health Tests
Flume Agent Health Tests
Garbage Collector Health Tests
HBase Health Tests
HBase REST Server Health Tests
HBase Thrift Server Health Tests
HDFS Health Tests
History Server Health Tests
Hive Health Tests
Hive Execution Health Tests
Hive Metastore Server Health Tests
HiveServer2 Health Tests
Host Health Tests
Host Monitor Health Tests
HttpFS Health Tests
Hue Health Tests
Hue Server Health Tests
Impala Health Tests
Impala Catalog Server Health Tests
Impala Daemon Health Tests
Impala Llama ApplicationMaster Health Tests
Impala StateStore Health Tests
JobHistory Server Health Tests
JobTracker Health Tests
JournalNode Health Tests
Kafka Health Tests
Kafka Broker Health Tests
Kafka MirrorMaker Health Tests
Kerberos Ticket Renewer Health Tests
Key Management Server Health Tests
Key Management Server Proxy Health Tests
Key-Value Store Indexer Health Tests
Kudu Health Tests
Lily HBase Indexer Health Tests
Load Balancer Health Tests
MapReduce Health Tests
Master Health Tests
Monitor Health Tests
NFS Gateway Health Tests
NameNode Health Tests
Navigator Audit Server Health Tests
Navigator Luna KMS Metastore Health Tests
Navigator Luna KMS Proxy Health Tests
Navigator Metadata Server Health Tests
Navigator Thales KMS Metastore Health Tests
Navigator Thales KMS Proxy Health Tests
NodeManager Health Tests
Oozie Health Tests
Oozie Server Health Tests
Passive Database Health Tests
Passive Key Trustee Server Health Tests
RegionServer Health Tests
Reports Manager Health Tests
ResourceManager Health Tests
SecondaryNameNode Health Tests
Sentry Health Tests
Sentry Server Health Tests
Service Monitor Health Tests
Solr Health Tests
Solr Server Health Tests
Spark Health Tests
Spark (Standalone) Health Tests
Tablet Server Health Tests
TaskTracker Health Tests
Telemetry Publisher Health Tests
Tracer Health Tests
WebHCat Server Health Tests
Worker Health Tests
YARN (MR2 Included) Health Tests
ZooKeeper Health Tests
ZooKeeper Server Health Tests


Cloudera Manager Metrics

Accumulo Metrics
Active Database Metrics
Active Key Trustee Server Metrics
Activity Metrics
Activity Monitor Metrics
Agent Metrics
Alert Publisher Metrics
Attempt Metrics
Authentication Server Metrics
Authentication Server Load Balancer Metrics
Authentication Service Metrics
Cloudera Management Service Metrics
Cloudera Manager Server Metrics
Cluster Metrics
DSSD DataNode Metrics
DataNode Metrics
Directory Metrics
Disk Metrics
Event Server Metrics
Failover Controller Metrics
Filesystem Metrics
Flume Metrics
Flume Channel Metrics
Flume Sink Metrics
Flume Source Metrics
Garbage Collector Metrics
HBase Metrics
HBase REST Server Metrics
HBase RegionServer Replication Peer Metrics
HBase Thrift Server Metrics
HDFS Metrics
HDFS Cache Directive Metrics
HDFS Cache Pool Metrics
HRegion Metrics
HTable Metrics
History Server Metrics
Hive Metrics
Hive Execution Metrics
Hive Metastore Server Metrics
HiveServer2 Metrics
Host Metrics
Host Monitor Metrics
HttpFS Metrics
Hue Metrics
Hue Server Metrics
Impala Metrics
Impala Catalog Server Metrics
Impala Daemon Metrics
Impala Daemon Resource Pool Metrics
Impala Llama ApplicationMaster Metrics
Impala Pool Metrics
Impala Pool User Metrics
Impala Query Metrics
Impala StateStore Metrics
Isilon Metrics
Java KeyStore KMS Metrics
JobHistory Server Metrics
JobTracker Metrics
JournalNode Metrics
Kafka Metrics
Kafka Broker Metrics
Kafka Broker Topic Metrics
Kafka Broker Topic Partition Metrics
Kafka Consumer Metrics
Kafka Consumer Group Metrics
Kafka MirrorMaker Metrics
Kafka Producer Metrics
Kafka Replica Metrics
Kerberos Ticket Renewer Metrics
Key Management Server Metrics
Key Management Server Proxy Metrics
Key Trustee KMS Metrics
Key Trustee Server Metrics
Key-Value Store Indexer Metrics
Kudu Metrics
Kudu Replica Metrics
Lily HBase Indexer Metrics
Load Balancer Metrics
MapReduce Metrics
Master Metrics
Monitor Metrics
NFS Gateway Metrics
NameNode Metrics
Navigator Audit Server Metrics
Navigator HSM KMS backed by SafeNet Luna HSM Metrics
Navigator HSM KMS backed by Thales HSM Metrics
Navigator Luna KMS Metastore Metrics
Navigator Luna KMS Proxy Metrics
Navigator Metadata Server Metrics
Navigator Thales KMS Metastore Metrics
Navigator Thales KMS Proxy Metrics
Network Interface Metrics
NodeManager Metrics
Oozie Metrics
Oozie Server Metrics
Passive Database Metrics
Passive Key Trustee Server Metrics
RegionServer Metrics
Reports Manager Metrics
ResourceManager Metrics
SecondaryNameNode Metrics
Sentry Metrics
Sentry Server Metrics
Server Metrics
Service Monitor Metrics
Solr Metrics
Solr Replica Metrics
Solr Server Metrics
Solr Shard Metrics
Spark Metrics
Spark (Standalone) Metrics
Sqoop 1 Client Metrics
Tablet Server Metrics
TaskTracker Metrics
Telemetry Publisher Metrics
Time Series Table Metrics
Tracer Metrics
User Metrics
WebHCat Server Metrics
Worker Metrics
YARN (MR2 Included) Metrics
YARN Pool Metrics
YARN Pool User Metrics
ZooKeeper Metrics
Disabling Metrics for Specific Roles






Performance Management

Optimizing Performance in CDH
Choosing and Configuring Data Compression
Tuning the Solr Server
Tuning Spark Applications
Tuning YARN
Tuning JVM Garbage Collection


Resource Management

Static Service Pools

Linux Control Groups (cgroups)


Dynamic Resource Pools
YARN (MRv2) and MapReduce (MRv1) Schedulers

Configuring the Fair Scheduler
Enabling and Disabling Fair Scheduler Preemption


Data Storage for Monitoring Data
Cluster Utilization Reports

Creating a Custom Cluster Utilization Report




High Availability

HDFS High Availability

Introduction to HDFS High Availability
Configuring Hardware for HDFS HA
Enabling HDFS HA
Disabling and Redeploying HDFS HA
Configuring Other CDH Components to Use HDFS HA
Administering an HDFS High Availability Cluster
Changing a Nameservice Name for Highly Available HDFS Using Cloudera Manager


MapReduce (MRv1) and YARN (MRv2) High Availability

YARN (MRv2) ResourceManager High Availability
Work Preserving Recovery for YARN Components
MapReduce (MRv1) JobTracker High Availability


Cloudera Navigator Key Trustee Server High Availability
Enabling Key Trustee KMS High Availability
Enabling Navigator HSM KMS High Availability
High Availability for Other CDH Components

HBase High Availability

HBase Read Replicas


Oozie High Availability
Search High Availability


Navigator Data Management in a High Availability Environment
Configuring Cloudera Manager for High Availability With a Load Balancer

Introduction to Cloudera Manager Deployment Architecture
Prerequisites for Setting up Cloudera Manager High Availability
Cloudera Manager Failover Protection
High-Level Steps to Configure Cloudera Manager High Availability

Step 1: Setting Up Hosts and the Load Balancer
Step 2: Installing and Configuring Cloudera Manager Server for High Availability
Step 3: Installing and Configuring Cloudera Management Service for High Availability
Step 4: Automating Failover with Corosync and Pacemaker


Database High Availability Configuration
TLS and Kerberos Configuration for Cloudera Manager High Availability




Backup and Disaster Recovery

Port Requirements for Backup and Disaster Recovery
Data Replication

Designating a Replication Source
HDFS Replication

Monitoring the Performance of HDFS Replications


Hive/Impala Replication

Monitoring the Performance of Hive/Impala Replications


Replicating Data to Impala Clusters
Using Snapshots with Replication
Enabling Replication Between Clusters with Kerberos Authentication
Replication of Encrypted Data
HBase Replication


Snapshots

Cloudera Manager Snapshot Policies
Managing HBase Snapshots
Managing HDFS Snapshots


BDR Tutorials

How To Back Up and Restore Apache Hive Data Using Cloudera Enterprise BDR
How To Back Up and Restore HDFS Data Using Cloudera Enterprise BDR
BDR Automation Examples


Migrating Data between Clusters Using distcp

Copying Cluster Data Using DistCp
Copying Data between a Secure and an Insecure Cluster using DistCp and WebHDFS
Post-migration Verification




Backing Up Databases
Cloudera Navigator Administration
Accessing Storage Using Amazon S3

Configuring the Amazon S3 Connector

Using S3 Credentials with YARN, MapReduce, or Spark


Using Fast Upload with Amazon S3
Configuring and Managing S3Guard
How to Configure a MapReduce Job to Access S3 with an HDFS Credstore
Importing Data into Amazon S3 Using Sqoop


Accessing Storage Using Microsoft ADLS

Configuring ADLS Access Using Cloudera Manager
Configuring ADLS Gen1 Connectivity
Configuring ADLS Gen2 Connectivity
Importing Data into Microsoft Azure Data Lake Store Using Sqoop


Configuring Google Cloud Storage Connectivity
How To Create a Multitenant Enterprise Data Hub


Security

Overview

Authentication Overview
Encryption Overview

Encryption Mechanisms


Authorization Overview
Auditing and Data Governance


Authentication

Kerberos Security Artifacts Overview
Configuring Authentication in Cloudera Manager

Cloudera Manager User Accounts
Configuring External Authentication and Authorization for Cloudera Manager
Enabling Kerberos Authentication for CDH

Step 1: Install Cloudera Manager and CDH
Step 2: Install JCE Policy Files for AES-256 Encryption
Step 3: Create the Kerberos Principal for Cloudera Manager Server
Step 4: Enabling Kerberos Using the Wizard
Step 5: Create the HDFS Superuser
Step 6: Get or Create a Kerberos Principal for Each User Account
Step 7: Prepare the Cluster for Each User
Step 8: Verify that Kerberos Security is Working
Step 9: (Optional) Enable Authentication for HTTP Web Consoles for Hadoop Roles


Kerberos Authentication for Non-Default Users
Customizing Kerberos Principals
Managing Kerberos Credentials Using Cloudera Manager
Using a Custom Kerberos Keytab Retrieval Script
Adding Trusted Realms to the Cluster
Using Auth-to-Local Rules to Isolate Cluster Users


Configuring Authentication for Cloudera Navigator

Cloudera Navigator and External Authentication

Configuring Cloudera Navigator for Active Directory
Configuring Cloudera Navigator for LDAP
Configuring Cloudera Navigator for SAML


Configuring Groups for Cloudera Navigator


Configuring Authentication for Other Components

Flume Authentication

Configuring Kerberos for Flume Thrift Source and Sink Using Cloudera Manager
Writing to a Secure HBase Cluster
Using Substitution Variables with Flume for Kerberos Artifacts


HBase Authentication

Configuring Kerberos Authentication for HBase
Configuring Secure HBase Replication
Configuring the HBase Client TGT Renewal Period


Hive Authentication

HiveServer2 Security Configuration
Using Hive to Run Queries on a Secure HBase Server


HttpFS Authentication
Hue Authentication

Enable Hue to Use Kerberos for Authentication


Impala Authentication

Enabling Kerberos Authentication for Impala
Enabling LDAP Authentication for Impala
Using Multiple Authentication Methods with Impala
Configuring Impala Delegation for Hue and BI Tools


Cloudera Search Authentication

Using Kerberos with Cloudera Search


Spark Authentication
Sqoop1 Authentication
ZooKeeper Authentication


Configuring a Dedicated MIT KDC for Cross-Realm Trust
Integrating MIT Kerberos and Active Directory
Hadoop Users (user:group) and Kerberos Principals
Mapping Kerberos Principals to Short Names


Authorization

Cloudera Manager User Roles
HDFS Extended ACLs
Authorization for HDFS Web UIs
Configuring LDAP Group Mappings
Authorization With Apache Sentry
Configuring HBase Authorization


Encrypting Data in Transit

Understanding Keystores and Truststores
Configuring TLS Encryption for Cloudera Manager and CDH Using Auto-TLS
Manually Configuring TLS Encryption for Cloudera Manager
Manually Configuring TLS Encryption on the Agent Listening Port
Manually Configuring TLS/SSL Encryption for CDH Services

Configuring TLS/SSL for HDFS, YARN and MapReduce
Configuring TLS/SSL for HBase
Configuring TLS/SSL for Flume
Configuring Encrypted Communication Between HiveServer2 and Client Drivers
Configuring TLS/SSL for Hue
Configuring TLS/SSL for Impala
Configuring TLS/SSL for Oozie
Configuring TLS/SSL for Solr
Spark Encryption
Configuring TLS/SSL for HttpFS


Configuring TLS/SSL for Navigator Audit Server
Configuring TLS/SSL for Navigator Metadata Server
Configuring TLS/SSL for Kafka (Navigator Event Broker)
Configuring Encrypted Transport for HDFS
Configuring Encrypted Transport for HBase


Encrypting Data at Rest

Data at Rest Encryption Reference Architecture
Data at Rest Encryption Requirements
Resource Planning for Data at Rest Encryption
HDFS Transparent Encryption

Optimizing Performance for HDFS Transparent Encryption
Enabling HDFS Encryption Using the Wizard
Managing Encryption Keys and Zones
Configuring the Key Management Server (KMS)
Securing the Key Management Server (KMS)

Configuring KMS Access Control Lists (ACLs)


Migrating from a Key Trustee KMS to an HSM KMS
Migrating Keys from a Java KeyStore to Cloudera Navigator Key Trustee Server
Migrating a Key Trustee KMS Server Role Instance to a New Host
Configuring CDH Services for HDFS Encryption




Cloudera Navigator Key Trustee Server

Backing Up and Restoring Key Trustee Server and Clients
Initializing Standalone Key Trustee Server
Configuring a Mail Transfer Agent for Key Trustee Server
Verifying Cloudera Navigator Key Trustee Server Operations
Managing Key Trustee Server Organizations
Managing Key Trustee Server Certificates


Cloudera Navigator Key HSM

Initializing Navigator Key HSM
HSM-Specific Setup for Cloudera Navigator Key HSM
Validating Key HSM Settings
Managing the Navigator Key HSM Service
Integrating Key HSM with Key Trustee Server


Cloudera Navigator Encrypt

Registering Cloudera Navigator Encrypt with Key Trustee Server
Preparing for Encryption Using Cloudera Navigator Encrypt
Encrypting and Decrypting Data Using Cloudera Navigator Encrypt
Converting from Device Names to UUIDs for Encrypted Devices
Navigator Encrypt Access Control List
Maintaining Cloudera Navigator Encrypt


Configuring Encryption for Data Spills

Configuring Encrypted On-disk File Channels for Flume


Impala Security Overview

Security Guidelines for Impala
Securing Impala Data and Log Files
Installation Considerations for Impala Security
Securing the Hive Metastore Database
Securing the Impala Web User Interface


Kudu Security Overview
How-To Guides

Add Root and Intermediate CAs to Truststore for TLS/SSL
Amazon S3 Security
Authenticate Kerberos Principals Using Java
Check Cluster Security Settings
Configure Antivirus Software on CDH Hosts
Configure Browser-based Interfaces to Require Authentication (SPNEGO)
Configure Browsers for Kerberos Authentication (SPNEGO)
Configure Cluster to Use Kerberos Authentication
Convert DER, JKS, PEM Files for TLS/SSL Artifacts
Configure Authentication for Amazon S3
Configure Encryption for Amazon S3
Configure AWS Credentials
Enable Sensitive Data Redaction
Log a Security Support Case
Obtain and Deploy Keys and Certificates for TLS/SSL
Renew and Redistribute Certificates
Set Up a Gateway Host to Restrict Access to the Cluster
Set Up Access to Cloudera EDH or Altus Director (Microsoft Azure Marketplace)
Use Self-Signed Certificates for TLS


Troubleshooting Security Issues

Error Messages
Authentication and Kerberos Issues
HDFS Encryption Issues
Key Trustee KMS Encryption Issues
TLS/SSL Issues
YARN, MRv1, and Linux OS Security

TaskController Error Codes (MRv1)
ContainerExecutor Error Codes (YARN)






Cloudera Navigator Data Management

Overview
Search

Performing Actions on Entities


Auditing

Using Audit Events to Understand Cluster Activity
Exploring Audit Data
Cloudera Navigator Audit Event Reports


Analytics
Policies
Lineage

Using the Lineage View
Using Lineage to Display Table Schema
Generating Lineage Diagrams


Business Metadata

Defining Managed Properties
Adding and Editing Metadata


Administration (Navigator Console)

Managing Metadata Storage with Purge
Administering Navigator User Roles


Navigator Configuration and Management

Accessing Navigator Data Management Logs
Backing Up Cloudera Navigator Data
Authentication and Authorization
Configuring Cloudera Navigator to work with Hue HA
Cloudera Navigator support for Virtual Private Clusters
Encryption (TLS/SSL) and Cloudera Navigator
Limiting Sensitive Data in Navigator Logs
Preventing Concurrent Logins from the Same User
Navigator Audit Server Management

Setting Up Navigator Audit Server
Enabling Audit and Log Collection for Services
Configuring Service Auditing Properties
Adding Audit Filters
Monitoring Navigator Audit Service Health
Publishing Audit Events
Maintaining Navigator Audit Server


Navigator Metadata Server Management

Setting Up Navigator Metadata Server
Navigator Metadata Server Tuning
Configuring and Managing Extraction
Hive and Impala Lineage Configuration
Configuring the Server for Policy Messages




Cloudera Navigator and the Cloud

Using Cloudera Navigator with Altus Clusters

Configuring Extraction for Altus Clusters on AWS


Using Cloudera Navigator with Amazon S3

Configuring Extraction for Amazon S3




Cloudera Navigator APIs

Navigator APIs Overview
Applying Metadata to HDFS and Hive Entities using the API
Using the Purge APIs for Metadata Maintenance Tasks


Cloudera Navigator Reference

Lineage Diagram Icons
Search Syntax and Properties
Service Audit Events
Service Metadata Entity Types
Metadata Policy Expressions
User Roles and Privileges Reference


Troubleshooting Navigator Data Management


CDH Component Guides

Crunch
Flume

Configuring

Configuring the Flume Properties File
Files Installed by the Flume RPM and Debian Packages
Configuring Flume Security with Kafka


Using & Managing

Running Flume
Supported Sources, Sinks, and Channels
Flume Kudu Sink
Viewing the Flume Documentation




HBase

Configuring

Accessing HBase by using the HBase Shell
HBase Online Merge
Using MapReduce with HBase
Configuring HBase Garbage Collection
Configuring the HBase Canary
Configuring the Blocksize for HBase
Configuring the HBase BlockCache
Configuring Quotas
Configuring the HBase Scanner Heartbeat
Limiting the Speed of Compactions
Configuring and Using the HBase REST API
Configuring HBase MultiWAL Support
Storing Medium Objects (MOBs) in HBase
Configuring the Storage Policy for the Write-Ahead Log (WAL)


Using & Managing

Starting and Stopping HBase
Accessing HBase by using the HBase Shell
Using HBase Command-Line Utilities
Using the HBCK2 Tool to Remediate HBase Clusters
Hedged Reads
Reading Data from HBase
HBase Filtering
Writing Data to HBase
Importing Data Into HBase
Exposing HBase Metrics to a Ganglia Server
Using HashTable and SyncTable Tool


Security
Troubleshooting


Hive

Installation and Upgrade
Configuring

Configuring HiveServer2
File System Permissions
Starting, Stopping, & Using HS2
Using Hive w/HBase
Installing JDBC/ODBC Drivers
Setting HADOOP_MAPRED_HOME


Using & Managing

Managing Hive with Cloudera Manager
Ingesting & Querying Data
Using Parquet Tables
Running Hive on Spark
Using HS2 Web UI
Using Query Plan Graph View
Accessing Table Statistics
Managing UDFs
Hive ETL Jobs on S3
Hive with ADLS
Erasure Coding with Hive
Removing the Hive Compilation Lock
Sqoop HS2 Import


Tuning

Tuning Hive on Spark
Tuning Hive on S3
Configuring HS2 HA
Enabling Query Vectorization


Hive Metastore (HMS)

Configuring

Configuring HMS
Configuring HMS HA
Configuring HMS for HDFS HA
Configuring Shared Amazon RDS as HMS


Using & Managing

Starting the Metastore
Using Metastore Schema Tool




Data Replication
Security
HCatalog

HCatalog Prerequisites
Configuration Change on Hosts Used with HCatalog
Accessing Table Information with the HCatalog Command-line API
Accessing Table Data with MapReduce
Accessing Table Data with Pig
Accessing Table Information with REST
Viewing the HCatalog Documentation


Troubleshooting


Hue

Hue Versions
Reference Architecture
Installation & Upgrade
Using

Enable SQL Editor Autocompleter
Use Governance-Based Data Discovery
Use S3 as Source or Sink in Hue


Administration

Configuring
Customize Hue Web UI
Enable Governance-Based Data Discovery
Enable S3 Cloud Storage
Run Shell Commands
Connecting a Database

Connect to MySQL or MariaDB
Connect to PostgreSQL
Connect to Oracle (Parcel)
Connect to Oracle (Package)
Custom Database Tutorial


Migrate the Database
Populate the Database


Performance Tuning

Add Load Balancer
Configure High Availability
Hue/HDFS High Availability


Security

User Permissions
Create Password Scripts
Authenticate Users with LDAP
Synchronize with LDAP Server
Authenticate Users with SAML
Authorize Groups with Sentry


Troubleshooting

Potential Misconfiguration
Unable to connect to database with provided credential
Unable to view Snappy-compressed files
“Unknown Attribute Name” exception while enabling SAML
Invalid query handle
Services backed by Postgres fail or hang
Downloading query results from Hue takes long time
Error validating LDAP user in Hue
502 Proxy Error while accessing Hue from the Load Balancer
Hue Load Balancer does not start after enabling TLS
Unable to kill Hive queries from Job Browser
1040, 'Too many connections' exception
Unable to connect Oracle database to Hue using SCAN
Increasing the maximum number of processes for Oracle database
Unable to authenticate to Hbase when using Hue




Impala

Concepts and Architecture

Components
Developing Applications
Role in the Hadoop Ecosystem


Deployment Planning

Impala Requirements
Designing Schemas


Tutorials
Administration

Setting Timeouts
Load-Balancing Proxy for HA
Managing Disk Space
Auditing
Viewing Lineage Info


SQL Reference

Comments
Data Types

ARRAY Complex Type (CDH 5.5 or higher only)
BIGINT
BOOLEAN
CHAR
DECIMAL
DOUBLE
FLOAT
INT
MAP Complex Type (CDH 5.5 or higher only)
REAL
SMALLINT
STRING
STRUCT Complex Type (CDH 5.5 or higher only)
TIMESTAMP

Customizing Time Zones


TINYINT
VARCHAR
Complex Types (CDH 5.5 or higher only)


Literals
SQL Operators
Schema Objects and Object Names

Aliases
Databases
Functions
Identifiers
Tables
Views


SQL Statements

DDL Statements
DML Statements
ALTER DATABASE
ALTER TABLE
ALTER VIEW
COMMENT
COMPUTE STATS
CREATE DATABASE
CREATE FUNCTION
CREATE ROLE
CREATE TABLE
CREATE VIEW
DELETE
DESCRIBE
DROP DATABASE
DROP FUNCTION
DROP ROLE
DROP STATS
DROP TABLE
DROP VIEW
EXPLAIN
GRANT
INSERT
INVALIDATE METADATA
LOAD DATA
REFRESH
REFRESH AUTHORIZATION
REFRESH FUNCTIONS
REVOKE
SELECT

Joins
ORDER BY Clause
GROUP BY Clause
HAVING Clause
LIMIT Clause
OFFSET Clause
UNION Clause
Subqueries
TABLESAMPLE Clause
WITH Clause
DISTINCT Operator


SET

Query Options for the SET Statement

ABORT_ON_ERROR
ALLOW_ERASURE_CODED_FILES
ALLOW_UNSUPPORTED_FORMATS
APPX_COUNT_DISTINCT
BATCH_SIZE
BUFFER_POOL_LIMIT
COMPRESSION_CODEC
COMPUTE_STATS_MIN_SAMPLE_SIZE
DEBUG_ACTION
DECIMAL_V2
DEFAULT_JOIN_DISTRIBUTION_MODE
DEFAULT_SPILLABLE_BUFFER_SIZE
DISABLE_CODEGEN
DISABLE_CODEGEN_ROWS_THRESHOLD
DISABLE_ROW_RUNTIME_FILTERING
DISABLE_STREAMING_PREAGGREGATIONS
DISABLE_UNSAFE_SPILLS
ENABLE_EXPR_REWRITES
EXEC_SINGLE_NODE_ROWS_THRESHOLD
EXEC_TIME_LIMIT_S
EXPLAIN_LEVEL
HBASE_CACHE_BLOCKS
HBASE_CACHING
IDLE_SESSION_TIMEOUT
KUDU_READ_MODE
LIVE_PROGRESS
LIVE_SUMMARY
MAX_ERRORS
MAX_MEM_ESTIMATE_FOR_ADMISSION
MAX_NUM_RUNTIME_FILTERS
MAX_ROW_SIZE
MAX_SCAN_RANGE_LENGTH
MEM_LIMIT
MIN_SPILLABLE_BUFFER_SIZE
MT_DOP
NUM_NODES
NUM_ROWS_PRODUCED_LIMIT
NUM_SCANNER_THREADS
OPTIMIZE_PARTITION_KEY_SCANS
PARQUET_COMPRESSION_CODEC
PARQUET_ANNOTATE_STRINGS_UTF8
PARQUET_ARRAY_RESOLUTION
PARQUET_DICTIONARY_FILTERING
PARQUET_FALLBACK_SCHEMA_RESOLUTION
PARQUET_FILE_SIZE
PARQUET_READ_STATISTICS
PREFETCH_MODE
QUERY_TIMEOUT_S
REPLICA_PREFERENCE
REQUEST_POOL
RESOURCE_TRACE_RATIO
RUNTIME_BLOOM_FILTER_SIZE
RUNTIME_FILTER_MAX_SIZE
RUNTIME_FILTER_MIN_SIZE
RUNTIME_FILTER_MODE
RUNTIME_FILTER_WAIT_TIME_MS
S3_SKIP_INSERT_STAGING
SCAN_BYTES_LIMIT
SCHEDULE_RANDOM_REPLICA
SCRATCH_LIMIT
SHUFFLE_DISTINCT_EXPRS
SUPPORT_START_OVER
SYNC_DDL
THREAD_RESERVATION_AGGREGATE_LIMIT
THREAD_RESERVATION_LIMIT
TIMEZONE
TOPN_BYTES_LIMIT




SHOW
SHUTDOWN
TRUNCATE TABLE
UPDATE
UPSERT
USE
VALUES
Optimizer Hints


Built-In Functions

Mathematical Functions
Bit Functions
Type Conversion Functions
Date and Time Functions
Conditional Functions
String Functions
Miscellaneous Functions
Aggregate Functions

APPX_MEDIAN
AVG
COUNT
GROUP_CONCAT
MAX
MIN
NDV
STDDEV, STDDEV_SAMP, STDDEV_POP
SUM
VARIANCE, VARIANCE_SAMP, VARIANCE_POP, VAR_SAMP, VAR_POP


Analytic Functions


User-Defined Functions (UDFs)
SQL Differences Between Impala and Hive
Porting SQL


Resource Management

Admission Control and Query Queuing
Configuring Resource Pools and Admission Control
Admission Control Sample Scenario


Performance Tuning

Performance Best Practices
Join Performance
Table and Column Statistics
Benchmarking
Controlling Resource Usage
Runtime Filtering
HDFS Caching
HDFS Block Skew
Data Cache for Remote Reads
Testing Impala Performance
EXPLAIN Plans and Query Profiles


Scalability Considerations

Scaling Limits and Guidelines
Dedicated Coordinators
Metadata Management


Partitioning
File Formats

Text Data Files
Parquet Data Files
ORC Data Files
Avro Data Files
RCFile Data Files
SequenceFile Data Files


Using Impala to Query Kudu Tables
HBase Tables
S3 Tables

Configure with Cloudera Manager
Configure from Command Line


ADLS Tables
Logging
Impala Client Access

The Impala Shell

Configuration Options
Connecting to impalad
Running Commands and SQL Statements
Command Reference


Configuring Impala to Work with ODBC
Configuring Impala to Work with JDBC


Troubleshooting Impala

Web User Interface
Breakpad Minidumps


Ports Used by Impala
Impala Reserved Words
Impala Frequently Asked Questions


Kafka

Setup
Cloudera Manager
Clients
Brokers
Integration

Security
Managing Multiple Kafka Versions
Managing Topics across Multiple Kafka Clusters
Setting up an End-to-End Data Streaming Pipeline
Developing Kafka Clients
Metrics


Administration

Administration Basics
Broker Migration
User Limits for Kafka
Quotas
Kafka Command Line Tools
Disk Management
JBOD

Setup and Migration


Delegation Tokens

Enable Delegation Tokens
Managing Individual Delegation Tokens
Rotating the Master Key/Secret
Client Authentication
Kafka Security Hardening with Zookeeper ACLs


Kafka Streams


Performance Tuning

Handling Large Messages
Cluster Sizing
Broker Configuration
System-Level Broker Tuning
Kafka-ZooKeeper Performance Tuning


Reference

Metrics Reference
Useful Shell Command Reference


Kafka Public APIs
FAQ


Kudu

Concepts and Architecture
Usage Limitations
Installation and Upgrade
Configuration
Administration
Developing Applications with Kudu
Using Apache Impala with Kudu
Using the Hive Metastore with Kudu
Schema Design
Transaction Semantics
Background Tasks
Scaling Guide
Troubleshooting
More Resources


Oozie

Configuration

Configuring an External Database for Oozie
Oozie High Availability
Configuring Oozie to Use HDFS HA
Oozie Authentication
Using Sqoop Actions with Oozie
Configuring Oozie to Enable MapReduce Jobs To Read/Write from Amazon S3
Configuring Oozie to Enable MapReduce Jobs To Read/Write from Microsoft Azure (ADLS)


Oozie

Starting, Stopping, and Accessing the Oozie Server
Adding the Oozie Service Using Cloudera Manager
Redeploying the Oozie ShareLib
Configuring Oozie Data Purge Settings Using Cloudera Manager
Dumping and Loading an Oozie Database Using Cloudera Manager
Adding Schema to Oozie Using Cloudera Manager
Enabling the Oozie Web Console on Managed Clusters
Enabling Oozie SLA with Cloudera Manager
Setting the Oozie Database Timezone
Scheduling in Oozie Using Cron-like Syntax




Phoenix

Release Notes
Prerequisites
Installing Apache Phoenix using Cloudera Manager
Using Apache Phoenix to Store and Access Data

Orchestrating SQL and APIs with Apache Phoenix
Configuring Phoenix Query Server

Connecting to PQS


Creating and Using User-Defined Functions (UDFs) in Phoenix
Mapping Phoenix Schemas to HBase Namespaces
Associating Tables of a Schema to a Namespace
Using Phoenix Client to Load Data
Using the Index in Phoenix


Understanding Apache Phoenix-Spark Connector
Understanding Apache Phoenix-Hive Connector
Performance Tuning
Frequently Asked Questions
Uninstalling Phoenix Parcel


Search

Search

Understanding
Search and Other CDH Components
Architecture
Tasks and Processes


Tutorial

Validating Search Deployment
Preparing to Index Sample Tweets
Using MapReduce Batch Indexing to Index Sample Tweets
Near Real Time (NRT) Indexing Tweets Using Flume
Using Hue with Search


Deployment Planning

Schemaless Mode


Deploying

Using Search through a Proxy for High Availability
Using Custom JAR Files with Search
Cloudera Search Security

Enable Kerberos Authentication in Cloudera Search




Managing

Configuration
Collections
solrctl Reference
Example solrctl Usage
Migrating Solr Replicas
Backing Up and Restoring


ETL with Cloudera Morphlines

Example Morphline Usage


Indexing Data

Near Real Time Indexing

Flume NRT Indexing

Flume MorphlineSolrSink Configuration Options
Flume MorphlineInterceptor Configuration Options
Flume Solr UUIDInterceptor Configuration Options
Flume Solr BlobHandler Configuration Options
Flume Solr BlobDeserializer Configuration Options


Lily HBase NRT Indexing

Using the Lily HBase NRT Indexer Service
Configuring Lily HBase Indexer Security




Batch Indexing

Spark Indexing
MapReduce Indexing

MapReduceIndexerTool
Lily HBase Batch Indexing






FAQ
Troubleshooting

Configuration and Log Files
Identifying Problems
Solr Query Returns no Documents when Executed with a Non-Privileged User




Sentry

Before You Install Sentry
Installing and Upgrading the Sentry Service
Configuring

Sentry High Availability
Enabling Sentry Authorization for Impala
Configuring Sentry Authorization for Cloudera Search


Using & Managing

Synchronizing HDFS ACLs and Sentry Permissions
Authorization Privilege Model for Hive and Impala
Authorization Privilege Model for Cloudera Search
Hive SQL Syntax for Use with Sentry
Object Ownership
Using the Sentry Web Server
Sentry Debugging and Failure Scenarios


Troubleshooting
How-To Guides

Enabling High Availability
Verify HDFS ACL Sync
Managing Table Access in Hue




Spark

Running Your First Spark Application
Troubleshooting for Spark
Frequently Asked Questions about Apache Spark in CDH
Spark Application Overview
Developing Spark Applications

Developing and Running a Spark WordCount Application
Using Spark Streaming
Using Spark SQL
Using Spark MLlib
Accessing External Storage

Accessing Data Stored in Amazon S3 through Spark
Accessing Data Stored in Azure Data Lake Store (ADLS) through Spark
Accessing Avro Data Files From Spark SQL Applications
Accessing Parquet Files From Spark SQL Applications


Building Spark Applications
Configuring Spark Applications


Running Spark Applications

Running Spark Applications on YARN
Using PySpark

Running Spark Python Applications
Spark and IPython and Jupyter Notebooks


Tuning Spark Applications


Spark and Hadoop Integration

Building and Running a Crunch Application with Spark




File Formats and Compression

Parquet

Predicate Pushdown in Parquet


Avro
Data Compression
Snappy Compression




Glossary





To read this documentation, you must turn JavaScript on.




Impala Tutorials

This section includes tutorial scenarios that demonstrate how to begin using Impala once the software is installed. It focuses on techniques for loading data, because once you have some
data in tables and can query that data, you can quickly progress to more advanced Impala features.
Note:
Where practical, the tutorials take you from "ground zero" to having the desired Impala tables and data. In some cases, you might need to download additional files
from outside sources, set up additional software components, modify commands or scripts to fit your own configuration, or substitute your own sample data.

Before trying these tutorial lessons, install Impala using one of these procedures:

If you already have some CDH environment set up and just need to add Impala to it, add the Impala service using the instructions in Adding a Service. Make sure to also install the Hive metastore service
if you do not already have Hive configured.
To set up Impala and all its prerequisites at once, in a minimal configuration that you can use for small-scale experiments, set up the Cloudera QuickStart VM, which
includes CDH and Impala on CentOS. Use this single-node VM to try out basic SQL functionality, not anything related to performance and scalability. For more information, see the Cloudera QuickStart VM.


Continue reading:

Tutorials for Getting Started
Advanced Tutorials
Dealing with Parquet Files with Unknown Schema




Tutorials for Getting Started

These tutorials demonstrate the basics of using Impala. They are intended for first-time users, and for trying out Impala on any new cluster to make sure the major components are working
correctly.

Continue reading:

Explore a New Impala Instance
Load CSV Data from Local Files
Point an Impala Table at Existing Data Files
Describe the Impala Table
Query the Impala Table
Data Loading and Querying Examples




Explore a New Impala Instance

This tutorial demonstrates techniques for finding your way around the tables and databases of an unfamiliar (possibly empty) Impala instance.
When you connect to an Impala instance for the first time, you use the SHOW DATABASES and SHOW TABLES statements to view the
most common types of objects. Also, call the version() function to confirm which version of Impala you are running; the version number is important when consulting
documentation and dealing with support issues.
A completely empty Impala instance contains no tables, but still has two databases:

default, where new tables are created when you do not specify any other database.
_impala_builtins, a system database used to hold all the built-in functions.

The following example shows how to see the available databases, and the tables in each. If the list of databases or tables is long, you can use wildcard notation to locate specific
databases or tables based on their names.
$ impala-shell -i localhost --quiet
Starting Impala Shell without Kerberos authentication
Welcome to the Impala shell. Press TAB twice to see a list of available commands.

Copyright (c) 2012 Cloudera, Inc. All rights reserved.

(Shell build version: Impala Shell v...
[localhost:21000] > select version();
+-------------------------------------------
| version()
+-------------------------------------------
| impalad version ...
| Built on ...
+-------------------------------------------
[localhost:21000] > show databases;
+--------------------------+
| name                     |
+--------------------------+
| _impala_builtins         |
| ctas                     |
| d1                       |
| d2                       |
| d3                       |
| default                  |
| explain_plans            |
| external_table           |
| file_formats             |
| tpc                      |
+--------------------------+
[localhost:21000] > select current_database();
+--------------------+
| current_database() |
+--------------------+
| default            |
+--------------------+
[localhost:21000] > show tables;
+-------+
| name  |
+-------+
| ex_t  |
| t1    |
+-------+
[localhost:21000] > show tables in d3;

[localhost:21000] > show tables in tpc;
+------------------------+
| name                   |
+------------------------+
| city                   |
| customer               |
| customer_address       |
| customer_demographics  |
| household_demographics |
| item                   |
| promotion              |
| store                  |
| store2                 |
| store_sales            |
| ticket_view            |
| time_dim               |
| tpc_tables             |
+------------------------+
[localhost:21000] > show tables in tpc like 'customer*';
+-----------------------+
| name                  |
+-----------------------+
| customer              |
| customer_address      |
| customer_demographics |
+-----------------------+

Once you know what tables and databases are available, you descend into a database with the USE statement. To understand the structure of each table, you
use the DESCRIBE command. Once inside a database, you can issue statements such as INSERT and SELECT that
operate on particular tables.
The following example explores a database named TPC whose name we learned in the previous example. It shows how to filter the table names within a database
based on a search string, examine the columns of a table, and run queries to examine the characteristics of the table data. For example, for an unfamiliar table you might want to know the number of
rows, the number of different values for a column, and other properties such as whether the column contains any NULL values. When sampling the actual data values from a
table, use a LIMIT clause to avoid excessive output if the table contains more rows or distinct values than you expect.
[localhost:21000] > use tpc;
[localhost:21000] > show tables like '*view*';
+-------------+
| name        |
+-------------+
| ticket_view |
+-------------+
[localhost:21000] > describe city;
+-------------+--------+---------+
| name        | type   | comment |
+-------------+--------+---------+
| id          | int    |         |
| name        | string |         |
| countrycode | string |         |
| district    | string |         |
| population  | int    |         |
+-------------+--------+---------+
[localhost:21000] > select count(*) from city;
+----------+
| count(*) |
+----------+
| 0        |
+----------+
[localhost:21000] > desc customer;
+------------------------+--------+---------+
| name                   | type   | comment |
+------------------------+--------+---------+
| c_customer_sk          | int    |         |
| c_customer_id          | string |         |
| c_current_cdemo_sk     | int    |         |
| c_current_hdemo_sk     | int    |         |
| c_current_addr_sk      | int    |         |
| c_first_shipto_date_sk | int    |         |
| c_first_sales_date_sk  | int    |         |
| c_salutation           | string |         |
| c_first_name           | string |         |
| c_last_name            | string |         |
| c_preferred_cust_flag  | string |         |
| c_birth_day            | int    |         |
| c_birth_month          | int    |         |
| c_birth_year           | int    |         |
| c_birth_country        | string |         |
| c_login                | string |         |
| c_email_address        | string |         |
| c_last_review_date     | string |         |
+------------------------+--------+---------+
[localhost:21000] > select count(*) from customer;
+----------+
| count(*) |
+----------+
| 100000   |
+----------+
[localhost:21000] > select count(distinct c_birth_month) from customer;
+-------------------------------+
| count(distinct c_birth_month) |
+-------------------------------+
| 12                            |
+-------------------------------+
[localhost:21000] > select count(*) from customer where c_email_address is null;
+----------+
| count(*) |
+----------+
| 0        |
+----------+
[localhost:21000] > select distinct c_salutation from customer limit 10;
+--------------+
| c_salutation |
+--------------+
| Mr.          |
| Ms.          |
| Dr.          |
|              |
| Miss         |
| Sir          |
| Mrs.         |
+--------------+

When you graduate from read-only exploration, you use statements such as CREATE DATABASE and CREATE TABLE to set up your own
database objects.
The following example demonstrates creating a new database holding a new table. Although the last example ended inside the TPC database, the new
EXPERIMENTS database is not nested inside TPC; all databases are arranged in a single top-level list.
[localhost:21000] > create database experiments;
[localhost:21000] > show databases;
+--------------------------+
| name                     |
+--------------------------+
| _impala_builtins         |
| ctas                     |
| d1                       |
| d2                       |
| d3                       |
| default                  |
| experiments              |
| explain_plans            |
| external_table           |
| file_formats             |
| tpc                      |
+--------------------------+
[localhost:21000] > show databases like 'exp*';
+---------------+
| name          |
+---------------+
| experiments   |
| explain_plans |
+---------------+

The following example creates a new table, T1. To illustrate a common mistake, it creates this table inside the wrong database, the TPC database where the previous example ended. The ALTER TABLE statement lets you move the table to the intended database, EXPERIMENTS, as part of a rename operation. The USE statement is always needed to switch to a new database, and the current_database() function confirms which database the session is in, to avoid these kinds of mistakes.
[localhost:21000] > create table t1 (x int);

[localhost:21000] > show tables;
+------------------------+
| name                   |
+------------------------+
| city                   |
| customer               |
| customer_address       |
| customer_demographics  |
| household_demographics |
| item                   |
| promotion              |
| store                  |
| store2                 |
| store_sales            |
| t1                     |
| ticket_view            |
| time_dim               |
| tpc_tables             |
+------------------------+
[localhost:21000] > select current_database();
+--------------------+
| current_database() |
+--------------------+
| tpc                |
+--------------------+
[localhost:21000] > alter table t1 rename to experiments.t1;
[localhost:21000] > use experiments;
[localhost:21000] > show tables;
+------+
| name |
+------+
| t1   |
+------+
[localhost:21000] > select current_database();
+--------------------+
| current_database() |
+--------------------+
| experiments        |
+--------------------+

For your initial experiments with tables, you can use ones with just a few columns and a few rows, and text-format data files.
Note: As you graduate to more realistic scenarios, you will use more elaborate tables with many columns, features such as partitioning, and file
formats such as Parquet. When dealing with realistic data volumes, you will bring in data using LOAD DATA or INSERT ... SELECT statements
to operate on millions or billions of rows at once.
The following example sets up a couple of simple tables with a few rows, and performs queries involving sorting, aggregate functions and joins.
[localhost:21000] > insert into t1 values (1), (3), (2), (4);
[localhost:21000] > select x from t1 order by x desc;
+---+
| x |
+---+
| 4 |
| 3 |
| 2 |
| 1 |
+---+
[localhost:21000] > select min(x), max(x), sum(x), avg(x) from t1;
+--------+--------+--------+--------+
| min(x) | max(x) | sum(x) | avg(x) |
+--------+--------+--------+--------+
| 1      | 4      | 10     | 2.5    |
+--------+--------+--------+--------+

[localhost:21000] > create table t2 (id int, word string);
[localhost:21000] > insert into t2 values (1, "one"), (3, "three"), (5, 'five');
[localhost:21000] > select word from t1 join t2 on (t1.x = t2.id);
+-------+
| word  |
+-------+
| one   |
| three |
+-------+

After completing this tutorial, you should now know:

How to tell which version of Impala is running on your system.
How to find the names of databases in an Impala instance, either displaying the full list or searching for specific names.
How to find the names of tables in an Impala database, either displaying the full list or searching for specific names.
How to switch between databases and check which database you are currently in.
How to learn the column names and types of a table.
How to create databases and tables, insert small amounts of test data, and run simple queries.




Load CSV Data from Local Files

This scenario illustrates how to create some very small tables, suitable for first-time users to experiment with Impala SQL features. TAB1 and TAB2 are loaded with data from files in HDFS. A subset of data is copied from TAB1 into TAB3.
Populate HDFS with the data you want to query. To begin this process, create one or more new subdirectories underneath your user directory in HDFS. The data for each table resides in a
separate subdirectory. Substitute your own username for username where appropriate. This example uses the -p option with the mkdir operation to create any necessary parent directories if they do not already exist.
$ whoami
username
$ hdfs dfs -ls /user
Found 3 items
drwxr-xr-x   - username username            0 2013-04-22 18:54 /user/username
drwxrwx---   - mapred   mapred              0 2013-03-15 20:11 /user/history
drwxr-xr-x   - hue      supergroup          0 2013-03-15 20:10 /user/hive

$ hdfs dfs -mkdir -p /user/username/sample_data/tab1 /user/username/sample_data/tab2
Here is some sample data, for two tables named TAB1 and TAB2.
Copy the following content to .csv files in your local filesystem:
tab1.csv:
1,true,123.123,2012-10-24 08:55:00
2,false,1243.5,2012-10-25 13:40:00
3,false,24453.325,2008-08-22 09:33:21.123
4,false,243423.325,2007-05-12 22:32:21.33454
5,true,243.325,1953-04-22 09:11:33

tab2.csv:
1,true,12789.123
2,false,1243.5
3,false,24453.325
4,false,2423.3254
5,true,243.325
60,false,243565423.325
70,true,243.325
80,false,243423.325
90,true,243.325

Put each .csv file into a separate HDFS directory using commands like the following, which use paths available in the Impala Demo VM:
$ hdfs dfs -put tab1.csv /user/username/sample_data/tab1
$ hdfs dfs -ls /user/username/sample_data/tab1
Found 1 items
-rw-r--r--   1 username username        192 2013-04-02 20:08 /user/username/sample_data/tab1/tab1.csv


$ hdfs dfs -put tab2.csv /user/username/sample_data/tab2
$ hdfs dfs -ls /user/username/sample_data/tab2
Found 1 items
-rw-r--r--   1 username username        158 2013-04-02 20:09 /user/username/sample_data/tab2/tab2.csv

The name of each data file is not significant. In fact, when Impala examines the contents of the data directory for the first time, it considers all files in the directory to make up the
data of the table, regardless of how many files there are or what the files are named.
To understand what paths are available within your own HDFS filesystem and what the permissions are for the various directories and files, issue hdfs dfs -ls
/ and work your way down the tree doing -ls operations for the various directories.
Use the impala-shell command to create tables, either interactively or through a SQL script.
The following example shows creating three tables. For each table, the example shows creating columns with various attributes such as Boolean or integer types. The example also includes
commands that provide information about how the data is formatted, such as rows terminating with commas, which makes sense in the case of importing data from a .csv
file. Where we already have .csv files containing data in the HDFS directory tree, we specify the location of the directory containing the appropriate .csv file. Impala considers all the data from all the files in that directory to represent the data for the table.
DROP TABLE IF EXISTS tab1;
-- The EXTERNAL clause means the data is located outside the central location
-- for Impala data files and is preserved when the associated Impala table is dropped.
-- We expect the data to already exist in the directory specified by the LOCATION clause.
CREATE EXTERNAL TABLE tab1
(
   id INT,
   col_1 BOOLEAN,
   col_2 DOUBLE,
   col_3 TIMESTAMP
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/user/username/sample_data/tab1';

DROP TABLE IF EXISTS tab2;
-- TAB2 is an external table, similar to TAB1.
CREATE EXTERNAL TABLE tab2
(
   id INT,
   col_1 BOOLEAN,
   col_2 DOUBLE
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/user/username/sample_data/tab2';

DROP TABLE IF EXISTS tab3;
-- Leaving out the EXTERNAL clause means the data will be managed
-- in the central Impala data directory tree. Rather than reading
-- existing data files when the table is created, we load the
-- data after creating the table.
CREATE TABLE tab3
(
   id INT,
   col_1 BOOLEAN,
   col_2 DOUBLE,
   month INT,
   day INT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

Note: Getting through these CREATE TABLE statements successfully is an important validation step to confirm
everything is configured correctly with the Hive metastore and HDFS permissions. If you receive any errors during the CREATE TABLE statements:

Make sure the hive.metastore.warehouse.dir property points to a directory that Impala can write to. The ownership should be hive:hive, and the impala user should also be a member of the hive group.
If the value of hive.metastore.warehouse.dir is different in the Cloudera Manager dialogs and in the Hive shell, you might need to
designate the hosts running
impalad with the "gateway" role for Hive, and deploy the client configuration files to those hosts.





Point an Impala Table at Existing Data Files

A convenient way to set up data for Impala to access is to use an external table, where the data already exists in a set of HDFS files and you just point the Impala table at the
directory containing those files. For example, you might run in impala-shell a *.sql file with contents similar to the following, to
create an Impala table that accesses an existing data file used by Hive.
The following examples set up 2 tables, referencing the paths and sample data from the sample TPC-DS kit for Impala. For historical reasons, the data physically resides in an HDFS
directory tree under /user/hive, although this particular data is entirely managed by Impala rather than Hive. When we create an external table, we specify the
directory containing one or more data files, and Impala queries the combined content of all the files inside that directory. Here is how we examine the directories and files within the HDFS
filesystem:
$ cd ~/username/datasets
$ ./tpcds-setup.sh
... Downloads and unzips the kit, builds the data and loads it into HDFS ...
$ hdfs dfs -ls /user/hive/tpcds/customer
Found 1 items
-rw-r--r--   1 username supergroup   13209372 2013-03-22 18:09 /user/hive/tpcds/customer/customer.dat
$ hdfs dfs -cat /user/hive/tpcds/customer/customer.dat | more
1|AAAAAAAABAAAAAAA|980124|7135|32946|2452238|2452208|Mr.|Javier|Lewis|Y|9|12|1936|CHILE||Javie
r.Lewis@VFAxlnZEvOx.org|2452508|
2|AAAAAAAACAAAAAAA|819667|1461|31655|2452318|2452288|Dr.|Amy|Moses|Y|9|4|1966|TOGO||Amy.Moses@
Ovk9KjHH.com|2452318|
3|AAAAAAAADAAAAAAA|1473522|6247|48572|2449130|2449100|Miss|Latisha|Hamilton|N|18|9|1979|NIUE||
Latisha.Hamilton@V.com|2452313|
4|AAAAAAAAEAAAAAAA|1703214|3986|39558|2450030|2450000|Dr.|Michael|White|N|7|6|1983|MEXICO||Mic
hael.White@i.org|2452361|
5|AAAAAAAAFAAAAAAA|953372|4470|36368|2449438|2449408|Sir|Robert|Moran|N|8|5|1956|FIJI||Robert.
Moran@Hh.edu|2452469|
...

Here is a SQL script to set up Impala tables pointing to some of these data files in HDFS. (The script in the VM sets up tables like this through Hive; ignore those tables for purposes
of this demonstration.) Save the following as customer_setup.sql:
--
-- store_sales fact table and surrounding dimension tables only
--
create database tpcds;
use tpcds;

drop table if exists customer;
create external table customer
(
    c_customer_sk             int,
    c_customer_id             string,
    c_current_cdemo_sk        int,
    c_current_hdemo_sk        int,
    c_current_addr_sk         int,
    c_first_shipto_date_sk    int,
    c_first_sales_date_sk     int,
    c_salutation              string,
    c_first_name              string,
    c_last_name               string,
    c_preferred_cust_flag     string,
    c_birth_day               int,
    c_birth_month             int,
    c_birth_year              int,
    c_birth_country           string,
    c_login                   string,
    c_email_address           string,
    c_last_review_date        string
)
row format delimited fields terminated by '|'
location '/user/hive/tpcds/customer';

drop table if exists customer_address;
create external table customer_address
(
    ca_address_sk             int,
    ca_address_id             string,
    ca_street_number          string,
    ca_street_name            string,
    ca_street_type            string,
    ca_suite_number           string,
    ca_city                   string,
    ca_county                 string,
    ca_state                  string,
    ca_zip                    string,
    ca_country                string,
    ca_gmt_offset             float,
    ca_location_type          string
)
row format delimited fields terminated by '|'
location '/user/hive/tpcds/customer_address';

We would run this script with a command such as:
impala-shell -i localhost -f customer_setup.sql



Describe the Impala Table

Now that you have updated the database metadata that Impala caches, you can confirm that the expected tables are accessible by Impala and examine the attributes of one of the tables. We
created these tables in the database named default. If the tables were in a database other than the default, we would issue a command use
db_name to switch to that database before examining or querying its tables. We could also qualify the name of a table by prepending the database name, for
example default.customer and default.customer_name.
[impala-host:21000] > show databases
Query finished, fetching results ...
default
Returned 1 row(s) in 0.00s
[impala-host:21000] > show tables
Query finished, fetching results ...
customer
customer_address
Returned 2 row(s) in 0.00s
[impala-host:21000] > describe customer_address
+------------------+--------+---------+
| name             | type   | comment |
+------------------+--------+---------+
| ca_address_sk    | int    |         |
| ca_address_id    | string |         |
| ca_street_number | string |         |
| ca_street_name   | string |         |
| ca_street_type   | string |         |
| ca_suite_number  | string |         |
| ca_city          | string |         |
| ca_county        | string |         |
| ca_state         | string |         |
| ca_zip           | string |         |
| ca_country       | string |         |
| ca_gmt_offset    | float  |         |
| ca_location_type | string |         |
+------------------+--------+---------+
Returned 13 row(s) in 0.01



Query the Impala Table

You can query data contained in the tables. Impala coordinates the query execution across a single node or multiple nodes depending on your configuration, without the overhead of running
MapReduce jobs to perform the intermediate processing.
There are a variety of ways to execute queries on Impala:

Using the impala-shell command in interactive mode:
$ impala-shell -i impala-host
Connected to localhost:21000
[impala-host:21000] > select count(*) from customer_address;
50000
Returned 1 row(s) in 0.37s

Passing a set of commands contained in a file:
$ impala-shell -i impala-host -f myquery.sql
Connected to localhost:21000
50000
Returned 1 row(s) in 0.19s
Passing a single command to the impala-shell command. The query is executed, the results are returned, and the shell exits. Make sure to quote the
command, preferably with single quotation marks to avoid shell expansion of characters such as *.
$ impala-shell -i impala-host -q 'select count(*) from customer_address'
Connected to localhost:21000
50000
Returned 1 row(s) in 0.29s




Data Loading and Querying Examples

This section describes how to create some sample tables and load data into them. These tables can then be queried using the Impala shell.


Loading Data

Loading data involves:

Establishing a data set. The example below uses .csv files.
Creating tables to which to load data.
Loading the data into the tables you created.




Sample Queries

To run these sample queries, create a SQL query file query.sql, copy and paste each query into the query file, and then run the query file using the shell.
For example, to run query.sql on impala-host, you might use the command:
impala-shell.sh -i impala-host -f query.sql
The examples and results below assume you have loaded the sample data into the tables as described above.

Example: Examining Contents of Tables
Let's start by verifying that the tables do contain the data we expect. Because Impala often deals with tables containing millions or billions of rows, when examining tables of unknown
size, include the LIMIT clause to avoid huge amounts of unnecessary output, as in the final query. (If your interactive query starts displaying an unexpected volume of
data, press Ctrl-C in impala-shell to cancel the query.)
SELECT * FROM tab1;
SELECT * FROM tab2;
SELECT * FROM tab2 LIMIT 5;
Results:
+----+-------+------------+-------------------------------+
| id | col_1 | col_2      | col_3                         |
+----+-------+------------+-------------------------------+
| 1  | true  | 123.123    | 2012-10-24 08:55:00           |
| 2  | false | 1243.5     | 2012-10-25 13:40:00           |
| 3  | false | 24453.325  | 2008-08-22 09:33:21.123000000 |
| 4  | false | 243423.325 | 2007-05-12 22:32:21.334540000 |
| 5  | true  | 243.325    | 1953-04-22 09:11:33           |
+----+-------+------------+-------------------------------+

+----+-------+---------------+
| id | col_1 | col_2         |
+----+-------+---------------+
| 1  | true  | 12789.123     |
| 2  | false | 1243.5        |
| 3  | false | 24453.325     |
| 4  | false | 2423.3254     |
| 5  | true  | 243.325       |
| 60 | false | 243565423.325 |
| 70 | true  | 243.325       |
| 80 | false | 243423.325    |
| 90 | true  | 243.325       |
+----+-------+---------------+

+----+-------+-----------+
| id | col_1 | col_2     |
+----+-------+-----------+
| 1  | true  | 12789.123 |
| 2  | false | 1243.5    |
| 3  | false | 24453.325 |
| 4  | false | 2423.3254 |
| 5  | true  | 243.325   |
+----+-------+-----------+

Example: Aggregate and Join
SELECT tab1.col_1, MAX(tab2.col_2), MIN(tab2.col_2)
FROM tab2 JOIN tab1 USING (id)
GROUP BY col_1 ORDER BY 1 LIMIT 5;
Results:
+-------+-----------------+-----------------+
| col_1 | max(tab2.col_2) | min(tab2.col_2) |
+-------+-----------------+-----------------+
| false | 24453.325       | 1243.5          |
| true  | 12789.123       | 243.325         |
+-------+-----------------+-----------------+

Example: Subquery, Aggregate and Joins
SELECT tab2.*
FROM tab2,
(SELECT tab1.col_1, MAX(tab2.col_2) AS max_col2
 FROM tab2, tab1
 WHERE tab1.id = tab2.id
 GROUP BY col_1) subquery1
WHERE subquery1.max_col2 = tab2.col_2;
Results:
+----+-------+-----------+
| id | col_1 | col_2     |
+----+-------+-----------+
| 1  | true  | 12789.123 |
| 3  | false | 24453.325 |
+----+-------+-----------+

Example: INSERT Query
INSERT OVERWRITE TABLE tab3
SELECT id, col_1, col_2, MONTH(col_3), DAYOFMONTH(col_3)
FROM tab1 WHERE YEAR(col_3) = 2012;
Query TAB3 to check the result:
SELECT * FROM tab3;

Results:
+----+-------+---------+-------+-----+
| id | col_1 | col_2   | month | day |
+----+-------+---------+-------+-----+
| 1  | true  | 123.123 | 10    | 24  |
| 2  | false | 1243.5  | 10    | 25  |
+----+-------+---------+-------+-----+





Advanced Tutorials

These tutorials walk you through advanced scenarios or specialized features.

Continue reading:

Attaching an External Partitioned Table to an HDFS Directory Structure
Switching Back and Forth Between Impala and Hive
Cross Joins and Cartesian Products with the CROSS JOIN Operator




Attaching an External Partitioned Table to an HDFS Directory Structure

This tutorial shows how you might set up a directory tree in HDFS, put data files into the lowest-level subdirectories, and then use an Impala external table to query the data files from
their original locations.
The tutorial uses a table with web log data, with separate subdirectories for the year, month, day, and host. For simplicity, we use a tiny amount of CSV data, loading the same data into
each partition.
First, we make an Impala partitioned table for CSV data, and look at the underlying HDFS directory structure to understand the directory structure to re-create elsewhere in HDFS. The
columns field1, field2, and field3 correspond to the contents of the CSV data files. The year, month, day, and host columns are all represented as subdirectories within the
table structure, and are not part of the CSV files. We use STRING for each of these columns so that we can produce consistent subdirectory names, with leading zeros for
a consistent length.
create database external_partitions;
use external_partitions;
create table logs (field1 string, field2 string, field3 string)
  partitioned by (year string, month string , day string, host string)
  row format delimited fields terminated by ',';
insert into logs partition (year="2013", month="07", day="28", host="host1") values ("foo","foo","foo");
insert into logs partition (year="2013", month="07", day="28", host="host2") values ("foo","foo","foo");
insert into logs partition (year="2013", month="07", day="29", host="host1") values ("foo","foo","foo");
insert into logs partition (year="2013", month="07", day="29", host="host2") values ("foo","foo","foo");
insert into logs partition (year="2013", month="08", day="01", host="host1") values ("foo","foo","foo");

Back in the Linux shell, we examine the HDFS directory structure. (Your Impala data directory might be in a different location; for historical reasons, it is sometimes under the HDFS
path /user/hive/warehouse.) We use the hdfs dfs -ls command to examine the nested subdirectories corresponding to each partitioning
column, with separate subdirectories at each level (with = in their names) representing the different values for each partitioning column. When we get to the lowest
level of subdirectory, we use the hdfs dfs -cat command to examine the data file and see CSV-formatted data produced by the INSERT
statement in Impala.
$ hdfs dfs -ls /user/impala/warehouse/external_partitions.db
Found 1 items
drwxrwxrwt   - impala hive          0 2013-08-07 12:24 /user/impala/warehouse/external_partitions.db/logs
$ hdfs dfs -ls /user/impala/warehouse/external_partitions.db/logs
Found 1 items
drwxr-xr-x   - impala hive          0 2013-08-07 12:24 /user/impala/warehouse/external_partitions.db/logs/year=2013
$ hdfs dfs -ls /user/impala/warehouse/external_partitions.db/logs/year=2013
Found 2 items
drwxr-xr-x   - impala hive          0 2013-08-07 12:23 /user/impala/warehouse/external_partitions.db/logs/year=2013/month=07
drwxr-xr-x   - impala hive          0 2013-08-07 12:24 /user/impala/warehouse/external_partitions.db/logs/year=2013/month=08
$ hdfs dfs -ls /user/impala/warehouse/external_partitions.db/logs/year=2013/month=07
Found 2 items
drwxr-xr-x   - impala hive          0 2013-08-07 12:22 /user/impala/warehouse/external_partitions.db/logs/year=2013/month=07/day=28
drwxr-xr-x   - impala hive          0 2013-08-07 12:23 /user/impala/warehouse/external_partitions.db/logs/year=2013/month=07/day=29
$ hdfs dfs -ls /user/impala/warehouse/external_partitions.db/logs/year=2013/month=07/day=28
Found 2 items
drwxr-xr-x   - impala hive          0 2013-08-07 12:21 /user/impala/warehouse/external_partitions.db/logs/year=2013/month=07/day=28/host=host1
drwxr-xr-x   - impala hive          0 2013-08-07 12:22 /user/impala/warehouse/external_partitions.db/logs/year=2013/month=07/day=28/host=host2
$ hdfs dfs -ls /user/impala/warehouse/external_partitions.db/logs/year=2013/month=07/day=28/host=host1
Found 1 items
-rw-r--r--   3 impala hive         12 2013-08-07 12:21 /user/impala/warehouse/external_partiti
ons.db/logs/year=2013/month=07/day=28/host=host1/3981726974111751120--8907184999369517436_822630111_data.0
$ hdfs dfs -cat /user/impala/warehouse/external_partitions.db/logs/year=2013/month=07/day=28/\
host=host1/3981726974111751120--8 907184999369517436_822630111_data.0
foo,foo,foo

Still in the Linux shell, we use hdfs dfs -mkdir to create several data directories outside the HDFS directory tree that Impala controls (/user/impala/warehouse in this example, maybe different in your case). Depending on your configuration, you might need to log in as a user with permission to write into this HDFS
directory tree; for example, the commands shown here were run while logged in as the hdfs user.
hdfs dfs -mkdir -p /user/impala/data/logs/year=2013/month=07/day=28/host=host1
hdfs dfs -mkdir -p /user/impala/data/logs/year=2013/month=07/day=28/host=host2
hdfs dfs -mkdir -p /user/impala/data/logs/year=2013/month=07/day=28/host=host1
hdfs dfs -mkdir -p /user/impala/data/logs/year=2013/month=07/day=29/host=host1
hdfs dfs -mkdir -p /user/impala/data/logs/year=2013/month=08/day=01/host=host1
We make a tiny CSV file, with values different than in the INSERT statements used earlier, and put a copy within each subdirectory that we will use as an
Impala partition.
$ cat >dummy_log_data
bar,baz,bletch
hdfs dfs -mkdir -p /user/impala/data/external_partitions/year=2013/month=08/day=01/host=host1
hdfs dfs -mkdir -p /user/impala/data/external_partitions/year=2013/month=07/day=28/host=host1
hdfs dfs -mkdir -p /user/impala/data/external_partitions/year=2013/month=07/day=28/host=host2
hdfs dfs -mkdir -p /user/impala/data/external_partitions/year=2013/month=07/day=29/host=host1
hdfs dfs -put dummy_log_data /user/impala/data/logs/year=2013/month=07/day=28/host=host1
$ hdfs dfs -put dummy_log_data /user/impala/data/logs/year=2013/month=07/day=28/host=host2
hdfs dfs -put dummy_log_data /user/impala/data/logs/year=2013/month=07/day=29/host=host1
$ hdfs dfs -put dummy_log_data /user/impala/data/logs/year=2013/month=08/day=01/host=host1

Back in the impala-shell interpreter, we move the original Impala-managed table aside, and create a new external table with a
LOCATION clause pointing to the directory under which we have set up all the partition subdirectories and data files.
use external_partitions;
alter table logs rename to logs_original;
create external table logs (field1 string, field2 string, field3 string)
  partitioned by (year string, month string, day string, host string)
  row format delimited fields terminated by ','
  location '/user/impala/data/logs';

Because partition subdirectories and data files come and go during the data lifecycle, you must identify each of the partitions through an ALTER TABLE
statement before Impala recognizes the data files they contain.
alter table logs add partition (year="2013",month="07",day="28",host="host1")
alter table log_type add partition (year="2013",month="07",day="28",host="host2");
alter table log_type add partition (year="2013",month="07",day="29",host="host1");
alter table log_type add partition (year="2013",month="08",day="01",host="host1");

We issue a REFRESH statement for the table, always a safe practice when data files have been manually added, removed, or changed. Then the data is ready to
be queried. The SELECT * statement illustrates that the data from our trivial CSV file was recognized in each of the partitions where we copied it. Although in this
case there are only a few rows, we include a LIMIT clause on this test query just in case there is more data than we expect.
refresh log_type;
select * from log_type limit 100;
+--------+--------+--------+------+-------+-----+-------+
| field1 | field2 | field3 | year | month | day | host  |
+--------+--------+--------+------+-------+-----+-------+
| bar    | baz    | bletch | 2013 | 07    | 28  | host1 |
| bar    | baz    | bletch | 2013 | 08    | 01  | host1 |
| bar    | baz    | bletch | 2013 | 07    | 29  | host1 |
| bar    | baz    | bletch | 2013 | 07    | 28  | host2 |
+--------+--------+--------+------+-------+-----+-------+



Switching Back and Forth Between Impala and Hive

Sometimes, you might find it convenient to switch to the Hive shell to perform some data loading or transformation operation, particularly on file formats such as RCFile, SequenceFile,
and Avro that Impala currently can query but not write to.
Whenever you create, drop, or alter a table or other kind of object through Hive, the next time you switch back to the impala-shell interpreter,
issue a one-time INVALIDATE METADATA statement so that Impala recognizes the new or changed object.
Whenever you load, insert, or change data in an existing table through Hive (or even through manual HDFS operations such as the hdfs command), the
next time you switch back to the impala-shell interpreter, issue a one-time REFRESH table_name
statement so that Impala recognizes the new or changed data.
For examples showing how this process works for the REFRESH statement, look at the examples of creating RCFile and SequenceFile tables in Impala, loading
data through Hive, and then querying the data through Impala. See Using the RCFile File Format with Impala Tables and Using the SequenceFile File Format with Impala Tables for those examples.
For examples showing how this process works for the INVALIDATE METADATA statement, look at the example of creating and loading an Avro table in Hive, and
then querying the data through Impala. See Using the Avro File Format with Impala Tables for that example.
Note:
Originally, Impala did not support UDFs, but this feature is available in Impala starting in Impala 1.2. Some INSERT ... SELECT
transformations that you originally did through Hive can now be done through Impala. See User-Defined Functions (UDFs) for details.
Prior to Impala 1.2, the REFRESH and INVALIDATE METADATA statements needed to be issued on each Impala node
to which you connected and issued queries. In Impala 1.2 and higher, when you issue either of those statements on any Impala node, the results are broadcast to all the Impala nodes in the cluster,
making it truly a one-step operation after each round of DDL or ETL operations in Hive.




Cross Joins and Cartesian Products with the CROSS JOIN Operator

Originally, Impala restricted join queries so that they had to include at least one equality comparison between the columns of the tables on each side of the join operator. With the huge
tables typically processed by Impala, any miscoded query that produced a full Cartesian product as a result set could consume a huge amount of cluster resources.
In Impala 1.2.2 and higher, this restriction is lifted when you use the CROSS JOIN operator in the query. You still cannot remove all WHERE clauses from a query like SELECT * FROM t1 JOIN t2 to produce all combinations of rows from both tables. But you can use the CROSS JOIN operator to explicitly request such a Cartesian product. Typically, this operation is applicable for smaller tables, where the result set still fits within the memory of
a single Impala node.
The following example sets up data for use in a series of comic books where characters battle each other. At first, we use an equijoin query, which only allows characters from the same
time period and the same planet to meet.
[localhost:21000] > create table heroes (name string, era string, planet string);
[localhost:21000] > create table villains (name string, era string, planet string);
[localhost:21000] > insert into heroes values
                  > ('Tesla','20th century','Earth'),
                  > ('Pythagoras','Antiquity','Earth'),
                  > ('Zopzar','Far Future','Mars');
Inserted 3 rows in 2.28s
[localhost:21000] > insert into villains values
                  > ('Caligula','Antiquity','Earth'),
                  > ('John Dillinger','20th century','Earth'),
                  > ('Xibulor','Far Future','Venus');
Inserted 3 rows in 1.93s
[localhost:21000] > select concat(heroes.name,' vs. ',villains.name) as battle
                  > from heroes join villains
                  > where heroes.era = villains.era and heroes.planet = villains.planet;
+--------------------------+
| battle                   |
+--------------------------+
| Tesla vs. John Dillinger |
| Pythagoras vs. Caligula  |
+--------------------------+
Returned 2 row(s) in 0.47s
Readers demanded more action, so we added elements of time travel and space travel so that any hero could face any villain. Prior to Impala 1.2.2, this type of query was impossible
because all joins had to reference matching values between the two tables:
[localhost:21000] > -- Cartesian product not possible in Impala 1.1.
                  > select concat(heroes.name,' vs. ',villains.name) as battle from heroes join villains;
ERROR: NotImplementedException: Join between 'heroes' and 'villains' requires at least one conjunctive equality predicate between the two tables
With Impala 1.2.2, we rewrite the query slightly to use CROSS JOIN rather than JOIN, and now the result set includes all
combinations:
[localhost:21000] > -- Cartesian product available in Impala 1.2.2 with the CROSS JOIN syntax.
                  > select concat(heroes.name,' vs. ',villains.name) as battle from heroes cross join villains;
+-------------------------------+
| battle                        |
+-------------------------------+
| Tesla vs. Caligula            |
| Tesla vs. John Dillinger      |
| Tesla vs. Xibulor             |
| Pythagoras vs. Caligula       |
| Pythagoras vs. John Dillinger |
| Pythagoras vs. Xibulor        |
| Zopzar vs. Caligula           |
| Zopzar vs. John Dillinger     |
| Zopzar vs. Xibulor            |
+-------------------------------+
Returned 9 row(s) in 0.33s
The full combination of rows from both tables is known as the Cartesian product. This type of result set is often used for creating grid data structures. You can also filter the result
set by including WHERE clauses that do not explicitly compare columns between the two tables. The following example shows how you might produce a list of combinations
of year and quarter for use in a chart, and then a shorter list with only selected quarters.
[localhost:21000] > create table x_axis (x int);
[localhost:21000] > create table y_axis (y int);
[localhost:21000] > insert into x_axis values (1),(2),(3),(4);
Inserted 4 rows in 2.14s
[localhost:21000] > insert into y_axis values (2010),(2011),(2012),(2013),(2014);
Inserted 5 rows in 1.32s
[localhost:21000] > select y as year, x as quarter from x_axis cross join y_axis;
+------+---------+
| year | quarter |
+------+---------+
| 2010 | 1       |
| 2011 | 1       |
| 2012 | 1       |
| 2013 | 1       |
| 2014 | 1       |
| 2010 | 2       |
| 2011 | 2       |
| 2012 | 2       |
| 2013 | 2       |
| 2014 | 2       |
| 2010 | 3       |
| 2011 | 3       |
| 2012 | 3       |
| 2013 | 3       |
| 2014 | 3       |
| 2010 | 4       |
| 2011 | 4       |
| 2012 | 4       |
| 2013 | 4       |
| 2014 | 4       |
+------+---------+
Returned 20 row(s) in 0.38s
[localhost:21000] > select y as year, x as quarter from x_axis cross join y_axis where x in (1,3);
+------+---------+
| year | quarter |
+------+---------+
| 2010 | 1       |
| 2011 | 1       |
| 2012 | 1       |
| 2013 | 1       |
| 2014 | 1       |
| 2010 | 3       |
| 2011 | 3       |
| 2012 | 3       |
| 2013 | 3       |
| 2014 | 3       |
+------+---------+
Returned 10 row(s) in 0.39s



Dealing with Parquet Files with Unknown Schema

As data pipelines start to include more aspects such as NoSQL or loosely specified schemas, you might encounter situations where you have data files (particularly in Parquet format)
where you do not know the precise table definition. This tutorial shows how you can build an Impala table around data that comes from non-Impala or even non-SQL sources, where you do not have control
of the table layout and might not be familiar with the characteristics of the data.
The data used in this tutorial represents airline on-time arrival statistics, from October 1987 through April 2008. See the details on the 2009 ASA Data Expo web site. You can also see the explanations of the columns; for purposes of this exercise, wait until after following the tutorial before examining the schema, to better simulate a real-life situation where you cannot
rely on assumptions and assertions about the ranges and representations of data values.


Download the Data Files into HDFS

First, we download and unpack the data files. There are 8 files totalling 1.4 GB.
$ wget -O airlines_parquet.tar.gz https://home.apache.org/~arodoni/airlines_parquet.tar.gz
$ wget https://home.apache.org/~arodoni/airlines_parquet.tar.gz.sha512
$ shasum -a 512 -c airlines_parquet.tar.gz.sha512
airlines_parquet.tar.gz: OK

$ tar xvzf airlines_parquet.tar.gz

$ cd airlines_parquet/

$ du -kch *.parq
253M   4345e5eef217aa1b-c8f16177f35fd983_1150363067_data.0.parq
14M    4345e5eef217aa1b-c8f16177f35fd983_1150363067_data.1.parq
253M   4345e5eef217aa1b-c8f16177f35fd984_501176748_data.0.parq
64M    4345e5eef217aa1b-c8f16177f35fd984_501176748_data.1.parq
184M   4345e5eef217aa1b-c8f16177f35fd985_1199995767_data.0.parq
241M   4345e5eef217aa1b-c8f16177f35fd986_2086627597_data.0.parq
212M   4345e5eef217aa1b-c8f16177f35fd987_1048668565_data.0.parq
152M   4345e5eef217aa1b-c8f16177f35fd988_1432111844_data.0.parq
1.4G   total
Next, we put the Parquet data files in HDFS, all together in a single directory, with permissions on the directory and the files so that the impala user
will be able to read them.
After unpacking, we saw the largest Parquet file was 253 MB. When copying Parquet files into HDFS for Impala to use, for maximum query performance, make sure that each file resides in a
single HDFS data block. Therefore, we pick a size larger than any single file and specify that as the block size, using the argument -Ddfs.block.size=253m on the
hdfs dfs -put command.
$ sudo -u hdfs hdfs dfs -mkdir -p /user/impala/staging/airlines
$ sudo -u hdfs hdfs dfs -Ddfs.block.size=253m -put *.parq /user/impala/staging/airlines
$ sudo -u hdfs hdfs dfs -ls /user/impala/staging
Found 1 items

$ sudo -u hdfs hdfs dfs -ls /user/impala/staging/airlines
Found 8 items



Create Database and Tables

With the files in an accessible location in HDFS, you create a database table that uses the data in those files:

The CREATE EXTERNAL syntax and the LOCATION attribute point Impala at the appropriate HDFS directory.
The LIKE PARQUET 'path_to_any_parquet_file' clause means we skip the list of column names and types; Impala
automatically gets the column names and data types straight from the data files. (Currently, this technique only works for Parquet files.)
Ignore the warning about lack of READ_WRITE access to the files in HDFS; the impala user can read the files, which will be
sufficient for us to experiment with queries and perform some copy and transform operations into other tables.


$ impala-shell
> CREATE DATABASE airlines_data;
  USE airlines_data;
  CREATE EXTERNAL TABLE airlines_external
  LIKE PARQUET 'hdfs:staging/airlines/4345e5eef217aa1b-c8f16177f35fd983_1150363067_data.0.parq'
  STORED AS PARQUET LOCATION 'hdfs:staging/airlines';
WARNINGS: Impala does not have READ_WRITE access to path 'hdfs://myhost.com:8020/user/impala/staging'



Examine Physical and Logical Schema

With the table created, we examine its physical and logical characteristics to confirm that the data is really there and in a format and shape that we can work with.

The SHOW TABLE STATS statement gives a very high-level summary of the table, showing how many files and how much total data it contains. Also, it
confirms that the table is expecting all the associated data files to be in Parquet format. (The ability to work with all kinds of HDFS data files in different formats means that it is possible to
have a mismatch between the format of the data files, and the format that the table expects the data files to be in.)
The SHOW FILES statement confirms that the data in the table has the expected number, names, and sizes of the original Parquet files.
The DESCRIBE statement (or its abbreviation DESC) confirms the names and types of the columns that Impala automatically
created after reading that metadata from the Parquet file.
The DESCRIBE FORMATTED statement prints out some extra detail along with the column definitions. The pieces we care about for this exercise are:

The containing database for the table.
The location of the associated data files in HDFS.
The table is an external table so Impala will not delete the HDFS files when we finish the experiments and drop the table.
The table is set up to work exclusively with files in the Parquet format.




> SHOW TABLE STATS airlines_external;
+-------+--------+--------+--------------+-------------------+---------+-------------------+
| #Rows | #Files | Size   | Bytes Cached | Cache Replication | Format  | Incremental stats |
+-------+--------+--------+--------------+-------------------+---------+-------------------+
| -1    | 8      | 1.34GB | NOT CACHED   | NOT CACHED        | PARQUET | false             |
+-------+--------+--------+--------------+-------------------+---------+-------------------+

> SHOW FILES IN airlines_external;
+----------------------------------------------------------------------------------------+----------+-----------+
| path                                                                                   | size     | partition |
+----------------------------------------------------------------------------------------+----------+-----------+
| /user/impala/staging/airlines/4345e5eef217aa1b-c8f16177f35fd983_1150363067_data.0.parq | 252.99MB |           |
| /user/impala/staging/airlines/4345e5eef217aa1b-c8f16177f35fd983_1150363067_data.1.parq | 13.43MB  |           |
| /user/impala/staging/airlines/4345e5eef217aa1b-c8f16177f35fd984_501176748_data.0.parq  | 252.84MB |           |
| /user/impala/staging/airlines/4345e5eef217aa1b-c8f16177f35fd984_501176748_data.1.parq  | 63.92MB  |           |
| /user/impala/staging/airlines/4345e5eef217aa1b-c8f16177f35fd985_1199995767_data.0.parq | 183.64MB |           |
| /user/impala/staging/airlines/4345e5eef217aa1b-c8f16177f35fd986_2086627597_data.0.parq | 240.04MB |           |
| /user/impala/staging/airlines/4345e5eef217aa1b-c8f16177f35fd987_1048668565_data.0.parq | 211.35MB |           |
| /user/impala/staging/airlines/4345e5eef217aa1b-c8f16177f35fd988_1432111844_data.0.parq | 151.46MB |           |
+----------------------------------------------------------------------------------------+----------+-----------+

> DESCRIBE airlines_external;
+---------------------+--------+---------------------------------------------------+
| name                | type   | comment                                           |
+---------------------+--------+---------------------------------------------------+
| year                | int    | Inferred from Parquet file.                       |
| month               | int    | Inferred from Parquet file.                       |
| day                 | int    | Inferred from Parquet file.                       |
| dayofweek           | int    | Inferred from Parquet file.                       |
| dep_time            | int    | Inferred from Parquet file.                        |
| crs_dep_time        | int    | Inferred from Parquet file.                       |
| arr_time            | int    | Inferred from Parquet file.                       |
| crs_arr_time        | int    | Inferred from Parquet file.                       |
| carrier             | string | Inferred from Parquet file.                       |
| flight_num          | int    | Inferred from Parquet file.                       |
| tail_num            | int    | Inferred from Parquet file.                       |
| actual_elapsed_time | int    | Inferred from Parquet file.                       |
| crs_elapsed_time    | int    | Inferred from Parquet file.                       |
| airtime             | int    | Inferred from Parquet file.                       |
| arrdelay            | int    | Inferred from Parquet file.                       |
| depdelay            | int    | Inferred from Parquet file.                       |
| origin              | string | Inferred from Parquet file.                       |
| dest                | string | Inferred from Parquet file.                       |
| distance            | int    | Inferred from Parquet file.                       |
| taxi_in             | int    | Inferred from Parquet file.                       |
| taxi_out            | int    | Inferred from Parquet file.                       |
| cancelled           | int    | Inferred from Parquet file.                       |
| cancellation_code   | string | Inferred from Parquet file.                       |
| diverted            | int    | Inferred from Parquet file.                       |
| carrier_delay       | int    | Inferred from Parquet file.                       |
| weather_delay       | int    | Inferred from Parquet file.                       |
| nas_delay           | int    | Inferred from Parquet file.                       |
| security_delay      | int    | Inferred from Parquet file.                       |
| late_aircraft_delay | int    | Inferred from Parquet file.                       |
+---------------------+--------+---------------------------------------------------+

> DESCRIBE FORMATTED airlines_external;
+------------------------------+-------------------------------
| name                         | type
+------------------------------+-------------------------------
...
| # Detailed Table Information | NULL
| Database:                    | airlines_data
| Owner:                       | impala
...
| Location:                    | /user/impala/staging/airlines
| Table Type:                  | EXTERNAL_TABLE
...
| # Storage Information        | NULL
| SerDe Library:               | org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
| InputFormat:                 | org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputForma
| OutputFormat:                | org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
...



Analyze Data

Now that we are confident that the connections are solid between the Impala table and the underlying Parquet files, we run some initial queries to understand the characteristics of the
data: the overall number of rows, and the ranges and how many different values are in certain columns.
> SELECT COUNT(*) FROM airlines_external;
+-----------+
| count(*)  |
+-----------+
| 123534969 |
+-----------+

The NDV() function returns a number of distinct values, which, for performance reasons, is an estimate when there are lots of different values in the
column, but is precise when the cardinality is less than 16 K. Use NDV() function for this kind of exploration rather than COUNT(DISTINCT
colname), because Impala can evaluate multiple NDV() functions in a single query, but only a single instance of COUNT DISTINCT.
> SElECT NDV(carrier), NDV(flight_num), NDV(tail_num),
  NDV(origin), NDV(dest) FROM airlines_external;
+--------------+-----------------+---------------+-------------+-----------+
| ndv(carrier) | ndv(flight_num) | ndv(tail_num) | ndv(origin) | ndv(dest) |
+--------------+-----------------+---------------+-------------+-----------+
| 29           | 8463            | 3             | 342         | 349       |
+--------------+-----------------+---------------+-------------+-----------+

> SELECT tail_num, COUNT(*) AS howmany FROM airlines_external
  GROUP BY tail_num;
+----------+-----------+
| tail_num | howmany   |
+----------+-----------+
| NULL     | 123122001 |
| 715      | 1         |
| 0        | 406405    |
| 112      | 6562      |
+----------+-----------+

> SELECT DISTINCT dest FROM airlines_external
  WHERE dest NOT IN (SELECT origin FROM airlines_external);
+------+
| dest |
+------+
| CBM  |
| SKA  |
| LAR  |
| RCA  |
| LBF  |
+------+

> SELECT DISTINCT dest FROM airlines_external
  WHERE dest NOT IN (SELECT DISTINCT origin FROM airlines_external);
+------+
| dest |
+------+
| CBM  |
| SKA  |
| LAR  |
| RCA  |
| LBF  |
+------+

> SELECT DISTINCT origin FROM airlines_external
  WHERE origin NOT IN (SELECT DISTINCT dest FROM airlines_external);
Fetched 0 row(s) in 2.63
With the above queries, we see that there are modest numbers of different airlines, flight numbers, and origin and destination airports. Two things jump out from this query: the number
of tail_num values is much smaller than we might have expected, and there are more destination airports than origin airports. Let's dig further. What we find is that
most tail_num values are NULL. It looks like this was an experimental column that wasn't filled in accurately. We make a mental note that
if we use this data as a starting point, we'll ignore this column. We also find that certain airports are represented in the ORIGIN column but not the DEST column; now we know that we cannot rely on the assumption that those sets of airport codes are identical.
Note: The first SELECT DISTINCT DEST query takes almost 40 seconds. We expect all queries on such a small data set,
less than 2 GB, to take a few seconds at most. The reason is because the expression NOT IN (SELECT origin FROM airlines_external) produces an intermediate result set of
123 million rows, then runs 123 million comparisons on each data node against the tiny set of destination airports. The way the NOT IN operator works internally means
that this intermediate result set with 123 million rows might be transmitted across the network to each data node in the cluster. Applying another DISTINCT inside the
NOT IN subquery means that the intermediate result set is only 340 items, resulting in much less network traffic and fewer comparison operations. The more efficient
query with the added DISTINCT is approximately 7 times as fast.
Next, we try doing a simple calculation, with results broken down by year. This reveals that some years have no data in the airtime column. That means we
might be able to use that column in queries involving certain date ranges, but we cannot count on it to always be reliable. The question of whether a column contains any NULL values, and if so what is their number, proportion, and distribution, comes up again and again when doing initial exploration of a data set.
> SELECT year, SUM(airtime) FROM airlines_external
  GROUP BY year ORDER BY year DESC;
+------+--------------+
| year | sum(airtime) |
+------+--------------+
| 2008 | 713050445    |
| 2007 | 748015545    |
| 2006 | 720372850    |
| 2005 | 708204026    |
| 2004 | 714276973    |
| 2003 | 665706940    |
| 2002 | 549761849    |
| 2001 | 590867745    |
| 2000 | 583537683    |
| 1999 | 561219227    |
| 1998 | 538050663    |
| 1997 | 536991229    |
| 1996 | 519440044    |
| 1995 | 513364265    |
| 1994 | NULL         |
| 1993 | NULL         |
| 1992 | NULL         |
| 1991 | NULL         |
| 1990 | NULL         |
| 1989 | NULL         |
| 1988 | NULL         |
| 1987 | NULL         |
+------+--------------+

With the notion of NULL values in mind, let's come back to the tail_num column that we discovered had a lot of NULLs. Let's quantify the NULL and non-NULL values in that column for better understanding. First, we just count the
overall number of rows versus the non-NULL values in that column. That initial result gives the appearance of relatively few non-NULL
values, but we can break it down more clearly in a single query. Once we have the COUNT(*) and the COUNT(colname) numbers, we can encode that initial query in a WITH clause, then run a follow-on query that performs multiple arithmetic
operations on those values. Seeing that only one-third of one percent of all rows have non-NULL values for the tail_num column clearly
illustrates that column is not of much use.
> SELECT COUNT(*) AS 'rows', COUNT(tail_num) AS 'non-null tail numbers'
  FROM airlines_external;
+-----------+-----------------------+
| rows      | non-null tail numbers |
+-----------+-----------------------+
| 123534969 | 412968                |
+-----------+-----------------------+

> WITH t1 AS
  (SELECT COUNT(*) AS 'rows', COUNT(tail_num) AS 'nonnull'
  FROM airlines_external)
SELECT `rows`, `nonnull`, `rows` - `nonnull` AS 'nulls',
  (`nonnull` / `rows`) * 100 AS 'percentage non-null'
FROM t1;
+-----------+---------+-----------+---------------------+
| rows      | nonnull | nulls     | percentage non-null |
+-----------+---------+-----------+---------------------+
| 123534969 | 412968  | 123122001 | 0.3342923897119365  |
+-----------+---------+-----------+---------------------+

By examining other columns using these techniques, we can form a mental picture of the way data is distributed throughout the table, and which columns are most significant for query
purposes. For this tutorial, we focus mostly on the fields likely to hold discrete values, rather than columns such as actual_elapsed_time whose names suggest they hold
measurements. We would dig deeper into those columns once we had a clear picture of which questions were worthwhile to ask, and what kinds of trends we might look for. For the final piece of initial
exploration, let's look at the year column. A simple GROUP BY query shows that it has a well-defined range, a manageable number of
distinct values, and relatively even distribution of rows across the different years.
> SELECT MIN(year), MAX(year), NDV(year) FROM airlines_external;
+-----------+-----------+-----------+
| min(year) | max(year) | ndv(year) |
+-----------+-----------+-----------+
| 1987      | 2008      | 22        |
+-----------+-----------+-----------+

> SELECT year, COUNT(*) howmany FROM airlines_external
  GROUP BY year ORDER BY year DESC;
+------+---------+
| year | howmany |
+------+---------+
| 2008 | 7009728 |
| 2007 | 7453215 |
| 2006 | 7141922 |
| 2005 | 7140596 |
| 2004 | 7129270 |
| 2003 | 6488540 |
| 2002 | 5271359 |
| 2001 | 5967780 |
| 2000 | 5683047 |
| 1999 | 5527884 |
| 1998 | 5384721 |
| 1997 | 5411843 |
| 1996 | 5351983 |
| 1995 | 5327435 |
| 1994 | 5180048 |
| 1993 | 5070501 |
| 1992 | 5092157 |
| 1991 | 5076925 |
| 1990 | 5270893 |
| 1989 | 5041200 |
| 1988 | 5202096 |
| 1987 | 1311826 |
+------+---------+

We could go quite far with the data in this initial raw format, just as we downloaded it from the web. If the data set proved to be useful and worth persisting in Impala for extensive
queries, we might want to copy it to an internal table, letting Impala manage the data files and perhaps reorganizing a little for higher efficiency. In this next stage of the tutorial, we copy the
original data into a partitioned table, still in Parquet format. Partitioning based on the year column lets us run queries with clauses such as WHERE year = 2001 or WHERE year BETWEEN 1989 AND 1999, which can dramatically cut down on I/O by ignoring all the data from years outside the desired
range. Rather than reading all the data and then deciding which rows are in the matching years, Impala can zero in on only the data files from specific year partitions.
To do this, Impala physically reorganizes the data files, putting the rows from each year into data files in a separate HDFS directory for each year value. Along the
way, we'll also get rid of the tail_num column that proved to be almost entirely NULL.
The first step is to create a new table with a layout very similar to the original airlines_external table. We'll do that by reverse-engineering a
CREATE TABLE statement for the first table, then tweaking it slightly to include a PARTITION BY clause for year, and excluding the tail_num column. The SHOW CREATE TABLE statement gives us the starting point.
Although we could edit that output into a new SQL statement, all the ASCII box characters make such editing inconvenient. To get a more stripped-down CREATE
TABLE to start with, we restart the impala-shell command with the -B option, which turns off the box-drawing behavior.
$ impala-shell -i localhost -B -d airlines_data;

> SHOW CREATE TABLE airlines_external;
"CREATE EXTERNAL TABLE airlines_data.airlines_external (
  year INT COMMENT 'inferred from: optional int32 year',
  month INT COMMENT 'inferred from: optional int32 month',
  day INT COMMENT 'inferred from: optional int32 day',
  dayofweek INT COMMENT 'inferred from: optional int32 dayofweek',
  dep_time INT COMMENT 'inferred from: optional int32 dep_time',
  crs_dep_time INT COMMENT 'inferred from: optional int32 crs_dep_time',
  arr_time INT COMMENT 'inferred from: optional int32 arr_time',
  crs_arr_time INT COMMENT 'inferred from: optional int32 crs_arr_time',
  carrier STRING COMMENT 'inferred from: optional binary carrier',
  flight_num INT COMMENT 'inferred from: optional int32 flight_num',
  tail_num INT COMMENT 'inferred from: optional int32 tail_num',
  actual_elapsed_time INT COMMENT 'inferred from: optional int32 actual_elapsed_time',
  crs_elapsed_time INT COMMENT 'inferred from: optional int32 crs_elapsed_time',
  airtime INT COMMENT 'inferred from: optional int32 airtime',
  arrdelay INT COMMENT 'inferred from: optional int32 arrdelay',
  depdelay INT COMMENT 'inferred from: optional int32 depdelay',
  origin STRING COMMENT 'inferred from: optional binary origin',
  dest STRING COMMENT 'inferred from: optional binary dest',
  distance INT COMMENT 'inferred from: optional int32 distance',
  taxi_in INT COMMENT 'inferred from: optional int32 taxi_in',
  taxi_out INT COMMENT 'inferred from: optional int32 taxi_out',
  cancelled INT COMMENT 'inferred from: optional int32 cancelled',
  cancellation_code STRING COMMENT 'inferred from: optional binary cancellation_code',
  diverted INT COMMENT 'inferred from: optional int32 diverted',
  carrier_delay INT COMMENT 'inferred from: optional int32 carrier_delay',
  weather_delay INT COMMENT 'inferred from: optional int32 weather_delay',
  nas_delay INT COMMENT 'inferred from: optional int32 nas_delay',
  security_delay INT COMMENT 'inferred from: optional int32 security_delay',
  late_aircraft_delay INT COMMENT 'inferred from: optional int32 late_aircraft_delay'
)
STORED AS PARQUET
LOCATION 'hdfs://a1730.example.com:8020/user/impala/staging/airlines'
TBLPROPERTIES ('numFiles'='0', 'COLUMN_STATS_ACCURATE'='false',
  'transient_lastDdlTime'='1439425228', 'numRows'='-1', 'totalSize'='0',
  'rawDataSize'='-1')"

After copying and pasting the CREATE TABLE statement into a text editor for fine-tuning, we quit and restart impala-shell without the -B option, to switch back to regular output.
Next we run the CREATE TABLE statement that we adapted from the SHOW CREATE TABLE output. We kept the STORED AS PARQUET clause because we want to rearrange the data somewhat but still keep it in the high-performance Parquet format. The LOCATION and
TBLPROPERTIES clauses are not relevant for this new table, so we edit those out. Because we are going to partition the new table based on the year column, we move that column name (and its type) into a new PARTITIONED BY clause.
> CREATE TABLE airlines_data.airlines
 (month INT,
  day INT,
  dayofweek INT,
  dep_time INT,
  crs_dep_time INT,
  arr_time INT,
  crs_arr_time INT,
  carrier STRING,
  flight_num INT,
  actual_elapsed_time INT,
  crs_elapsed_time INT,
  airtime INT,
  arrdelay INT,
  depdelay INT,
  origin STRING,
  dest STRING,
  distance INT,
  taxi_in INT,
  taxi_out INT,
  cancelled INT,
  cancellation_code STRING,
  diverted INT,
  carrier_delay INT,
  weather_delay INT,
  nas_delay INT,
  security_delay INT,
  late_aircraft_delay INT)
PARTITIONED BY (year INT)
STORED AS PARQUET
;

Next, we copy all the rows from the original table into this new one with an INSERT statement. (We edited the CREATE TABLE
statement to make an INSERT statement with the column names in the same order.) The only change is to add a PARTITION(year) clause, and
move the year column to the very end of the SELECT list of the INSERT statement. Specifying PARTITION(year), rather than a fixed value such as PARTITION(year=2000), means that Impala figures out the partition value for each row based on the
value of the very last column in the SELECT list. This is the first SQL statement that legitimately takes any substantial time, because the rows from different years
are shuffled around the cluster; the rows that go into each partition are collected on one node, before being written to one or more new data files.
> INSERT INTO airlines_data.airlines
  PARTITION (year)
  SELECT
    month,
    day,
    dayofweek,
    dep_time,
    crs_dep_time,
    arr_time,
    crs_arr_time,
    carrier,
    flight_num,
    actual_elapsed_time,
    crs_elapsed_time,
    airtime,
    arrdelay,
    depdelay,
    origin,
    dest,
    distance,
    taxi_in,
    taxi_out,
    cancelled,
    cancellation_code,
    diverted,
    carrier_delay,
    weather_delay,
    nas_delay,
    security_delay,
    late_aircraft_delay,
    year
  FROM airlines_data.airlines_external;
Once partitioning or join queries come into play, it's important to have statistics that Impala can use to optimize queries on the corresponding tables. The COMPUTE INCREMENTAL STATS statement is the way to collect statistics for partitioned tables. Then the SHOW TABLE STATS statement confirms that the
statistics are in place for each partition, and also illustrates how many files and how much raw data is in each partition.
> COMPUTE INCREMENTAL STATS airlines;
+-------------------------------------------+
| summary                                   |
+-------------------------------------------+
| Updated 22 partition(s) and 27 column(s). |
+-------------------------------------------+

> SHOW TABLE STATS airlines;
+-------+-----------+--------+----------+--------------+-------------------+---------+-------------------+----------------------------------------------------------------------------------------------------------+
| year  | #Rows     | #Files | Size     | Bytes Cached | Cache Replication | Format  | Incremental stats | Location                                                                                                 |
+-------+-----------+--------+----------+--------------+-------------------+---------+-------------------+----------------------------------------------------------------------------------------------------------+
| 1987  | 1311826   | 1      | 11.75MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=1987 |
| 1988  | 5202096   | 1      | 44.04MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=1988 |
| 1989  | 5041200   | 1      | 46.07MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=1989 |
| 1990  | 5270893   | 1      | 46.25MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=1990 |
| 1991  | 5076925   | 1      | 46.77MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=1991 |
| 1992  | 5092157   | 1      | 48.21MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=1992 |
| 1993  | 5070501   | 1      | 47.46MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=1993 |
| 1994  | 5180048   | 1      | 47.47MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=1994 |
| 1995  | 5327435   | 1      | 62.40MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=1995 |
| 1996  | 5351983   | 1      | 62.93MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=1996 |
| 1997  | 5411843   | 1      | 65.05MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=1997 |
| 1998  | 5384721   | 1      | 62.21MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=1998 |
| 1999  | 5527884   | 1      | 65.10MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=1999 |
| 2000  | 5683047   | 1      | 67.68MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=2000 |
| 2001  | 5967780   | 1      | 74.03MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=2001 |
| 2002  | 5271359   | 1      | 74.00MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=2002 |
| 2003  | 6488540   | 1      | 99.35MB  | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=2003 |
| 2004  | 7129270   | 1      | 123.29MB | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=2004 |
| 2005  | 7140596   | 1      | 120.72MB | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=2005 |
| 2006  | 7141922   | 1      | 121.88MB | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=2006 |
| 2007  | 7453215   | 1      | 130.87MB | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=2007 |
| 2008  | 7009728   | 1      | 123.14MB | NOT CACHED   | NOT CACHED        | PARQUET | true              | hdfs://myhost.com:8020/user/hive/warehouse/airline_data.db/airlines/year=2008 |
| Total | 123534969 | 22     | 1.55GB   | 0B           |                   |         |                   |                                                                                                          |
+-------+-----------+--------+----------+--------------+-------------------+---------+-------------------+----------------------------------------------------------------------------------------------------------+

At this point, we sanity check the partitioning we did. All the partitions have exactly one file, which is on the low side. A query that includes a clause WHERE
year=2004 will only read a single data block; that data block will be read and processed by a single data node; therefore, for a query targeting a single year, all the other nodes in the
cluster will sit idle while all the work happens on a single machine. It's even possible that by chance (depending on HDFS replication factor and the way data blocks are distributed across the
cluster), that multiple year partitions selected by a filter such as WHERE year BETWEEN 1999 AND 2001 could all be read and processed by the same data node. The more
data files each partition has, the more parallelism you can get and the less probability of "hotspots" occurring on particular nodes, therefore a bigger performance boost by
having a big cluster.
However, the more data files, the less data goes in each one. The overhead of dividing the work in a parallel query might not be worth it if each node is only reading a few megabytes. 50
or 100 megabytes is a decent size for a Parquet data block; 9 or 37 megabytes is on the small side. Which is to say, the data distribution we ended up with based on this partitioning scheme is on the
borderline between sensible (reasonably large files) and suboptimal (few files in each partition). The way to see how well it works in practice is to run the same queries against the original flat
table and the new partitioned table, and compare times.
Spoiler: in this case, with my particular 4-node cluster with its specific distribution of data blocks and my particular exploratory queries, queries against the partitioned table do
consistently run faster than the same queries against the unpartitioned table. But I could not be sure that would be the case without some real measurements. Here are some queries I ran to draw that
conclusion, first against airlines_external (no partitioning), then against AIRLINES (partitioned by year). The AIRLINES queries are consistently faster. Changing the volume of data, changing the size of the cluster, running queries that did or didn't refer to the partition key columns, or
other factors could change the results to favor one table layout or the other.
Note: If you find the volume of each partition is only in the low tens of megabytes, consider lowering the granularity of partitioning. For
example, instead of partitioning by year, month, and day, partition by year and month or even just by year. The ideal layout to distribute work efficiently in a parallel query is many tens or even
hundreds of megabytes per Parquet file, and the number of Parquet files in each partition somewhat higher than the number of data nodes.
> SELECT SUM(airtime) FROM airlines_external;
+--------------+
| 8662859484   |
+--------------+

> SELECT SUM(airtime) FROM airlines;
+--------------+
| 8662859484   |
+--------------+

> SELECT SUM(airtime) FROM airlines_external WHERE year = 2005;
+--------------+
| 708204026    |
+--------------+

> SELECT SUM(airtime) FROM airlines WHERE year = 2005;
+--------------+
| 708204026    |
+--------------+

Now we can finally analyze this data set that from the raw data files and we didn't know what columns they contained. Let's see whether the airtime of a
flight tends to be different depending on the day of the week. We can see that the average is a little higher on day number 6; perhaps Saturday is a busy flying day and planes have to circle for
longer at the destination airport before landing.
> SELECT dayofweek, AVG(airtime) FROM airlines
  GROUP BY dayofweek ORDER BY dayofweek;
+-----------+-------------------+
| dayofweek | avg(airtime)      |
+-----------+-------------------+
| 1         | 102.1560425016671 |
| 2         | 102.1582931538807 |
| 3         | 102.2170009256653 |
| 4         | 102.37477661846   |
| 5         | 102.2697358763511 |
| 6         | 105.3627448363705 |
| 7         | 103.4144351202054 |
+-----------+-------------------+

To see if the apparent trend holds up over time, let's do the same breakdown by day of week, but also split up by year. Now we can see that day number 6 consistently has a higher average
air time in each year. We can also see that the average air time increased over time across the board. And the presence of NULL for this column in years 1987 to 1994
shows that queries involving this column need to be restricted to a date range of 1995 and higher.
> SELECT year, dayofweek, AVG(airtime) FROM airlines
  GROUP BY year, dayofweek ORDER BY year DESC, dayofweek;
+------+-----------+-------------------+
| year | dayofweek | avg(airtime)      |
+------+-----------+-------------------+
| 2008 | 1         | 103.1821651651355 |
| 2008 | 2         | 103.2149301386094 |
| 2008 | 3         | 103.0585076622796 |
| 2008 | 4         | 103.4671383539038 |
| 2008 | 5         | 103.5575385182659 |
| 2008 | 6         | 107.4006306562128 |
| 2008 | 7         | 104.8648851041755 |
| 2007 | 1         | 102.2196114337825 |
| 2007 | 2         | 101.9317791906348 |
| 2007 | 3         | 102.0964767689043 |
| 2007 | 4         | 102.6215927201686 |
| 2007 | 5         | 102.4289399000661 |
| 2007 | 6         | 105.1477448215756 |
| 2007 | 7         | 103.6305945644095 |
...
| 1996 | 1         | 99.33860750862108 |
| 1996 | 2         | 99.54225446396656 |
| 1996 | 3         | 99.41129336113134 |
| 1996 | 4         | 99.5110373340348  |
| 1996 | 5         | 99.22120745027595 |
| 1996 | 6         | 101.1717447111921 |
| 1996 | 7         | 99.95410136133704 |
| 1995 | 1         | 96.93779698300494 |
| 1995 | 2         | 96.93458674589712 |
| 1995 | 3         | 97.00972311337051 |
| 1995 | 4         | 96.90843832024412 |
| 1995 | 5         | 96.78382115425562 |
| 1995 | 6         | 98.70872826057003 |
| 1995 | 7         | 97.85570478374616 |
| 1994 | 1         | NULL              |
| 1994 | 2         | NULL              |
| 1994 | 3         | NULL              |
...
| 1987 | 5         | NULL              |
| 1987 | 6         | NULL              |
| 1987 | 7         | NULL              |
+------+-----------+-------------------+




Categories: Data Analysts | Developers | File Formats | Getting Started | Impala | Parquet | Querying | SQL | Schemas | Tables | Tutorials | All Categories



Designing Schemas


Administration


















About Cloudera
Resources
Contact
Careers
Press
Documentation

United States: +1 888 789 1488
Outside the US: +1 650 362 0488



© 2021 Cloudera, Inc. All rights reserved. Apache Hadoop and associated open source project names are trademarks of the Apache Software Foundation. For a complete list of trademarks, click here.
If this documentation includes code, including but not limited to, code examples, Cloudera makes this available to you under the terms of the Apache License, Version 2.0, including any required
notices. A copy of the Apache License Version 2.0 can be found here.










Terms & Conditions  |  Privacy Policy

Page generated September 29, 2021.












