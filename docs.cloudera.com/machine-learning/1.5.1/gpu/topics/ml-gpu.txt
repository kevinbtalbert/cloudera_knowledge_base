Using GPUs for Cloudera Machine Learning projectsCloudera Docs
Using GPUs for Cloudera Machine Learning projects

A GPU is
         a specialized processor that can be used to accelerate highly parallelized
         computationally-intensive workloads. Because of their computational power, GPUs have been
         found to be particularly well-suited to deep learning workloads. Ideally, CPUs and GPUs should be
         used in tandem for data engineering and data science workloads. A typical machine learning
         workflow involves data preparation, model training, model scoring, and model fitting. You
         can use existing general-purpose CPUs for each stage of the workflow, and optionally
         accelerate the math-intensive steps with the selective application of special-purpose GPUs.
         For example, GPUs allow you to accelerate model fitting using frameworks such as Tensorflow, PyTorch, and Keras.
By
         enabling GPU support, data scientists can share GPU resources available on Cloudera Machine
         Learning workspaces. Users can request a specific number of GPU instances, up to the total
         number available, which are then allocated to the running session or job for the duration
         of the run. 
For information on installing your GPUs, see CDP Private Cloud
            Data Services Installation Software Requirements, below.
Enabling GPUs on ML Workspaces
If you are using a Legacy Engine, to enable GPU usage on Cloudera Machine Learning, select
         GPUs when you are provisioning the workspace. If your existing workspace does not have GPUs
         provisioned, contact your ML administrator to provision a new one for you. For
         instructions, see Provisioning ML Workspaces.

Related informationCDP Private Cloud Experiences Installation Software RequirementsProvision an ML WorkspaceTesting GPU SetupGPU node setup