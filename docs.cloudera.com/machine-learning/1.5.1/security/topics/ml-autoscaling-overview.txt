Autoscaling Workloads with KubernetesCloudera Docs
Autoscaling Workloads with Kubernetes

Autoscaling on Private Cloud
CML on Private Cloud supports application autoscaling on multiple fronts. Additional
        compute resources are utilized when users self-provision sessions, run jobs, and utilize
        other compute capabilities. Within a session, users can also leverage the worker API to
        launch resources necessary to host TensorFlow, PyTorch, or other distributed applications.
        Spark on Kubernetes scales up to any number of executors as requested by the user at
        runtime.


