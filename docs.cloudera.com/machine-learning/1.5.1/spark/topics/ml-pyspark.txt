Using Spark 2 from PythonCloudera Docs
Using Spark 2 from Python
Cloudera Machine Learning supports using Spark 2 from Python via PySpark. This topic
    describes how to set up and test a PySpark project. 
PySpark Environment Variables
The default Cloudera Machine Learning engine currently includes
          Python 2.7.17 and Python 3.6.9. To use PySpark with lambda functions that
        run within the CDH cluster, the Spark executors must have access to a matching version of
        Python. For many common operating systems, the default system Python will not match the
        minor release of Python included in Machine Learning.
To ensure that the Python versions match, Python
        can either be installed on every CDH host or made available per job run
        using Sparkâ€™s ability to distribute dependencies. Given the size of a
        typical isolated Python environment, Cloudera recommends installing
        Python 2.7 and 3.6 on the cluster if you are using PySpark with lambda
        functions.
You can install Python 2.7 and 3.6 on the
        cluster using any method and set the corresponding
          PYSPARK_PYTHON environment variable in
        your project. Cloudera Machine Learning includes a separate environment
        variable for Python 3 sessions called
          PYSPARK3_PYTHON. Python 2 sessions
        continue to use the default PYSPARK_PYTHON
        variable. This will allow you to run Python 2 and Python 3 sessions in
        parallel without either variable being overridden by the other.

Creating and Running a PySpark Project
To get started quickly, use the PySpark
          template project to create a new project. For instructions, see Create a Project
          from a Built-in Template.
To run a PySpark project, navigate to the project's overview page, open
        the workbench console and launch a Python session. For detailed instructions, see
          Native Workbench Console and Editor.


Testing a PySpark Project in Spark Local Mode
Spark's local mode is often useful for testing and debugging
         purposes. Use the following sample code snippet to start a PySpark
         session in local mode.

from pyspark.sql import SparkSession

spark = SparkSession\
    .builder \
    .appName("LocalSparkSession") \
    .master("local") \
    .getOrCreate()


For more details, refer to the Spark documentation: Running Spark
          Application.


Related informationNative Workbench Console and EditorExample: Montecarlo EstimationExample: Locating and Adding JARs to Spark 2 Configuration