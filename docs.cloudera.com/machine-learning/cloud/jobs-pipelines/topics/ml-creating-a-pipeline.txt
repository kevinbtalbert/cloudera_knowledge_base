Creating a PipelineCloudera Docs
Creating a Pipeline
This topic describes how to create a scheduled pipeline of jobs
    within a project.

As data science projects mature beyond ad hoc
        scripts, you might want to break them up into multiple steps. For
        example, a project may include one or more data acquisition, data
        cleansing, and finally, data analytics steps. For such projects,
        Cloudera Machine Learning allows you to schedule multiple jobs to
        run one after another in what is called a pipeline, where each job is dependent on the output of the one
        preceding it. 
The Jobs overview presents a list of all
        existing jobs created for a project along with a dependency graph to
        display any pipelines you've created. Job dependencies do not need to be
        configured at the time of job creation. Pipelines can be created after
        the fact by modifying the jobs to establish dependencies between them.
        From the job overview, you can modify the settings of a job, access the
        history of all job runs, and view the session output for individual job
        runs.
Let's take an example of a project that has two jobs, Read Weblogs and
        Write Weblogs. Given that you must read the data before you can run
        analyses and write to it, the Write Weblogs job should only be triggered
        after the Read Weblogs job completes a successful run. To create such a
        two-step pipeline:

Navigate to the project where the Read Weblogs and Write
          Weblogs jobs were created. Click Jobs.From the list of jobs, select Write Weblogs.Click the Settings tab.Click on the Schedule dropdown and select Dependent.
          Select Read Weblogs from the dropdown list of existing jobs in the
          project.Click Update Job.

