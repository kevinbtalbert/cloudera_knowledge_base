Autoscale GroupsCloudera Docs
Autoscale Groups
A Cloudera Machine Learning (CML) workspace or cluster consists of three different
    autoscaling groups: “infra”, “cpu” and “gpu”. These groups scale independently of one another. 
Infra Autoscaling Group 
The Infra autoscaling group is created automatically when a user provisions a CML cluster,
      and is not configurable from the UI. This group is meant to run the core CML services that are
      critical to the overall functioning of the workspace. This group is loosely analogous to the
      master node of legacy CDSW, however it can scale up or down if necessary. The instance count
      for this group ranges from 1 to 3, with the default set to 2. The instance type used for the
      group is m5.2xlarge on AWS, and Standard DS4 v2
      on Azure. On AWS, this group is restricted to a single availability zone (AZ) because it
      relies on Elastic Block Store (EBS).
CPU Autoscaling Group 
The CPU autoscaling group forms the main worker nodes of a CML cluster, and is somewhat
      configurable from the UI. The user can choose from three different instance types, and can
      also set the autoscaling range from 0 to 30 CPU worker nodes. This group is meant to run
      general CPU-only workloads. This group is available in multiple availability zones. 
GPU Autoscaling Group (not available on Azure)
The GPU autoscaling group consists of instances that have GPUs, and are meant for workloads
      that require GPU processing. Like the CPU group, this group is configurable from the UI.
      Unlike the CPU group, this group is meant exclusively for sessions that request > 0 GPUs,
      and are therefore fenced off from CPU-only workloads, in part because GPU instance types are
      much more expensive than regular instance types. This group is available in multiple
      availability zones. 

Parent topic: Autoscaling Workloads with Kubernetes