Basic Concepts and TerminologyCloudera Docs
Basic Concepts and Terminology
Starting with the current CML release, Engines are deprecated. We recommend using ML
    Runtimes for all new projects from now on. You can also migrate existing Engine-based projects
    to ML Runtimes. Engines are still supported, but new features will only be available for ML
    Runtimes.
In the context of Cloudera Machine Learning, engines are responsible for running data science
      workloads and intermediating access to the underlying cluster. Cloudera Machine Learning uses
      Docker containers to deliver application components and run isolated user workloads. On a per
      project basis, users can run R, Python, and Scala workloads with different versions of
      libraries and system packages. CPU and memory are also isolated, ensuring reliable, scalable
      execution in a multi-tenant setting. 
Cloudera Machine Learning engines are responsible for
      running R, Python, and Scala code written by users. You can think of an
      engine as a virtual machine, customized to have all the necessary
      dependencies while keeping each projectâ€™s environment entirely
      isolated.
To enable multiple users and concurrent access, Cloudera Machine Learning
      transparently subdivides and schedules containers across multiple hosts.
      This scheduling is done using Kubernetes, a container orchestration system
      used internally by Cloudera Machine Learning. Neither Docker nor
      Kubernetes are directly exposed to end users, with users interacting with
      Cloudera Machine Learning through a web application.

Base Engine Image

The base engine image is a Docker
            image that contains all the building blocks needed to launch a
            Cloudera Machine Learning session and run a workload. It
            consists of kernels for Python, R,
            and Scala along with additional libraries that can be used to run
            common data analytics operations. When you launch a session to run a
            project, an engine is kicked off
            from a container of this image. The base image itself is built and
            shipped along with Cloudera Machine Learning. 
Cloudera Machine Learning offers legacy engines and Machine Learning Runtimes. Both
            legacy engines and ML Runtimes are Docker images and contain OS, interpreters, and
            libraries to run user code in sessions, jobs, experiments, models, and applications.
            However, there are significant differences between these choices. See ML Runtimes
              versus Legacy Engines for a summary of these differences. 
New versions of the base engine image are
            released periodically. However, existing projects are not
            automatically upgraded to use new engine images. Older images are
            retained to ensure you are able to test code compatibility with the
            new engine before upgrading to it manually. 

Engine

The term engine refers to a virtual machine-style
              environment that is created when you run a project (via session or
              job) in Cloudera Machine Learning. You can use an engine to
              run R, Python, and Scala workloads on data stored in the
              underlying CDH cluster. 
Cloudera Machine Learning allows you to run code using
              either a session or a job. A session is a way to
              interactively launch an engine and run code while a job lets
              you batch process those actions and schedule them to run
              recursively. Each session and job launches its own engine that
              lives as long as the workload is running (or until it times
              out).
A running engine includes the following components: 




Kernel
Each engine runs a kernel with an R, Python or Scala
                    process that can be used to run code within the engine.
                    The kernel launched differs based on the option you select
                    (either Python 2/3, PySpark, R, or Scala) when you launch
                    the session or configure a job. 
The Python kernel is
                    based on the Jupyter IPython kernel; the R kernel is
                    custom-made for CML; and the Scala kernel is based on the
                    Apache Toree kernel.
                  


Project Filesystem Mount
Cloudera Machine Learning uses a persistent
                    filesystem to store project files such as user code,
                    installed libraries, or even small data files. Project files
                    are stored on the master host at
                      /var/lib/cdsw/current/projects.
Every
                    time you launch a new session or run a job for a project, a
                    new engine is created ,and the project filesystem is mounted
                    into the engine's environment at
                    /home/cdsw. Once the session/job ends, the
                    only project artifacts that remain are a log of the workload
                    you ran, and any files that were generated or modified,
                    including libraries you might have installed. All of the
                    installed dependencies persist through the lifetime of the
                    project. The next time you launch a session/job for the same
                    project, those dependencies will be mounted into the engine
                    environment along with the rest of the project filesystem.
                  


Host Mounts
If there are any files on the hosts that
                  should be mounted into the engines at launch time, use the
                  Site Administration panel to include them.For detailed instructions, see
                    Configuring the Engine Environment.






Related informationML Runtimes versus Legacy EnginesConfiguring the Engine Environment