Using GPUs for Cloudera Machine Learning projectsCloudera Docs
Using GPUs for Cloudera Machine Learning projects

A GPU is
         a specialized processor that can be used to accelerate highly parallelized
         computationally-intensive workloads. Because of their computational power, GPUs have been
         found to be particularly well-suited to deep learning workloads. Ideally, CPUs and GPUs should be
         used in tandem for data engineering and data science workloads. A typical machine learning
         workflow involves data preparation, model training, model scoring, and model fitting. You
         can use existing general-purpose CPUs for each stage of the workflow, and optionally
         accelerate the math-intensive steps with the selective application of special-purpose GPUs.
         For example, GPUs allow you to accelerate model fitting using frameworks such as Tensorflow, PyTorch, and Keras.
By
         enabling GPU support, data scientists can share GPU resources available on Cloudera Machine
         Learning workspaces. Users can request a specific number of GPU instances, up to the total
         number available, which are then allocated to the running session or job for the duration
         of the run. 
Enabling GPUs on ML Workspaces
If you are using ML Runtimes, you must use the ML Runtimes version for your Python library
         Nvidia GPU Edition. 
noteNvidia GPU Edition comes with CUDA 11.1 preinstalled.
If you are using a Legacy Engine, to enable GPU usage on Cloudera Machine Learning, select
         GPUs when you are provisioning the workspace. If your existing workspace does not have GPUs
         provisioned, contact your ML administrator to provision a new one for you. For
         instructions, see Provisioning ML Workspaces.
importantReview your cloud service provider account
        limitsFor example, AWS imposes certain default limits for AWS
        services, and you might not have default access to GPU instances at all.
        Make sure you review your account's current usage status and resource
        limits before you start provisioning GPU resources for CML.

Related informationProvisioning ML WorkspacesCustom CUDA-capable Engine ImageSite Admins: Add the Custom CUDA Engine to your Cloudera Machine Learning DeploymentProject Admins: Enable the CUDA Engine for your ProjectTesting GPU Setup