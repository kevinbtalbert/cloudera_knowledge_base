How replication policies workCloudera Docs
How replication policies work
In Replication Manager,
      you create replication policies to establish the rules you want applied to your replication
      jobs. The policy rules you set can include which cluster is the source and which is the
      destination, what data is replicated, what day and time the replication job occurs, the
      frequency of job runs, and bandwidth restrictions.
The first time you run a job (an instance of a policy) with data that has not been
         previously replicated, Replication Manager creates a new folder or database and bootstraps
         the data. During a bootstrap operation, all data is replicated from the source cluster to
         the destination. As a result, the initial execution of a job can take a significant amount
         of time, depending on how much data is being replicated, network bandwidth, and so on. So
         you should plan the bootstrap operation accordingly.
After the bootstrap operation succeeds, an incremental data replication is
         automatically performed. This job synchronizes, between the source and destination
         clusters, any events that occurred during the bootstrap process. After the data is
         synchronized, the replicated data is ready for use on the destination. Data is in a
         consistent state only after incremental replication has captured any new changes that
         occurred during bootstrap.
Subsequent replication jobs from the same source location to the same target on
         the destination are incremental, so only the changed data is copied. 
When a bootstrap operation is interrupted, such as due to a network failure or an
         unrecoverable error, the Replication Manager does not retry the job instead it runs the job
         at the next scheduled interval, if available. Therefore, if the bootstrap operation is
         interrupted, you must manually correct the issue and then run the policy.
When scheduling how often you want a replication job to run, you should consider the
         recovery point objective (RPO) of the data being replicated; that is, what is the
         acceptable lag time between the active site and the replicated data on the destination.

Related informationHDFS replication policyHive replication policyHBase replication policy
Replication policy
        considerations
You should take into
        consideration certain guidelines when creating or modifying a replication policy. It is
        important for you to understand the security restrictions and encryption policies within
        Replication Manager.
The guidelines you need to consider before you create or modify a replication policy
            includes:

Data security
To use an S3 or ABFS cluster for your policy, register your credentials on the
                        Cloud Credentials page.
A user with access to the Replication Manager user interface has the ability to
                    browse, within the Replication Manager UI, the folder structure of any clusters
                    enabled for Replication Manager. Therefore, users can view folders, files,
                        and databases in the Replication Manager user interface, that they might not
                        have access to in HDFS. Users cannot view from the Replication Manager UI
                        the content of files on the source or destination clusters. Nor do these
                        administrators have the ability to modify or delete folders or files that
                        are viewable from the Replication Manager UI.
Policy properties and settings
Consider the recovery point objective (RPO) of the data being replicated when
                    you schedule a replication policy. The RPO is the acceptable lag time between
                    the active site and the replicated data on the destination. Ensure that the
                    frequency is set so that a job finishes before the next job starts. 
Jobs based on the same policy cannot overlap. If a job is not completed before
                    another job starts, the second job does not execute and is given the status
                        Skipped. If a job is consistently skipped, you might
                    need to modify the frequency of the job.
Specify bandwidth per map, in MBps. Each map is restricted to consume only the
                    specified bandwidth. This is not always exact. The map throttles back its
                    bandwidth consumption during a copy in such a way that the net bandwidth
                    used tends towards the specified value.
Cluster requirements


Pair the clusters before you include them in a replication policy.
With a single cluster, you can replicate data on-premises to cloud.
With a single cluster, you cannot replicate data on-premises to
                            on-premises.
If the clusters are Replication Manager-enabled, it appears in the
                                Source Cluster or Destination or
                                Data Lake Cluster fields in the Create
                                Policy wizard. You must ensure that the clusters you
                            select are healthy before you start a policy instance (job). 


Hive restrictions


When creating a schedule for a Hive replication policy, you should set
                            the frequency so that changes are replicated often enough to avoid
                            overly large copies. 
ACID tables, managed tables, storage handler-based tables such as Apache
                            HBase, and column statistics are not replicated. Hive2 managed tables
                            are converted to external tables after replication.




