Spark application modelCloudera Docs
Spark application model

 Apache Spark is widely considered to be the successor to MapReduce for
      general purpose data processing on Apache Hadoop clusters. Like MapReduce
      applications, each Spark application is a self-contained computation that
      runs user-supplied code to compute a result. As with MapReduce jobs, Spark
      applications can use the resources of multiple hosts. However, Spark has
      many advantages over MapReduce. 
 In MapReduce, the highest-level unit of computation is a
        job. A job loads data, applies a map function, shuffles it,
      applies a reduce function, and writes data back out to persistent storage.
      In Spark, the highest-level unit of computation is an
        application. A Spark application can be used for a single
      batch job, an interactive session with multiple jobs, or a long-lived
      server continually satisfying requests. A Spark application can consist of
      more than just a single map and reduce. 
 MapReduce starts a process for each task. In contrast, a Spark
      application can have processes running on its behalf even when it's not
      running a job. Furthermore, multiple tasks can run within the same
      executor. Both combine to enable extremely fast task startup time as well
      as in-memory data storage, resulting in orders of magnitude faster
      performance over MapReduce.

