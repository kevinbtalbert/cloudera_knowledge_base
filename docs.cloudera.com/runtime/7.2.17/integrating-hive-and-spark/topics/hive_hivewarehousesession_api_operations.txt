Introduction to HWC and DataFrame APIsCloudera Docs
Introduction to HWC and DataFrame APIs
As an Apache Spark developer, you learn the code constructs for executing Apache Hive
    queries using the HiveWarehouseSession API. In Spark source code, you see how to create an
    instance of HiveWarehouseSession. You also learn how to access a Hive ACID table using
    DataFrames.
Supported APIs


Spark SQLSupports built-in Spark SQL query read (only) patterns. Output conforms to
              built-in spark.sql conventions.Example
$ spark-shell <parameters to specify HWC jar and config settings>
scala> sql("select * from managedTable").show 
scala> spark.read.table("managedTable").show

HWCSupports HiveWarehouse Session API operations using the HWC sql
              API. The .execute() and .executeQuery() methods are
              deprecated.Examplescala> val hive = com.hortonworks.hwc.HiveWarehouseSession.session(spark).build()
scala> hive.sql("select * from emp_acid").show
scala> hive.sql("select e.emp_id, e.first_name, d.name department from emp_acid e join dept_ext d on e.dept_id = d.id").show
DataFramesSupports accessing a Hive ACID table from Scala, or pySpark, directly
              using DataFrames. Direct reads and writes from the file are not supported.Hive
              ACID tables are tables in Hive metastore and must be formatted using DataFrames as
              follows:Syntaxval df = hive.sql("<SELECT query>")Examplescala> val df = hive.sql("select * from managedTable where a=100")
scala> df.collect()



Import statements and variables
The following string constants are defined by the API:

HIVE_WAREHOUSE_CONNECTOR
DATAFRAME_TO_STREAM
STREAM_TO_STREAM


Assuming spark is running in an existing SparkSession,
        use this code for imports:

Scala
import com.hortonworks.hwc.HiveWarehouseSession
import com.hortonworks.hwc.HiveWarehouseSession._
val hive = HiveWarehouseSession.session(spark).build()



Java
import com.hortonworks.hwc.HiveWarehouseSession;
import static com.hortonworks.hwc.HiveWarehouseSession.*;
HiveWarehouseSession hive = HiveWarehouseSession.session(spark).build();



Python
from pyspark_llap import HiveWarehouseSession
hive = HiveWarehouseSession.session(spark).build()



Executing queries
HWC supports the hive.sql() API for executing queries. You can also use
        the Spark SQL to query Hive managed tables, however, it is recommened that you use the HWC
          sql method.
.sql() 
Executes queries in all the read modes â€” Direct Reader, JDBC, and Secure access
                modes.
Consistent with the Spark sql interface.
Masks the internal implementation based on the cluster type you configured.


Results are returned as a DataFrame to Spark.
noteThe .execute() and
          .executeQuery() methods are deprecated and it is recommended that you use
        the hive.sql() method.

Support of HWC read modes on Hive tables or views
The following table represents the different Hive tables or views that are supported by the
        various HWC read modes:

Mode vs Table
Full ACID table
Insert-only ACID table
View (created on a managed table)
Materialized view (created on a managed table)



DIRECT_READER_V1
Yes
Yes
Yes
Yes


DIRECT_READER_V2
Yes
Yes
Yes
Yes


JDBC_CLUSTER
Yes
Yes
Yes
Yes


SECURE_ACCESS
Yes
Yes
Yes
Yes


It is recommended that you do not use Managed non-transactional tables. Such tables should
        ideally be converted to external tables.

Support of HWC read modes on table formats
The following table formats are supported by HWC while reading a table:

Mode
ORC
Parquet
Avro
Textfile



DIRECT_READER_V1
Yes
Yes
Yes
Yes


DIRECT_READER_V2
Yes
Yes
Yes
Yes


JDBC_CLUSTER
Yes
Yes
Yes
Yes


SECURE_ACCESS
Yes
Yes
Yes
Yes



hive.sql vs. spark.sql
There are a number of important differences between the hive.sql and spark.sql functions:
          
hive.sql() is explicitly defined in HWC and can be used across all
            read modes to query Apache Hive managed tables (full ACID and insert-only ACID
            tables).
spark.sql() can also be used across all the read modes to query an
            Apache Hive managed table. However, it is recommended that you use
              hive.sql() over spark.sql(). 
The Direct Reader imposes the constraint that the Hive table must be
            transactional.



Related informationHMS storageOrc vs ParquetHWC and DataFrame API limitationsThese limitations are in addition to Direct Reader mode, JDBC mode, Secure access mode,   and HWC and DataFrames API limitations.HWC supported types mapping To create HWC API apps, you must know how Hive Warehouse Connector maps Apache Hive     types to Apache Spark types, and vice versa. Awareness of a few unsupported types helps you     avoid problems.Catalog operations Short descriptions and the syntax of catalog operations, which include creating,     dropping, and describing an Apache Hive database and table from Apache Spark, helps you write     HWC API apps. Read and write operationsBrief descriptions of HWC API operations and examples cover how to read and write     Apache Hive tables from Apache Spark. You learn how to update statements and write DataFrames to     partitioned Hive tables, perform batch writes, and use HiveStreaming.Committing a transaction for Direct ReaderFor Direct Reader operations, you need to know how to commit or abort transactions. Closing HiveWarehouseSession operationsYou need to know how to release locks that Apache Spark operations puts on Apache         Hive resources. An example shows how and when to release these locks.Using HWC for streamingWhen using HiveStreaming to write a DataFrame to Apache Hive or an Apache Spark         Stream to Hive, you need to know how to escape any commas in the stream because the Hive         Warehouse Connector uses the commas as the field delimiter.HWC API ExamplesExamples of using the HWC API include how to create the DataFrame from any data source     and include an option to write the DataFrame to an Apache Hive table. Hive Warehouse Connector InterfacesThe HiveWarehouseSession, CreateTableBuilder, and MergeBuilder interfaces present     available HWC operations.Submitting a Scala or Java applicationA step-by-step procedure shows you how to submit an app based on the         HiveWarehouseConnector library to run on Apache Spark Shell.Examples of writing data in various file formatsHive Warehouse Connector (HWC) enables you to write to tables in various formats, such     as Parquet, ORC, AVRO, and Textfile. You see by example how to write a Dataframe in these     formats, to a pre-existing or new table.Parent topic: Introduction to HWC