BasicsCloudera Docs
Basics
A collection of frequently asked questions on the topic of Kafka aimed for beginners. 
What is Kafka?
Kafka is a streaming message platform. Breaking it down a bit further:
“Streaming”: Lots of messages (think tens or hundreds of thousands) being sent
        frequently by publishers ("producers"). Message polling occurring frequently by lots of
        subscribers ("consumers").
“Message”: From a technical standpoint, a key value pair. From a non-technical
        standpoint, a relatively small number of bytes (think hundreds to a few thousand bytes).
If this isn’t your planned use case, Kafka may not be the solution you are
        looking for. Contact your favorite Cloudera representative to discuss and find out. It is
        better to understand what you can and cannot do upfront than to go ahead based on some
        enthusiastic arbitrary vendor message with a solution that will not meet your expectations
        in the end.

What is Kafka designed for?
Kafka was designed at LinkedIn to be a horizontally scaling publish-subscribe system. It
        offers a great deal of configurability at the system- and message-level to achieve these
        performance goals. There are well documented cases that showcase how well Kafka can scale
        when everything is done right. One such example is LinkedIn.

What is Kafka not well fitted for (or what are the tradeoffs)?
It’s very easy to get caught up in all the things that Kafka can be used for
        without considering the tradeoffs. Kafka configuration is also not automatic. You need to
        understand each of your use cases to determine which configuration properties can be used to
        tune (and retune!) Kafka for each use case.
Some more specific examples where you need to be deeply knowledgeable and careful
        when configuring are:

Using Kafka as your microservices communication hubKafka can replace both the
            message queue and the services discovery part of your software infrastructure. However,
            this is generally at the cost of some added latency as well as the need to monitor a new
            complex system (i.e. your Kafka cluster).
Using Kafka as long-term storageWhile Kafka does have a way to configure
            message retention, it’s primarily designed for low latency message delivery. Kafka does
            not have any support for the features that are usually associated with filesystems (such
            as metadata or backups). As such, using some form of long-term ingestion, such as HDFS,
            is recommended instead.
Using Kafka as an end-to-end solutionKafka is only part of a solution. There
            are a lot of best practices to follow and support tools to build before you can get the
            most out of it (see this wise LinkedIn post).
Deploying Kafka without the right supportUber has given some numbers for their
            engineering organization. These numbers could help give you an idea what it takes to
            reach that kind of scale: 1300 microservers, 2000 engineers.


Where can I get a general Kafka overview?
Read Kafka Introduction and Kafka Architecture, which cover the basics and design of Kafka.
        This should serve as a good starting point. If you have any remaining questions, come to
        this FAQ or talk to your favorite Cloudera representative about training or a best practices
        deep dive.

Where does Kafka fit well into an Analytic Database solution?
Analytic Database deployments benefit from Kafka by utilizing it for data ingest. Data can
        then populate tables for various analytics workloads. For ad hoc BI the real-time aspect is
        less critical, but the ability to utilize the same data used in real time applications, in
        BI and analytics as well, is a benefit that Cloudera’s platform provides, as you will have
        Kafka for both purposes, already integrated, secured, governed and centrally managed.

Where does Kafka fit well into an Operational Database solution?
Kafka is commonly used in the real-time, mission-critical world of Operational Database
        deployments. It is used to ingest data and allow immediate serving to other applications and
        services through Kudu or HBase. The benefit of utilizing Kafka in the Cloudera platform for
        Operational Database is the integration, security, governance and central management. You
        avoid the risks and costs of siloed architecture and “yet another solution” to support.

What is a Kafka consumer?
If Kafka is the system that stores messages, then a consumer is the part of your
        system that reads those messages from Kafka.
While Kafka does come with a command line tool that can act as a consumer,
        practically speaking, you will most likely write Java code using the KafkaConsumer API for
        your production system.

What is a Kafka producer?
While consumers read from a Kafka cluster, producers write to a Kafka
        cluster.
Similar to the consumer (see previous question), your producer is also custom
        Java code for your particular use case.
Your producer may need some tuning for write performance and SLA guarantees, but
        will generally be simpler (fewer error cases) to tune than your consumer.

What functionality can I call in my Kafka Java code?
The best way to get more information on what functionality you can call in your Kafka Java
        code is to look at the Java documents. And read very carefully!

What’s a good size of a Kafka record if I care about performance and stability?
There is an older blog post from 2014 from LinkedIn titled: Benchmarking Apache Kafka: 2 Million Writes Per Second (On
          Three Cheap Machines). In the “Effect of Message Size” section, you can see two
        charts which indicate that Kafka throughput starts being affected at a record size of 100
        bytes through 1000 bytes and bottoming out around 10000 bytes. In general, keeping topics
        specific and keeping message sizes deliberately small helps you get the most out of
        Kafka.
Excerpting from Deploying Apache Kafka: A Practical FAQ:



How to send large messages or payloads through Kafka?
Cloudera benchmarks indicate that Kafka reaches maximum throughput with
                  message sizes of around 10 KB. Larger messages show decreased throughput. However,
                  in certain cases, users need to send messages much larger than 10 KB.
If the message payload sizes are in the order of 100s of MB, consider
                  exploring the following alternatives:

If shared storage is available (HDFS, S3, NAS), place the large
                    payload on shared storage and use Kafka just to send a message with the payload
                    location.
Handle large messages by chopping them into smaller parts before writing into
                    Kafka, using a message key to make sure all the parts are written to the same
                    partition so that they are consumed by the same Consumer, and re-assembling the
                    large message from its parts when consuming.





Where can I get Kafka training?
Cloudera Educational Services offers various courses on Kafka. For a basic training that
        gives an overview of Kafka, its architecture, messages, producers and consumers (clients),
        as well as command line tools, see Apache Kafka Basics. Additionally, if you are
        looking for more in depth training on Kafka, Kafka security, Kafka Connect, Streams
        Messaging Manager, and so on, search for Kafka in the course catalog on http://education.cloudera.com.


Parent topic: Kafka FAQ