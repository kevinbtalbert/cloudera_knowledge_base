Spark tuningCloudera Docs
Spark tuning
In general the Spark jobs were designed to run with minimal tuning and configuration.
  You can adjust the number of executors and resources to increase parallelism and performance using
  Sparkâ€™s configuration options.
If your tables are super wide and your default memory allocation is fairly low, you may see
   jobs fail. To resolve this, increase the Spark executor memory. A conservative rule of thumb is 1
   GiB per 50 columns.
If your Spark resources drastically outscale the Kudu cluster, then you may want to limit the
   number of concurrent tasks allowed to run on restore.

Parent topic: Kudu integration with Spark