Optimizing NameNode disk space with Hadoop archivesCloudera Docs
Optimizing NameNode disk
            space with Hadoop archives
Hadoop Archives (HAR) are special format archives that efficiently pack small files
            into HDFS blocks.
The Hadoop Distributed File System (HDFS) is
            designed to store and process large data sets, but HDFS can be less efficient when
            storing a large number of small files. When there are many small files stored in HDFS,
            these small files occupy a large portion of the namespace. As a result, disk space is
            under-utilized because of the namespace limitation.
Hadoop Archives (HAR) can be used to address
            the namespace limitations associated with storing many small files. A Hadoop Archive
            packs small files into HDFS blocks more efficiently, thereby reducing NameNode memory
            usage while still allowing transparent access to files. Hadoop Archives are also
            compatible with MapReduce, allowing transparent access to the original files by
            MapReduce jobs.

Overview of Hadoop archivesStoring a large number of small files in HDFS leads to inefficient utilization of       space â€“ the namespace is overutilized while the disk space might be underutilized. Hadoop       Archives (HAR) address this limitation by efficiently packing small files into large files       without impacting the file access.Hadoop archive componentsYou can use the Hadoop archiving tool to create Hadoop Archives (HAR). The Hadoop       Archive is integrated with the Hadoop file system interface. Files in a HAR are exposed       transparently to users. File data in a HAR is stored in multipart files, which are indexed to       retain the original separation of data.Creating a Hadoop archiveHadoop Archives can be created using the Hadoop archiving tool. The archiving tool       uses MapReduce to efficiently create Hadoop Archives in parallel. Use the "hadoop archive"       command to invoke the Hadoop archiving tool.List files in Hadoop archivesUse the hdfs dfs -ls command to list files in Hadoop             archives.Format for using Hadoop archives with MapReduceTo use Hadoop Archives with MapReduce, you must reference files differently than you         would with the default file system. If you have a Hadoop Archive stored in HDFS in             /user/zoo/foo.har, you must specify the input directory as             har:///user/zoo/foo.har to use it as a MapReduce input. Parent topic: Optimizing performance