Using HttpFS to provide access to HDFSCloudera Docs
Using HttpFS to provide access to HDFS
Apache Hadoop HttpFS is a service that provides HTTP access to HDFS. HttpFS has a REST
    HTTP API supporting all HDFS filesystem operations (both read and write).
Common HttpFS use cases are:
Read and write data in
          HDFS using HTTP utilities (such as curl or
            wget) and HTTP libraries from languages other than Java
          (such as Perl).
Transfer data between
          HDFS clusters running different versions of Hadoop (overcoming RPC versioning issues), for
          example using Hadoop DistCp.
Accessing WebHDFS using the NameNode Web UI port (default port 9870). Access to all data
          hosts in the cluster is required, because WebHDFS redirects clients to the DataNode port
          (default port 9864). If the cluster is behind a firewall, and you use WebHDFS to read and
          write data to HDFS, then Cloudera recommends you use the HttpFS server. The HttpFS server
          acts as a gateway. It is the only system that is allowed to send and receive data through
          the firewall.

HttpFS supports Hadoop pseudo-authentication, HTTP SPNEGO Kerberos, and additional
      authentication mechanisms using a plugin API. HttpFS also supports Hadoop proxy user
      functionality.
The webhdfs client file system implementation can access HttpFS
      using the Hadoop filesystem command (hadoop fs), by using Hadoop
      DistCp, and from Java applications using the Hadoop file system Java API.
The HttpFS HTTP REST API is interoperable with the WebHDFS REST HTTP API.

Add the HttpFS roleYou can use Cloudera Manager to add the HttpFS role.Using Load Balancer with HttpFSConfigure the HttpFS Service to work with the load balancer that you configured for         the service.