How Cloudera Search worksCloudera Docs
How Cloudera Search works
Learn about the operation of Cloudera Search
In near real-time indexing use cases, such as log or event stream
      analytics, Cloudera Search indexes events that are streamed through Apache
      Kafka, Spark Streaming, or HBase. Fields and events are mapped to standard
      Solr indexable schemas. Lucene indexes the incoming events and the index
      is written and stored in standard Lucene index files in HDFS.
The indexes by default are loaded from HDFS to Solr cores, but Solr is also able to read
      from local disk. The difference in the design of Cloudera Search is the robust, distributed,
      and scalable storage layer of HDFS, which helps eliminate costly downtime and allows for
      flexibility across workloads without having to move data. Search queries can then be submitted
      to Solr through either the standard Solr API, or through a simple search GUI application,
      included in Cloudera Search, which can be deployed in Hue.
Cloudera Search batch-oriented indexing capabilities can address needs
        for searching across batch uploaded files or large data sets that are
        less frequently updated and less in need of near-real-time indexing. It
        can also be conveniently used for re-indexing (a common pain point in
        stand-alone Solr) or ad-hoc indexing for on-demand data exploration.
        Often, batch indexing is done on a regular basis (hourly, daily, weekly,
        and so on) as part of a larger workflow.
For such cases, Cloudera Search includes a highly scalable indexing workflow based on
      MapReduce or Spark. A MapReduce or Spark workflow is launched for specified files or folders
      in HDFS, or tables in HBase, and the field extraction and Solr schema mapping occurs during
      the mapping phase. Reducers use embedded Lucene to write the data as a single index or as
      index shards, depending on your configuration and preferences. After the indexes are stored,
      they can be queried by using standard Solr mechanisms, as previously described above for the
      near-real-time indexing use case. You can also configure these batch indexing options to post
      newly indexed data directly into live, active indexes, served by Solr. This GoLive
      option enables a streamlined data pipeline without interrupting service to process regularly
      incoming batch updates.
The Lily HBase Indexer Service is a flexible, scalable, fault tolerant,
        transactional, near real-time oriented system for processing a
        continuous stream of HBase cell updates into live search indexes. The
        Lily HBase Indexer uses Solr to index data stored in HBase. As HBase
        applies inserts, updates, and deletes to HBase table cells, the indexer
        keeps Solr consistent with the HBase table contents, using standard
        HBase replication features. The indexer supports flexible custom
        application-specific rules to extract, transform, and load HBase data
        into Solr. Solr search results can contain
          columnFamily:qualifier links back to the data stored
        in HBase. This way applications can use the Search result set to
        directly access matching raw HBase cells. Indexing and searching do not
        affect operational stability or write throughput of HBase because the
        indexing and searching processes are separate and asynchronous to
        HBase.

